<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://zuolinlin.github.io/zuo.github.io/</id>
    <title>zuolinlin</title>
    <updated>2022-07-03T10:33:14.338Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://zuolinlin.github.io/zuo.github.io/"/>
    <link rel="self" href="https://zuolinlin.github.io/zuo.github.io/atom.xml"/>
    <subtitle>你要问我如何去二仙桥，我会告诉你走成华大道。可你要问人生，我也说不清。</subtitle>
    <logo>https://zuolinlin.github.io/zuo.github.io/images/avatar.png</logo>
    <icon>https://zuolinlin.github.io/zuo.github.io/favicon.ico</icon>
    <rights>All rights reserved 2022, zuolinlin</rights>
    <entry>
        <title type="html"><![CDATA[CompletableFuture]]></title>
        <id>https://zuolinlin.github.io/zuo.github.io/post/completablefuture/</id>
        <link href="https://zuolinlin.github.io/zuo.github.io/post/completablefuture/">
        </link>
        <updated>2022-04-10T06:32:09.000Z</updated>
        <content type="html"><![CDATA[<h1 id="completablefuture">CompletableFuture</h1>
<h2 id="目录">目录</h2>
<ul>
<li><a href="#%E5%9F%BA%E6%9C%ACapi">基本api</a>
<ul>
<li><a href="#SupplyAsync">supplyAsync</a></li>
<li><a href="#ThenCompose">thenCompose</a></li>
<li><a href="#ThenCombine">thenCombine</a></li>
<li><a href="#ThenApply">ThenApply</a></li>
<li><a href="#ApplyToEither%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%EF%BC%9F">ApplyToEither？为什么要垃圾回收？</a></li>
<li><a href="#Exceptionally">Exceptionally</a></li>
<li><a href="#AllOf">AllOf</a></li>
<li><a href="#Join">Join</a></li>
</ul>
</li>
</ul>
<h1 id="目录-2">目录</h1>
<h1 id="基本api">基本api</h1>
<h2 id="supplyasync">SupplyAsync</h2>
<pre><code>开启一个异步任务
</code></pre>
<pre><code class="language-java">public class _01_supplyAsync {
    public static void main(String[] args) {
        SmallTool.printTimeAndThread(&quot;小白进入餐厅&quot;);
        SmallTool.printTimeAndThread(&quot;小白点了 番茄炒蛋 + 一碗米饭&quot;);

        CompletableFuture&lt;String&gt; cf1 = CompletableFuture.supplyAsync(() -&gt; {
            SmallTool.printTimeAndThread(&quot;厨师炒菜&quot;);
            SmallTool.sleepMillis(200);
            SmallTool.printTimeAndThread(&quot;厨师打饭&quot;);
            SmallTool.sleepMillis(100);
            return &quot;番茄炒蛋 + 米饭 做好了&quot;;
        });

        SmallTool.printTimeAndThread(&quot;小白在打王者&quot;);
        SmallTool.printTimeAndThread(String.format(&quot;%s ,小白开吃&quot;, cf1.join()));
    }
}
</code></pre>
<h2 id="thencompose">ThenCompose</h2>
<pre><code>连接两个异步任务--将前一个任务的执行结构，提交给另一个任务
</code></pre>
<pre><code class="language-java">public class _02_thenCompose {
    public static void main(String[] args) {
        SmallTool.printTimeAndThread(&quot;小白进入餐厅&quot;);
        SmallTool.printTimeAndThread(&quot;小白点了 番茄炒蛋 + 一碗米饭&quot;);

        CompletableFuture&lt;String&gt; cf1 = CompletableFuture.supplyAsync(() -&gt; {
            SmallTool.printTimeAndThread(&quot;厨师炒菜&quot;);
            SmallTool.sleepMillis(200);
            return &quot;番茄炒蛋&quot;;
        }).thenCompose(dish -&gt; CompletableFuture.supplyAsync(() -&gt; {
            SmallTool.printTimeAndThread(&quot;服务员打饭&quot;);
            SmallTool.sleepMillis(100);
            return dish + &quot; + 米饭&quot;;
        }));

        SmallTool.printTimeAndThread(&quot;小白在打王者&quot;);
        SmallTool.printTimeAndThread(String.format(&quot;%s 好了,小白开吃&quot;, cf1.join()));
    }

    /**
     * 用 applyAsync 也能实现
     */
    private static void applyAsync() {
        SmallTool.printTimeAndThread(&quot;小白进入餐厅&quot;);
        SmallTool.printTimeAndThread(&quot;小白点了 番茄炒蛋 + 一碗米饭&quot;);

        CompletableFuture&lt;String&gt; cf1 = CompletableFuture.supplyAsync(() -&gt; {
            SmallTool.printTimeAndThread(&quot;厨师炒菜&quot;);
            SmallTool.sleepMillis(200);
            CompletableFuture&lt;String&gt; race = CompletableFuture.supplyAsync(() -&gt; {
                SmallTool.printTimeAndThread(&quot;服务员打饭&quot;);
                SmallTool.sleepMillis(100);
                return &quot; + 米饭&quot;;
            });
            return &quot;番茄炒蛋&quot; + race.join();
        });

        SmallTool.printTimeAndThread(&quot;小白在打王者&quot;);
        SmallTool.printTimeAndThread(String.format(&quot;%s 好了,小白开吃&quot;, cf1.join()));
    }
}
</code></pre>
<h2 id="thencombine">ThenCombine</h2>
<pre><code>合并两个异步任务---将前一个任务的执行结构，提交给另一个任务
</code></pre>
<pre><code class="language-java">public class _03_thenCombine {
    public static void main(String[] args) {
        SmallTool.printTimeAndThread(&quot;小白进入餐厅&quot;);
        SmallTool.printTimeAndThread(&quot;小白点了 番茄炒蛋 + 一碗米饭&quot;);

        CompletableFuture&lt;String&gt; cf1 = CompletableFuture.supplyAsync(() -&gt; {
            SmallTool.printTimeAndThread(&quot;厨师炒菜&quot;);
            SmallTool.sleepMillis(200);
            return &quot;番茄炒蛋&quot;;
        }).thenCombine(CompletableFuture.supplyAsync(() -&gt; {
            SmallTool.printTimeAndThread(&quot;服务员蒸饭&quot;);
            SmallTool.sleepMillis(300);
            return &quot;米饭&quot;;
        }), (dish, rice) -&gt; {
            SmallTool.printTimeAndThread(&quot;服务员打饭&quot;);
            SmallTool.sleepMillis(100);
            return String.format(&quot;%s + %s 好了&quot;, dish, rice);
        });

        SmallTool.printTimeAndThread(&quot;小白在打王者&quot;);
        SmallTool.printTimeAndThread(String.format(&quot;%s ,小白开吃&quot;, cf1.join()));

    }


    /**
     * 用 applyAsync 也能实现
     */
    private static void applyAsync() {
        SmallTool.printTimeAndThread(&quot;小白进入餐厅&quot;);
        SmallTool.printTimeAndThread(&quot;小白点了 番茄炒蛋 + 一碗米饭&quot;);

        CompletableFuture&lt;String&gt; cf1 = CompletableFuture.supplyAsync(() -&gt; {
            SmallTool.printTimeAndThread(&quot;厨师炒菜&quot;);
            SmallTool.sleepMillis(200);
            return &quot;番茄炒蛋&quot;;
        });
        CompletableFuture&lt;String&gt; race = CompletableFuture.supplyAsync(() -&gt; {
            SmallTool.printTimeAndThread(&quot;服务员蒸饭&quot;);
            SmallTool.sleepMillis(300);
            return &quot;米饭&quot;;
        });
        SmallTool.printTimeAndThread(&quot;小白在打王者&quot;);

        String result = String.format(&quot;%s + %s 好了&quot;, cf1.join(), race.join());
        SmallTool.printTimeAndThread(&quot;服务员打饭&quot;);
        SmallTool.sleepMillis(100);

        SmallTool.printTimeAndThread(String.format(&quot;%s ,小白开吃&quot;, result));
    }
}

</code></pre>
<h2 id="thenapply">ThenApply</h2>
<pre><code>做任务的异步处理 ---将前一个任务的执行结构，提交给另一个function
</code></pre>
<pre><code class="language-java">public class _01_thenApply {
    public static void main(String[] args) {
        SmallTool.printTimeAndThread(&quot;小白吃好了&quot;);
        SmallTool.printTimeAndThread(&quot;小白 结账、要求开发票&quot;);

        CompletableFuture&lt;String&gt; invoice = CompletableFuture.supplyAsync(() -&gt; {
            SmallTool.printTimeAndThread(&quot;服务员收款 500元&quot;);
            SmallTool.sleepMillis(100);
            return &quot;500&quot;;
        }).thenApplyAsync(money -&gt; {
            SmallTool.printTimeAndThread(String.format(&quot;服务员开发票 面额 %s元&quot;, money));
            SmallTool.sleepMillis(200);
            return String.format(&quot;%s元发票&quot;, money);
        });

        SmallTool.printTimeAndThread(&quot;小白 接到朋友的电话，想一起打游戏&quot;);

        SmallTool.printTimeAndThread(String.format(&quot;小白拿到%s，准备回家&quot;, invoice.join()));
    }


    private static void one() {
        SmallTool.printTimeAndThread(&quot;小白吃好了&quot;);
        SmallTool.printTimeAndThread(&quot;小白 结账、要求开发票&quot;);

        CompletableFuture&lt;String&gt; invoice = CompletableFuture.supplyAsync(() -&gt; {
            SmallTool.printTimeAndThread(&quot;服务员收款 500元&quot;);
            SmallTool.sleepMillis(100);
            SmallTool.printTimeAndThread(&quot;服务员开发票 面额 500元&quot;);
            SmallTool.sleepMillis(200);
            return &quot;500元发票&quot;;
        });

        SmallTool.printTimeAndThread(&quot;小白 接到朋友的电话，想一起打游戏&quot;);

        SmallTool.printTimeAndThread(String.format(&quot;小白拿到%s，准备回家&quot;, invoice.join()));
    }


    private static void two() {
        SmallTool.printTimeAndThread(&quot;小白吃好了&quot;);
        SmallTool.printTimeAndThread(&quot;小白 结账、要求开发票&quot;);

        CompletableFuture&lt;String&gt; invoice = CompletableFuture.supplyAsync(() -&gt; {
            SmallTool.printTimeAndThread(&quot;服务员收款 500元&quot;);
            SmallTool.sleepMillis(100);

            CompletableFuture&lt;String&gt; waiter2 = CompletableFuture.supplyAsync(() -&gt; {
                SmallTool.printTimeAndThread(&quot;服务员开发票 面额 500元&quot;);
                SmallTool.sleepMillis(200);
                return &quot;500元发票&quot;;
            });

            return waiter2.join();
        });

        SmallTool.printTimeAndThread(&quot;小白 接到朋友的电话，想一起打游戏&quot;);

        SmallTool.printTimeAndThread(String.format(&quot;小白拿到%s，准备回家&quot;, invoice.join()));
    }
}

</code></pre>
<h2 id="applytoeither">ApplyToEither</h2>
<pre><code>用来获取最先完成的任务---哪个任务先执行完，就先执行
</code></pre>
<pre><code class="language-java">
public class _02_applyToEither {
    public static void main(String[] args) {
        SmallTool.printTimeAndThread(&quot;张三走出餐厅，来到公交站&quot;);
        SmallTool.printTimeAndThread(&quot;等待 700路 或者 800路 公交到来&quot;);

        CompletableFuture&lt;String&gt; bus = CompletableFuture.supplyAsync(() -&gt; {
            SmallTool.printTimeAndThread(&quot;700路公交正在赶来&quot;);
            SmallTool.sleepMillis(100);
            return &quot;700路到了&quot;;
        }).applyToEither(CompletableFuture.supplyAsync(() -&gt; {
            SmallTool.printTimeAndThread(&quot;800路公交正在赶来&quot;);
            SmallTool.sleepMillis(200);
            return &quot;800路到了&quot;;
        }), firstComeBus -&gt; firstComeBus);

        SmallTool.printTimeAndThread(String.format(&quot;%s,小白坐车回家&quot;, bus.join()));
    }
}
</code></pre>
<h2 id="exceptionally">Exceptionally</h2>
<pre><code class="language-java">public class _03_exceptionally {
    public static void main(String[] args) {
        SmallTool.printTimeAndThread(&quot;张三走出餐厅，来到公交站&quot;);
        SmallTool.printTimeAndThread(&quot;等待 700路 或者 800路 公交到来&quot;);

        CompletableFuture&lt;String&gt; bus = CompletableFuture.supplyAsync(() -&gt; {
            SmallTool.printTimeAndThread(&quot;700路公交正在赶来&quot;);
            SmallTool.sleepMillis(100);
            return &quot;700路到了&quot;;
        }).applyToEither(CompletableFuture.supplyAsync(() -&gt; {
            SmallTool.printTimeAndThread(&quot;800路公交正在赶来&quot;);
            SmallTool.sleepMillis(200);
            return &quot;800路到了&quot;;
        }), firstComeBus -&gt; {
            SmallTool.printTimeAndThread(firstComeBus);
            if (firstComeBus.startsWith(&quot;700&quot;)) {
                throw new RuntimeException(&quot;撞树了……&quot;);
            }
            return firstComeBus;
        }).exceptionally(e -&gt; {
            SmallTool.printTimeAndThread(e.getMessage());
            SmallTool.printTimeAndThread(&quot;小白叫出租车&quot;);
            return &quot;出租车 叫到了&quot;;
        });

        SmallTool.printTimeAndThread(String.format(&quot;%s,小白坐车回家&quot;, bus.join()));
    }
}

</code></pre>
<h2 id="allof">AllOf</h2>
<h2 id="join">Join</h2>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[JVM]]></title>
        <id>https://zuolinlin.github.io/zuo.github.io/post/jvm/</id>
        <link href="https://zuolinlin.github.io/zuo.github.io/post/jvm/">
        </link>
        <updated>2022-04-08T13:44:30.000Z</updated>
        <content type="html"><![CDATA[<h1 id="jvm从0到实战">JVM从0到实战</h1>
<h2 id="目录">目录</h2>
<ul>
<li><a href="#JVM%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86">jvm运行原理</a>
<ul>
<li><a href="#JVM%E6%A0%B8%E5%BF%83%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B">jvm核心运行流程</a></li>
<li><a href="#JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6">JVM类加载机制</a></li>
<li><a href="#JVM%E4%B8%AD%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F">JVM中内存区域</a></li>
</ul>
</li>
<li><a href="#JVM%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6">JVM垃圾回收机制</a>
<ul>
<li><a href="#JVM%E7%9A%84%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6%E6%98%AF%E7%94%A8%E6%9D%A5%E5%B9%B2%E5%98%9B%E7%9A%84%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%EF%BC%9F">JVM的垃圾回收机制是用来干嘛的？为什么要垃圾回收？</a></li>
<li><a href="#JVM%E5%88%86%E4%BB%A3%E6%A8%A1%E5%9E%8B">JVM分代模型</a></li>
<li><a href="#JVM%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%97%E6%B3%95">JVM垃圾回收算法</a></li>
</ul>
</li>
<li><a href="#JVM%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8">JVM垃圾回收器</a>
<ul>
<li><a href="#parNew%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8">parNew垃圾回收器</a></li>
<li><a href="#CMS%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8">CMS垃圾回收器</a></li>
<li><a href="#G1%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8">G1垃圾回收器</a></li>
</ul>
</li>
<li><a href="#JVM%E5%AE%9E%E6%88%98">JVM实战</a>
<ul>
<li>[自己动手模拟出频繁Young GC的场景体验一下！](#自己动手模拟出频繁Young GC的场景体验一下！)</li>
<li><a href="#%E4%BD%BF%E7%94%A8jstat%E6%91%B8%E6%B8%85%E7%BA%BF%E4%B8%8A%E7%B3%BB%E7%BB%9F%E7%9A%84JVM%E8%BF%90%E8%A1%8C%E7%8A%B6%E5%86%B5">使用jstat摸清线上系统的JVM运行状况</a></li>
<li><a href="#%E4%BD%BF%E7%94%A8jmap%E5%92%8Cjhat%E6%91%B8%E6%B8%85%E7%BA%BF%E4%B8%8A%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%AF%B9%E8%B1%A1%E5%88%86%E5%B8%83">使用jmap和jhat摸清线上系统的对象分布</a></li>
</ul>
</li>
</ul>
<h2 id="大纲">大纲</h2>
<h2 id="jvm运行原理">JVM运行原理</h2>
<h3 id="jvm核心运行流程">JVM核心运行流程</h3>
<figure data-type="image" tabindex="1"><img src="images/jvm-process.png" alt="我们编写的java文件被执行的过程" loading="lazy"></figure>
<pre><code>JVM要运行这些字节码文件，首先得把这些.class文件加载进来
此时会采用类加载器将这些字节码文件加载搭配JVM，供后续的代码运行使用
最后一步，JVM会基于自己的**字节码执行引擎**，来执行加载到内存里我们写好的那些类了。
</code></pre>
<p>下一步讨论</p>
<h3 id="jvm类加载机制">JVM类加载机制</h3>
<p>一个类从加载到使用一般经历一下几个过程</p>
<figure data-type="image" tabindex="2"><img src="images/class_loader_process.png" alt="JVM的类加载器亲子层级" loading="lazy"></figure>
<p><strong>加载</strong>--<strong>验证</strong>--<strong>准备</strong>--<strong>解析</strong>--<strong>初始化</strong>--<strong>使用</strong>--<strong>卸载</strong></p>
<p>加载阶段：<br>
代码在使用这个类的时候，.class 文件会加载进内存</p>
<p>验证阶段：<br>
根据java虚拟机规范，来验证你加载进来的.class 文件的内容是否符合规范</p>
<p>准备阶段：<br>
给类遍历分配内存空间，来一个默认的初始值</p>
<p>解析阶段：<br>
把符号引用替换为直接引用</p>
<p>初始化：<br>
new Object() 初始化一个对象，在准备阶段知识只是分配了内存空间，给了默认值，真正的赋值是在初始化阶段完成的<br>
此外这里有个非常重要的规则，就是初始化一个类的时候，发现它的父类还没有初始化，那么必须先初始化它的父类。</p>
<h3 id="类加载器和双亲委派机制">类加载器和双亲委派机制</h3>
<pre><code> 从加载到初始化实际上是有类加载器来完成的
</code></pre>
<p><strong>java中有哪些类加载器呢？简单来说有下面几种</strong></p>
<pre><code>- 启动类加载器
    Bootstrap classLoader ，它主要是负债加载我们机器安装的java目录下的核心类的
    相信大家都知道，如果你要在机器上运用一个自己写好的Java系统，无论是Windows笔记本，还是linux服务器，是不是都得装一下JDK，
    那么你在安装目录下就有一个**lib**文件
    所以一旦你的JVM启动，那么首先就会依托启动类加载器，去加载你的JAVA 安装目录下的**lib**目录

- 扩展类加载器
    Extension ClassLoader，这个类加载器也是类似的，就是在你JAVA安装目录下，有个**lib\ext**目录，
    这里有一些类，就是需要使用这个类加载器来加载的，支撑你的系统的运行。
    那么你的JVM一旦启动，是不是也得从java安装目录下，加载这个**lib\ext**目录中的类

- 应用类加载器
    Application ClassLoader，这类加载器就去加载**classpath**环境变量所指定的路径中的类加载器
    其实大致就理解为去加载你写好的Java代码吧，这类加载器主要负责加载你写好的那些类到内存里。

- 自定义类加载器
    除了上面那几种情况外，还可以自定义类加载器，去根据你的需求加载你的类。
</code></pre>
<p>jvm的类加载器是有亲子层级机构的，就是启动类加载器子啊最上层，拓展类加载器在第二层，第三层是应用程序类加载器，最下面一层是自定义加载器</p>
<figure data-type="image" tabindex="3"><img src="images/classloder-qinzichengji.png" alt="JVM的类加载器亲子层级" loading="lazy"></figure>
<p>基于这个亲子层级关系，就有一个<strong>双亲委派机制</strong></p>
<p>双亲委派机制：先由父类加载，不行的话在由儿子来加载。</p>
<figure data-type="image" tabindex="4"><img src="images/class_loader_shuangqinweipai.png" alt="JVM的类加载器亲子层级" loading="lazy"></figure>
<p>首先类加载器由JVM提供，类加载器逻辑的第一个阶段解释加载阶段，后面的由JVM完成。</p>
<h3 id="jvm中内存区域">JVM中内存区域</h3>
<ul>
<li>
<p>元空间</p>
<p>jvm在运行我们写好的代码，他必须使用多块内存空间的，不同的内存空间用来存放不同的数据，然后配合我们写的代码流程才能让我们系统运行起来。<br>
我们现在知道了JVM会加载类到内存里供后续运行使用，所以JVM里必须有一块内存空间，用来存放我们写的那些类。<br>
方法区是在JDK1.8以前版本里，代表JVM的一块区域。</p>
<p>主要是存放.class文件中加载进来的类，还会有一些类似常亮池的东西放在这个区域里。<br>
但是JDK1.8之后，这块区域的名字变了，叫&quot;metespace&quot; ，可以认为&quot;元数据空间&quot;的意思，主要还是存放我们写的各种类的相关信息。<br>
'.calss' 后缀的字节码文件里，存放的就是你写出来的代码编译好的字节码了，对应着各种字节码指令。</p>
</li>
<li>
<p>程序计数器</p>
<p>现在当JVM加载类信息到内存之后，实际上就是使用自己的字节码执行引擎，去执行我们写的代码编译出来的代码指令</p>
<figure data-type="image" tabindex="5"><img src="images/zijiemazhixingyinqing.png" alt="jvm是有字节码执行引擎去执行.call文件里面的指令" loading="lazy"></figure>
<p>那么在执行字节码的时候，就需要一块特殊的指令区域:<strong>程序计数器</strong></p>
<p>这个程序计数器就是来记录，当前<strong>字节码指令执行位置的</strong></p>
<p><strong>每个线程都有一个自己的程序指令器</strong></p>
</li>
</ul>
<figure data-type="image" tabindex="6"><img src="images/chengxujishuqi.png" alt="每个线程都有一个自己的程序指令器" loading="lazy"></figure>
<ul>
<li>
<p>java虚拟机栈</p>
<p>java代码在执行的时候一定是一个线程来执行某个方法中的代码，<br>
但是在方法里，我们会定义一些方法内的局部变量，<br>
因此，JVM必须有一块区域来保存方法内的<strong>局部变量</strong>等数据，这个区域就是<strong>java虚拟机栈</strong></p>
<p><strong>每个线程都有自己的java虚拟机栈，</strong></p>
<p>如果一个线程执行一个方法，<strong>就会对这个方法调用创建一个栈帧</strong></p>
<p>栈帧里就会有这个方法的局部变量表，操作数栈，动态链表，方法出口等</p>
<p>在执行方法的时候会创建栈桢，压入到java虚拟机栈中，当方法执行完成时们，就会把对应方法的栈桢，从java虚拟机中出栈</p>
</li>
</ul>
<figure data-type="image" tabindex="7"><img src="images/javaxunijizhan.png" alt="img.png" loading="lazy"></figure>
<ul>
<li>java堆内存<br>
主要存放我们创建的对象</li>
</ul>
<p>核心内存区域的全流程串讲<br>
<img src="images/jvm_allprocess.png" alt="img.png" loading="lazy"></p>
<pre><code class="language-java">    class kafka{
    public static void main(String[] args){
        Magage manage = new Magage();
    }
    public class Manage{
        int i ;
        public int excute(){
            i ++;
            return i;
        }
    }
}
</code></pre>
<p>首先你的JVM进程会启动，加载你的Kafka类到内存⾥。</p>
<p>然后有⼀个main线程，开始执⾏你的Kafka中的main()⽅法。</p>
<p>main线程是关联了⼀个程序计数器的，那么他执⾏到哪⼀⾏指令，就会记录在这⾥<br>
⼤家结合上图中的程序计数器来理解⼀下。</p>
<p>其次，就是main线程在执⾏main()⽅法的时候，会在main线程关联的Java虚拟机栈⾥，压⼊⼀个main()⽅法的栈帧。</p>
<p>接着会发现需要创建⼀个ReplicaManager类的实例对象，此时会加载ReplicaManager类到内存⾥来。</p>
<p>然后会创建⼀个ReplicaManager的对象实例分配在Java堆内存⾥，并且在main()⽅法的栈帧⾥的局部变量表引⼊⼀个<br>
“replicaManager”变量，让他引⽤ReplicaManager对象在Java堆内存中的地址。</p>
<h2 id="jvm垃圾回收机制">JVM垃圾回收机制</h2>
<h3 id="jvm的垃圾回收机制是用来干嘛的为什么要垃圾回收">JVM的垃圾回收机制是用来干嘛的？为什么要垃圾回收？</h3>
<p>一旦方法执行完毕，压入栈桢中的方法就会出栈，栈桢里的局部变量也就没有了<br>
也就是说没有一个变量指向java堆内存中的实例变量了<br>
<img src="images/jvm_lajihuishou.png" alt="img.png" loading="lazy"></p>
<p>核心的知识点来了，java堆内存中的实例对象已经没有人引用它了。这个对象实际上已经没用了</p>
<p>内存资源是有限的。</p>
<p>我们在java堆内存里创建的对象都是占用内存资源的，而且内存资源有限。</p>
<p>对于不用的对象，我们应该怎么处理呢？</p>
<p>答案呼之欲出：<strong>JVM垃圾回收机制</strong></p>
<p>JVM本身就有垃圾回收机制的，它是一个<strong>后台自动运行的线程。</strong></p>
<p>你只要启动一个JVM进程，他就会自动携带一个垃圾回收的后台线程。</p>
<p>这个线程会在后台不断检测jvm堆内存中的各个实力对象。</p>
<p>那些不在被人引用的实例对象，即jvm中的<strong>垃圾</strong>，就会定期的被后台垃圾回收线程清理掉</p>
<figure data-type="image" tabindex="8"><img src="images/jvm_lajihuishouxianchen.png" alt="img.png" loading="lazy"></figure>
<p>思考题：加载到方法区的内会被回收吗，什么时候回收？为什么？</p>
<ol>
<li>该类的实例对象不存在时</li>
<li>该类的classLoader 不在被使用时</li>
<li>该类的class对象不再被使用时</li>
</ol>
<h3 id="jvm分代模型">JVM分代模型</h3>
<p>java将堆内存划分为新生代代  老年代</p>
<p>咱们平时创建出来的对象，一般情况就是两种</p>
<ol>
<li>一种是短期存活的对象，分配在堆内存后，迅速被垃圾回收</li>
<li>一种是长期存活的对象，需要一直活在堆内存里，供程序后续使用</li>
</ol>
<p>对象什么情况下进入年新生代什么情况下进入老年代</p>
<p><strong>大部分对象都优先在新生代分配内存</strong></p>
<pre><code class="language-java">        
   public KafKa{
    public static Manage manage = new Manage();
            
}

</code></pre>
<p>类manage的静态变量的对象 new manage() 会长期存活在堆内存中<br>
哪怕是这样，你开始创建 new manage() 对象的时候，他也是分配到新生代里</p>
<p>JVM 有一条规定<br>
如果一个对象在新生代中，在15次垃圾回收之后，还没有被回收掉，说明它已经15岁了<br>
这是对象的年龄，每次垃圾回收，如果对象没有被回收掉，那么它的年龄就会—加1<br>
达到一定的年龄限制后，会被认为是长期存活的对象。</p>
<p>然后会被转移到Java 堆内存中的老年代。</p>
<p><strong>思考题</strong><br>
老年代会进行垃圾回收吗？<br>
答案是肯定的，只要是不被引用的对象就会被回收。</p>
<p><strong>跟JVM内存相关的介个核心参数图解</strong><br>
在JVM内存分配中，几个核心参数比较重要</p>
<pre><code>    1. -Xms java堆内存的大小
    2. -Xmx java堆内存的最大大小
    3. -Xmn java堆内存中新生代大小，扣除新生代剩下的就是老年代的内存了
    4. -XX:PermSize 永久代大小 java1.8之后替换为 -XX:MetaSpaceSize 
    5. -XX:MaxPermSize 永久代的最大大小 java1.8之后替换为  -XX:MaxMetaSpaceSize
    6. -Xss:每个线程的栈内存大小
</code></pre>
<figure data-type="image" tabindex="9"><img src="images/jvm_params.png" alt="img.png" loading="lazy"></figure>
<p><strong>如何启动JVM参数</strong></p>
<p><strong>如何设置堆内存大小</strong></p>
<pre><code>根据对象创建的速度和对象的大小（每个属性的占用的字节数） 去估算
</code></pre>
<p><strong>如何设置永久代大小</strong></p>
<pre><code>一般设置个几百兆
</code></pre>
<p><strong>如何设置栈内存大小</strong></p>
<pre><code> 默认512kb -1M
</code></pre>
<p><strong>什么时候会触发垃圾回收</strong><br>
生代核心的垃圾回收触发时机</p>
<pre><code>新生代快满的时候就会触发垃圾回收
</code></pre>
<p>几个触发老年代GC的时机**</p>
<pre><code>1.第一是老年代的可用内存大小小于新生代的全部对象，如果没有开启空间担保参数，就只直接触发FullGC，所以一般空间担保参数都会打开
2.第二是老年代可用的内存大小，小于历次新生代GC后进入老年代的平均大小，此时会提前触发FullGC；
3.第三是新生代MinorGC后存活对象大于Survivor，那么就会进入老年代，此时老年代内存不足
4.-XX:CMSInitialingOccparencyFaction
</code></pre>
<p><strong>被哪些变量引用的对象是不能被回收的</strong></p>
<pre><code>  新生代快满时，进行垃圾回收的时候，到底哪些对能能够被回收，哪些对象不能被回收
  JVM运用了**可达性分析算法**，来判断哪些对象是可以被回收的，哪些对象是不能被回收的。
  这个算法的意思就是，**每个对象都分析一下，有谁在引用它，然后一层层向上判断，看看是否又一个GCROOT**
  一句话总结：**就是只要你的对象被方法的局部变量或者类的静态变量引用，就不会回收它**
</code></pre>
<p><strong>java中对象的不同引用类型</strong></p>
<pre><code>关于引用与垃圾回收的关系，就是java里，有不同的引用类型
强引用，软引用，弱引用，虚引用
强引用：一个变量引用一个对象，只要是强引用的对象，垃圾回收机制都不会去回收它
软引用：对象里面包裹着的那个对象，正常情况下，垃圾回收机制是不会回收软引用的对象的，但是发现内存还是不够存放新的对象时，
       内存都快溢出了，这是会把软引用的对象给回收掉，哪怕它被变量引用者，但是因为它是软引用还是要被回收掉
弱引用：很好理解，就是没有引用，如果发生垃圾回收，就会把这个垃圾回收掉
虚引用：
</code></pre>
<p><strong>finalize()方法的作用</strong></p>
<h3 id="jvm垃圾回收算法">JVM垃圾回收算法</h3>
<ol>
<li>
<p>复制算法（新生代的垃圾回收算法）</p>
<pre><code> 所谓的复制算法就是把新生代分为两块，使用其中一块，待那块区域快满的时候，把存活的对象放到另一块区域
 保证没有内存碎片，接着一次性回收，原来那块内存区域的垃圾对象，再次空出来一块内存区域。
 **两块内存区域都这么重复循环使用着**
</code></pre>
</li>
</ol>
<p><strong>复制算法的缺点</strong><br>
如果分配了1G的新生代内存，那么只有512M可以使用，有一半的内存是浪费的<br>
至始至终只有一半的内存可以用，对内存的使用效率太低了。</p>
<p><strong>复制算法的优化</strong>Eden和Survivor<br>
真正的复制算法会做如下优化<br>
把新生代的内存分成3块，<br>
一个Eden区两个Survivor区，Eden占80%的内存空间，每一个Survivor各占10%<br>
<img src="images/jvm_copy_suanfa.png" alt="img_1.png" loading="lazy"></p>
<pre><code>1.刚开始对象分配在eden区，如果eden的内存快满了就会触发垃圾回收。
2.此时就会把eden区存活的对象一次性转移到一块空的survivor区，接着eden区被清空
3.接着再分配对象到eden区，这时eden区和一块survivor是有对象的，其中survivor区存放的是上一次minor GC 存活下来的对象 
4.如果下次eden区满，再次触发minorGC，就会把Eden区和放着上一次minorGC存活对的survivor区内的存活对象转移到另一块survivor区
</code></pre>
<p>因为之前分析了每次垃圾回收，存活下来的对象1%，这样的设计就用100M的空间来存放每次垃圾回收之后存活下来的对象<br>
始终保存一块survivor区是空着的<br>
最大的好处就是只有10% 的空间是空闲的。90%的内存都用上了。<br>
无论是垃圾回收的性能，还是内存碎片的控制，都非常好</p>
<p><strong>对象如何进入老年代</strong></p>
<p>1.15岁规则</p>
<pre><code>默认设置下，当对象的年龄达到15岁的时候，也就是躲过15次GC的时候，他就会转移到老年代里去。
这个具体是多少岁进入老年代，可以通过JVM参数 **-XX:MAXTenuringThreshold**来设置，默认是15岁
</code></pre>
<p>2.动态对象年龄判断:这个规则也会让新生代对象进入老年代</p>
<pre><code>规则逻辑：年龄1+年龄2+年龄3+...+年龄N，对象的总和超过了Survivor区的50%，
此时就会把年龄n以上的对象都放在老年代。
</code></pre>
<p>3.大对象直接进入老年代</p>
<pre><code>有一个JVM参数，就是 -XX:PretenureSizeThreshold可以把它的值设置为字节数，比如 1048576，就是1M。
它的意思就是，如果你要创建一个大于这个大小的对象，比如一个超大的数组，或者别的啥东西，就直接把这个对象放到老年代里去，压根不会经过新生代
之所以这么做，就是要避免新生代里出现那种大对象，然后屡次躲过GC，还得把它在两个survivor来回复制多次才能进入老年代，这么大的一个对象在内存中来回复制不是很耗时吗
</code></pre>
<p><strong>Minor GC 后的对象太多无法存入Survivor区怎么办</strong></p>
<p>这个时候必须要把这些对象发在老年代区中<br>
<img src="images/jvm_kongjiandanbao.png" alt="img.png" loading="lazy"></p>
<p><strong>老年代空间分配担保规则</strong><br>
如果新生代有大量对象存活，确实是自己的Survivor区放不下了，必须转移到老年代中区？、，<br>
那么老年代的空间也不够存放这些对象，那怎么办</p>
<pre><code>首先在执行任何一次minorGC之前，JVM会先检查一下老年代可用的可用空间内存。
为啥要检查呢？如果所有的对象都存活下来，那不是新生代的全部对象都要进入老年代？

如果老年代的内存大小大于新生代的所有对象，此时就可以放心大胆的对新生代发起一起minorGC了，即使minorGC之后，所有的对象都存活下来了，Survivor区放不下，也可以转移到老年代去。
但是假如执行MinorGC之前，发现老年代的可用内存已经小于新生代的全部对象大小了，就会看一个 **-XX:HandlePromotionFailure**的参数是否设置了
    如果有这个参数的设置，就是看看老年代的内存大小，是否大于之前每一次MonirGC之后进入老年代的对象平均大小，就会执行MinorGC
        此时执行minorGC 有三种情况
            1.第一种可能，minorGC 过后，剩余的存活对象的大小，是小于Survivor区的大小的，那么此时存活的对象进入Survivor区即可。
            2.第二种可能， minorGC过后，剩余存活对象的大小，是大于SUrvivor区域的大小，但是小于老年代可能的内存大小的，此时直接进入老年代即可。
            3.第三种可能，很不幸，minorGC过后，剩余存活对象的大小，是大于Survivor区域的大小，也大于老年代可用内存的大小，
                此时老年代都存放不下这些存活的对象，就会发生**HandlePromotionFailure**的情况，这个时候就会触发一次FullGC
               
    如果上诉判断失败了，或者**-XX:HandlePromotionFailure**参数没有设置，就会直接触发一次**Fully GC**,就是对老年代对象进行回收，尽量腾出一些内存来，然后执行 MinorGC

    如果 Full GC 过后，老年代没有足够的空间存放monir GC过后剩余的存活对象，那么此时就会导致所谓的oom，内存溢出了
</code></pre>
<p>Full GC就是对老年代进行垃圾回收，同时也一般会对新生代进行垃圾回收。</p>
<p>2.标记整理算法(老年代的垃圾回收算法)</p>
<pre><code>老年代采用的算法是标记整理算法
</code></pre>
<p>标记存活的对象，把他们尽量的移动到一块，避免垃圾回收有过多的垃圾内存碎片，再一次性把垃圾对象回收掉<br>
值得注意的是，老年代的垃圾回收算法，比新生代的垃圾回收算法慢十倍，如果系统频繁的出现老年代的垃圾回收fullGc，会导致系统性能严重影响甚至出现卡顿。</p>
<p>彻底理解了jvm运行原理就会知道，<strong>所谓jvm优化，就是尽可能的让对象在新生代里分配和回收，尽量避免太多的对象频繁进入老年代，避免频繁对老年代进行垃圾回收，<br>
同时给系统足够的内存大小，避免频繁的对新生代的对象进行垃圾回收。</strong></p>
<h3 id="jvm垃圾回收器">JVM垃圾回收器</h3>
<p><strong>JVM的痛点： stop the world</strong><br>
因为在垃圾回收的时候，尽可能的让垃圾回收器专心致志的工作，不能让我们写的java系统继续创建对象，所以此时jvm后台会进入 stop the world 的状态<br>
也就是说他会直接停止掉我们写的系统的所有工作线程，让我们的代码不在运行，然后让垃圾回收线程专心致志的做垃圾回收的工作。</p>
<figure data-type="image" tabindex="10"><img src="images/stop_the_world.png" alt="img.png" loading="lazy"></figure>
<p>接着一旦垃圾回收完毕，就可以恢复我们java系统工作线程运行了</p>
<p><strong>stop the world 造成的系统停顿</strong></p>
<pre><code>无论是新生代还是老年代，都尽量避免频率过高，也避免持续时间过长，避免影响系统正常运行，这是使用jvm运行过程中最需要优化的一个地方，也是最大的一个痛点。
</code></pre>
<p><strong>不同的垃圾回收器不同的影响</strong></p>
<p>新生代</p>
<pre><code>serial垃圾回收器:就是利用一个线程进行垃圾回收，然后此时停止工作线程，所以我们在服务程序中很少用这种

parNew：平时常用的新生代垃圾回收器，它针对服务器都是多核cpu做了优化，它支持多线程垃圾回收，大幅度提升了回收的性能，缩短回收时间
</code></pre>
<p>老年代</p>
<pre><code>        G1
</code></pre>
<h4 id="parnew垃圾回收器">parNew垃圾回收器</h4>
<p><strong>parNew大致原理图如下</strong></p>
<figure data-type="image" tabindex="11"><img src="images/parNew.png" alt="img.png" loading="lazy"></figure>
<p><strong>如何为线上系统指定使用parNew垃圾收集器</strong></p>
<pre><code>在启动系统里使用参数 -XX:+UseParNewGC
</code></pre>
<p><strong>parNew垃圾回收器默认情况下的线程数量</strong></p>
<pre><code>默认给自己设置的垃圾回收线程的数量就是跟cpu的核数一致
但是你一定要调节parNew垃圾回收器回收线程数量也是可以的，使用 **-XX:ParallelGCThread** 设置参数即可设置它的线程数量，但是建议一般不要随便动这个参数
</code></pre>
<h4 id="cms垃圾回收器">CMS垃圾回收器</h4>
<p><strong>CMS垃圾回收的基本原理</strong></p>
<pre><code>采用的是标记-清理算法
将垃圾对象标记出来，然后清理
这样存在一个问题，就是造成很多垃圾碎片，浪费内存。
</code></pre>
<p>CMS垃圾回收器采用的是垃圾回收线程和工作线程尽可能的同时执行的模式来处理。</p>
<p><strong>CMS如何实现系统一边工作的同时进行垃圾回收</strong></p>
<pre><code>cms在执行垃圾回收的过程一共分为四个阶段：
1.初使标记
2.并发标记
3.重新标记
4.并发清理
</code></pre>
<p>1.首先CMS进行垃圾回收时，会先执行<strong>初始标记阶段</strong>，这个阶段会<strong>让系统全部停止工作</strong>，进入 stop the world 的状态</p>
<p><img src="images/cms_chushibiaoji.png" alt="img.png" loading="lazy"><br>
但其实影响并不大，因为它的速度很快，仅仅是标记GC Root直接引用的对象</p>
<p>2.进入第二阶段，并发标记阶段，这个阶段会让工作线程随意创建各种对象，继续运行<br>
第二阶段就是对老年代所有对象进行GC ROOT 追踪，其实是最耗时的，<br>
它需要追踪所有对象是否从根源上被GC ROOT 引用了，但是这个阶段是最耗时的，是跟系统程序并发进行的，所以这恶搞阶段不行对系统运行造成影响。<br>
3.接着会进入第三阶段，重新标记阶段，<br>
再次进入stop the world 的状态，<br>
这个重新标记阶段速度是很快的，他其实就是对第二阶段中被系统程序运行变动的少数对象进行标记，所以速度是很快的。<br>
4.接着重新恢复系统运行，进入第四阶段：<strong>并发清理</strong><br>
这个阶段让系统随意运行，然后他来清理之前标记的垃圾对象即可。<br>
这个阶段其实很耗时的，因为需要进行垃圾对象的清理，但是它是跟着系统并发进行的，所以其实也不影响系统程序的执行。</p>
<p><strong>对CMS垃圾回收机制进行性能分析</strong><br>
因为最耗时的，其实就是对老年代全部对象进行GC ROOR 跟踪，标记出来到底哪些可以回收，然后就是对各种垃圾对象从内存中清理掉，这是最耗时的。<br>
但是它的第二阶段和第四阶段，都是和系统程序并发执行的，所以基本这两个最耗时的阶段对性能影响不大。<br>
只有第一阶段和第三阶段需要 stop the world 的，但是这两个阶段都是简单的标记，速递非常快，所以基本上对系统的影响也不大。</p>
<p><strong>CMS垃圾回收期间的一些细节问题</strong><br>
并发回收导致CPU资源紧张<br>
CMS垃圾回收器有一个最大的问题，虽然能在垃圾回收的同时让系统工作，但是大家有没有发现，在并发标记和并发清理两个最耗时的阶段，<br>
垃圾回收线程和系统工作线程同时工作，会导致CPU资源被垃圾回收线程占了一部分。</p>
<pre><code>并发标记的时候需要对GC ROOT 进行深度追踪看所有对象里面到底有多少是存活的对象

但是因为老年代里存活的对象比较多，这个过程会追踪大量的对象，所以比较耗时，

并发清理，有需要把垃圾对象从各种随机内存位置清理掉，也是比较耗时的。

所以这两个阶段，CMS的垃圾回收线程比较消耗cpu资源的，CMS默认启动的垃圾回收线程数量是（CPU核数+3）/4 垃圾回收线程
</code></pre>
<p>所以其实CMS这个并发回收机制，第一个问题就是<strong>会消耗CPU资源</strong></p>
<p>Concurrent Mode Failure问题</p>
<pre><code>在并发清理阶段CMS 只不过是回收之前标记好的垃圾对象

但是这个阶段系统一直在运行，可能会随着系统运行让一些对象进入老年代，同时还变成垃圾对象，这种垃圾对象就是 浮动垃圾
因为他虽然成了垃圾，但是CMS只能回收之前标记出来的垃圾对象，不会回收他们，需要等到下一次GC的时候回收他们
</code></pre>
<p>CMS垃圾回收的触发时机，其中一个就是老年代占用内存达到一定比例，就自动执GC</p>
<p><strong>-XX:CMSInitialingOccparencyFaction</strong> 参数可以用来设置老年代占用多少比例的时候触发CMS垃圾回收。<br>
<strong>JDK1.6里面的默认值是92%</strong><br>
也就是说，老年代占用92%空间了，就会自动进行CMS垃圾回收，预留8%的空间给并发回收期间，系统会把一些新对象放入老年代中。</p>
<p>那么如果CMS垃圾回收期间，系统程序要放入老年代的对象大于可用空间对象，此时会如何？</p>
<p>这个时候，会发生 Concurrent Mode Failure</p>
<p>此时，就是直接强行把系统程序 stop the world，重新进行 GC ROOTS 追踪，标记出来全部垃圾对象，不允许新的对象产生</p>
<p>然后把垃圾对象全部回收掉，完事了在恢复系统程序。</p>
<p>所以在生产实践中，这个自动触发CMS垃圾回收的比例需要合理优化一下，避免 Concurrent Mode Failure 问题</p>
<p><strong>内存碎片的问题</strong><br>
CMS不是完全仅仅用<strong>标记-清理</strong>算法，因为太对的内存碎片，实际上会导致更加频繁的fullGC</p>
<p>CMS有一个参数是 <strong>-XX:+UseCMSCompactAtFullCollection</strong> 默认就打开的</p>
<p>它的意思就是在FullGC之后，再次进行 stop the world，停止工作线程，然后进行碎片整理，把存活的对象挪到一起，<br>
空出来大片的连续的内存空间，避免内存碎片。</p>
<p>还有一个参数是 -XX:CMSFullGCsBeforeCompaction ,这个意思是执行多少次FullGC之后，再进行一次内存碎片的整理工作，<br>
默认值是0 ，意思是每一次FullGC之后，都会进行一次内存整理</p>
<p><strong>几个触发老年代GC的时机</strong></p>
<pre><code>1.第一是老年代的可用内存大小小于新生代的全部对象，如果没有开启空间担保参数，就只直接触发FullGC，所以一般空间担保参数都会打开
2.第二是老年代可用的内存大小，小于历次新生代GC后进入老年代的平均大小，此时会提前触发FullGC；
3.第三是新生代MinorGC后存活对象大于Survivor，那么就会进入老年代，此时老年代内存不足
4.-XX:CMSInitialingOccparencyFaction
</code></pre>
<h4 id="g1垃圾回收器">G1垃圾回收器</h4>
<p><strong>parNew+CMS 组合让我们有哪些痛点</strong><br>
stop the world 这是最痛的一个痛点</p>
<p>G1垃圾回收器可以同时回收新生代和老年代的对象，不需要两个两个垃圾回收器配合运作。</p>
<p>他一个人就能搞定所有的垃圾回收。</p>
<p>他最大的特点就是把java堆内存拆分成多个大小相等的Region，</p>
<p>然后G1 也有新生代和老年的概念，只不过是逻辑上概念，也就是说新生代包含了某些region，老年代包含了某些region</p>
<p>而且G1最大的一个特点，就是可以让我们<strong>设置一个垃圾回收的预期停顿时间</strong><br>
在一个时间内，垃圾回收导致的系统停顿时间不能超过多久，G1全权给你负责，保证达到这个目标。</p>
<p><strong>G1是如何做到对垃圾回收导致系统停顿是可控的</strong></p>
<pre><code>其实G1如果要做到这一点，他就必须要追踪每个region里的回收价值，
他必须搞清楚每个region里有多少是垃圾对象，如果对这个region进行垃圾回收，需要消耗多长时间，可以回收掉多少垃圾
最后在垃圾回收的时候，尽量把垃圾回收对系统造成影响控制在你指定的时间范围内，同时在有限的时间内，尽量回收可能更多的垃圾对象。
这就是G1核心设计思路。
</code></pre>
<p><strong>如何设定G1对应的内存大小</strong></p>
<pre><code>我们都知道G1对应的是⼀⼤堆的Region内存区域，每个Region的⼤⼩是⼀致的。
其实这个默认情况下⾃动计算和设置的，我们可以给整个堆内存设置⼀个⼤⼩，⽐如说⽤“-Xms”和“-Xmx”来设置堆内存的⼤⼩。
然后JVM启动的时候⼀旦发现你使⽤的是G1垃圾回收器，可以使⽤“-XX:+UseG1GC”来指定使⽤G1垃圾回收器，此时会⾃动⽤堆⼤⼩
除以2048
因为JVM最多可以有2048个Region，然后Region的⼤⼩必须是2的倍数，⽐如说1MB、2MB、4MB之类的。

⽐如说堆⼤⼩是4G，那么就是4096MB，此时除以2048个Region，每个Region的⼤⼩就是2MB。⼤概就是这样⼦来决定Region的数
量和⼤⼩的，⼤家⼀般保持默认的计算⽅式就可以
如果通过⼿动⽅式来指定，则是“-XX:G1HeapRegionSize”
刚开始的时候，默认新⽣代对堆内存的占⽐是5%，也就是占据200MB左右的内存，对应⼤概是100个Region，这个是可以通过“-
XX:G1NewSizePercent”来设置新⽣代初始占⽐的，其实维持这个默认值即可。
因为在系统运⾏中，JVM其实会不停的给新⽣代增加更多的Region，但是最多新⽣代的占⽐不会超过60%，可以通过“-
XX:G1MaxNewSizePercent”。
⽽且⼀旦Region进⾏了垃圾回收，此时新⽣代的Region数量还会减少，这些其实都是动态的。
</code></pre>
<p><strong>G1的新生代垃圾回收</strong></p>
<pre><code>G1的新生代也有Eden和Survivor的区分那么触发垃圾回收机制是类似的
随着不停的在新生代的Eden对应的Region中放对象，JVM就会不停的给新生代加入更多的Region，直到新生代占据堆内存大小的最大比例60%

这个时候还是会触发新生代GC，G1，就会用用之前的复制算法，来进行垃圾回收，就会进入一个 stop the world 状态
然后Eden对应的Region中存活的对象放在S1对应的Region中，接着回收掉Eden对应的Region中的垃圾对象。

但是这个过程跟之前是有区别的，因为G1是可以设定目标GC停顿时间，也就是G1执行GC的时候最多可以让系统停顿多长时间，可以
通过“-XX:MaxGCPauseMills”参数来设定，默认值是200ms。

那么G1就会通过之前说的，对每个Region追踪回收他需要多少时间，可以回收多少对象来选择回收⼀部分的Region，保证GC停顿时间
控制在指定范围内，尽可能多的回收掉⼀些对象。
</code></pre>
<p><strong>对象什么时候进⼊⽼年代？</strong></p>
<pre><code>⼤家都知道，在G1的内存模型下，新⽣代和⽼年代各⾃都会占据⼀定的Region，⽼年代也会有⾃⼰的Region
按照默认新⽣代最多只能占据堆内存60%的Region来推算，⽼年代最多可以占据40%的Region，⼤概就是800个左右的Region。
那么对象什么时候从新⽣代进⼊⽼年代呢？
可以说跟之前⼏乎是⼀样的，还是这么⼏个条件：

（1）对象在新⽣代躲过了很多次的垃圾回收，达到了⼀定的年龄了，“-XX:MaxTenuringThreshold”参数可以设置这个年龄，他就会进
⼊⽼年代
（2）动态年龄判定规则，如果⼀旦发现某次新⽣代GC过后，存活对象超过了Survivor的50%
此时就会判断⼀下，⽐如年龄为1岁，2岁，3岁，4岁的对象的⼤⼩总和超过了Survivor的50%，此时4岁以上的对象全部会进⼊⽼年
代，这就是动态年龄判定规则
⼤家看下图，所以经过⼀段时间的新⽣代使⽤和垃圾回收之后，总有⼀些对象会进⼊⽼年代中。
</code></pre>
<p><strong>⼤对象Region</strong></p>
<pre><code>⼤家此时可能会疑惑了，唉？以前说是那种⼤对象也是可以直接进⼊⽼年代的，那么现在在G1的这套内存模型下呢？
实际上这⾥会有所改变，G1提供了专⻔的Region来存放⼤对象，⽽不是让⼤对象进⼊⽼年代的Region中。
在G1中，⼤对象的判定规则就是⼀个⼤对象超过了⼀个Region⼤⼩的50%，⽐如按照上⾯算的，每个Region是2MB，只要⼀个⼤对象
超过了1MB，就会被放⼊⼤对象专⻔的Region中
⽽且⼀个⼤对象如果太⼤，可能会横跨多个Region来存放。
https://apppukyptrl1086.pc.xiaoe-tech.com/detail/i_5d11e34610610_u5mTbQgL/1?from=p_5d0ef9900e896_MyDfcJi8&amp;type=6 6/7
肯定还有⼈会问，那堆内存⾥哪些Region⽤来存放⼤对象啊？
不是说60%的给新⽣代，40%的给⽼年代吗，那还有哪些Region给⼤对象？
很简单，之前说过了，在G1⾥，新⽣代和⽼年代的Region是不停的变化的
⽐如新⽣代现在占据了1200个Region，但是⼀次垃圾回收之后，就让⾥⾯1000个Region都空了，此时那1000个Region就可以不属于新
⽣代了，⾥⾯很多Region可以⽤来存放⼤对象。
那么还有⼈会问了，⼤对象既然不属于新⽣代和⽼年代，那么什么时候会触发垃圾回收呢？
也很简单，其实新⽣代、⽼年代在回收的时候，会顺带带着⼤对象Region⼀起回收，所以这就是在G1内存模型下对⼤对象的分配和回
收的策略。
</code></pre>
<p>YoungGC 指年轻代GC<br>
OldGC 指老年代GC<br>
FullGC 指年轻代老年代永久代共同的GC</p>
<p>YoungGC的触发时机<br>
其实一般就是新生代Eden区域满了之后就会触发。采用的是复制算法来回收新生代垃圾</p>
<p>OldGC 和FullGC的触发时机<br>
有3个条件，概括成一句话就是，老年代空间也不够了，没办法放入更多的对象</p>
<p>FullGc 其实就是包含了YoungGC 和OldGC</p>
<p><strong>永久代满了怎么办</strong></p>
<p>假如存放永久代的类信息，常量池满了之后，就会触发一次FullGC。<br>
如果回收之后发现没腾出来更多的地方，只能抛出内存不够的异常。</p>
<h3 id="jvm实战">JVM实战</h3>
<h4 id="自己动手模拟出频繁younggc的场景体验一下">自己动手模拟出频繁YoungGC的场景体验一下！</h4>
<p>https://apppukyptrl1086.pc.xiaoe-tech.com/detail/i_5d11e5a85d6b2_DwhvAoB3/1?from=p_5d0ef9900e896_MyDfcJi8&amp;type=6</p>
<p><strong>如何打印初JVM日志</strong></p>
<ol>
<li>
<p>-XX:+PrintGCDetils：打印详细的gc⽇志</p>
</li>
<li>
<p>-XX:+PrintGCTimeStamps：这个参数可以打印出来每次GC发⽣的时间</p>
</li>
<li>
<p>-Xloggc:gc.log：这个参数可以设置将gc⽇志写⼊⼀个磁盘⽂件</p>
<pre><code> -XX:NewSize=5242880
 -XX:MaxNewSize=5242880
 -XX:InitialHeapSize=10485760
 -XX:MaxHeapSize=10485760
 -XX:SurvivorRatio=8
 -XX:PretenureSizeThreshold=10485760
 -XX:+UseParNewGC
 -XX:+UseConcMarkSweepGC
 -XX:+PrintGCDetails
 -XX:+PrintGCTimeStamps
 -Xloggc:gc.log
</code></pre>
</li>
</ol>
<p><strong>JVM的YoungGC日志</strong></p>
<p>https://apppukyptrl1086.pc.xiaoe-tech.com/detail/i_5d11e5cb74122_FMDnQJY8/1?from=p_5d0ef9900e896_MyDfcJi8&amp;type=6</p>
<h4 id="自己动手模拟出对象进入老年代的场景体验一下">自己动手模拟出对象进入老年代的场景体验一下</h4>
<p>https://apppukyptrl1086.pc.xiaoe-tech.com/detail/i_5d11e5f81ddb5_Lb2e4fEx/1?from=p_5d0ef9900e896_MyDfcJi8&amp;type=6</p>
<p>https://apppukyptrl1086.pc.xiaoe-tech.com/detail/i_5d11e621d622a_fQtn5OrT/1?from=p_5d0ef9900e896_MyDfcJi8&amp;type=6</p>
<p><strong>JVM的Full GC日志</strong></p>
<p>https://apppukyptrl1086.pc.xiaoe-tech.com/detail/i_5d11e648db045_G5rICbYJ/1?from=p_5d0ef9900e896_MyDfcJi8&amp;type=6</p>
<h4 id="使用jstat摸清线上系统的jvm运行状况">使用jstat摸清线上系统的JVM运行状况</h4>
<p>https://apppukyptrl1086.pc.xiaoe-tech.com/detail/i_5d11e6b3c5c6a_K6Xk3Bs1/1?from=p_5d0ef9900e896_MyDfcJi8&amp;type=6</p>
<p>平常我们对运行中的系统，如果要检查他的JVM整体的运行情况，比较实用的工具就是 jstat</p>
<p><strong>jstat -gc PID</strong><br>
首先第一个命令就是在你们生产的机器的linux上，找到你们java进程的PID</p>
<p>运⾏这个命令之后会看到如下列，给⼤家解释⼀下：</p>
<ol>
<li>S0C：这是From Survivor区的⼤⼩</li>
<li>S1C：这是To Survivor区的⼤⼩</li>
<li>S0U：这是From Survivor区当前使⽤的内存⼤⼩</li>
<li>S1U：这是To Survivor区当前使⽤的内存⼤⼩</li>
<li>EC：这是Eden区的⼤⼩</li>
<li>EU：这是Eden区当前使⽤的内存⼤⼩</li>
<li>OC：这是⽼年代的⼤⼩</li>
<li>OU：这是⽼年代当前使⽤的内存⼤⼩</li>
<li>MC：这是⽅法区（永久代、元数据区）的⼤⼩</li>
<li>MU：这是⽅法区（永久代、元数据区）的当前使⽤的内存⼤⼩</li>
<li>YGC：这是系统运⾏迄今为⽌的Young GC次数</li>
<li>YGCT：这是Young GC的耗时</li>
<li>FGC：这是系统运⾏迄今为⽌的Full GC次数</li>
<li>FGCT：这是Full GC的耗时</li>
<li>GCT：这是所有GC的总耗时</li>
</ol>
<p><strong>其他的jstat命令</strong></p>
<ol>
<li>jstat -gccapacity PID：堆内存分析</li>
<li>jstat -gcnew PID：年轻代GC分析，这⾥的TT和MTT可以看到对象在年轻代存活的年龄和存活的最⼤年龄</li>
<li>jstat -gcnewcapacity PID：年轻代内存分析</li>
<li>jstat -gcold PID：⽼年代GC分析</li>
<li>jstat -gcoldcapacity PID：⽼年代内存分析</li>
<li>jstat -gcmetacapacity PID：元数据区内存分析</li>
</ol>
<p><strong>到底该如何使用jstat工具</strong></p>
<p>新生代对象的增长速率</p>
<p>jstat -gc 1000 10 ：每隔1s更新最新的一行jstat统计信息，一共执行10次jstat</p>
<p><strong>Young GC的触发频率和每次耗时</strong></p>
<p><strong>每次Young GC 后多少对象是存活的和进入老年代的</strong></p>
<h4 id="使用jmap和jhat摸清线上系统的对象分布">使用jmap和jhat摸清线上系统的对象分布</h4>
<p>使用jstat就可以非常轻松便捷的了解到线上系统的运行状态，从对象增速，Young GC 触发频率以及耗时，再到对象进入老年代的增速，以及Full GC 触发频率以及耗时，可以完全摸清楚线上系统JVM运行情况</p>
<p>到底什么对象占据了这么大的内存</p>
<p>先看一个命令 <strong>jmap -heap PID</strong></p>
<p>使用jmap 了解系统运行时对象分布式</p>
<p>jmap -histo PID :如果想了解JVM中对象内存的占用情况</p>
<p>使用jmap生成堆内存存储快照</p>
<p>jmap -dump:live,format=b,file=dump.hprof PID</p>
<p>这个命令会在当前目录下生成一个dump.hprof文件，这里是一个二进制文件，他把 这一时刻JVM堆内存里所有对象的快照都放到这个文件中<br>
供后续分析</p>
<p>5.使用jhat 在浏览器中分析堆转出快照<br>
使用如下命令可以启动jhat服务器，还可以指定自己想要的http端口号，默认好似7000端口<br>
jhat dump.hprof -port 7000</p>
<p>如何分析JVM运行状况及合理优化？</p>
<p>优化思路其实简单来说就是一句话：<br>
尽量让每次Young GC 存活的对象小于survivor区域的50Z5，都留在年轻代里；尽量别让对象进入老年代。<br>
尽量减少fullGC的频率，避免Full GC对JVM性能的影响。</p>
<p>比较 常用的监控系统：<br>
Zabbix OpenFalcon Ganglia 等等</p>
<p>然后部署你的系统，把JVM的监控项发送到这些监控系统中，此时你就可以在这些监控系统中可视化的界面看到你需要的指标数据</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dubbo]]></title>
        <id>https://zuolinlin.github.io/zuo.github.io/post/dubbo/</id>
        <link href="https://zuolinlin.github.io/zuo.github.io/post/dubbo/">
        </link>
        <updated>2022-04-08T13:39:07.000Z</updated>
        <content type="html"><![CDATA[<h1 id="dubbo">Dubbo</h1>
<h2 id="目录">目录</h2>
<ul>
<li><a href="#%E7%AE%80%E4%BB%8B">简介</a>
<ul>
<li><a href="#%E7%AE%80%E4%BB%8B">简介</a></li>
</ul>
</li>
<li><a href="#%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%9E%B6%E6%9E%84">概念与架构</a>
<ul>
<li><a href="#%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0">服务发现</a></li>
<li><a href="#%E5%8D%8F%E8%AE%AE">协议</a></li>
<li><a href="#%E6%B5%81%E9%87%8F%E7%AE%A1%E7%90%86">流量管理</a></li>
<li><a href="#%E9%85%8D%E7%BD%AE">配置</a></li>
<li><a href="#%E9%83%A8%E7%BD%B2%E6%9E%B6%E6%9E%84">部署架构</a></li>
<li><a href="#%E6%89%A9%E5%B1%95%E6%80%A7">扩展性</a></li>
</ul>
</li>
</ul>
<h1 id="目录-2">目录</h1>
<h2 id="简介">简介</h2>
<p>Apache Dubbo 是一款微服务开发框架，它提供了 RPC通信 与 微服务治理 两大关键能力。<br>
这意味着，使用 Dubbo 开发的微服务，将具备相互之间的远程发现与通信能力， 同时利用 Dubbo 提供的丰富服务治理能力，可以实现诸如服务发现、负载均衡、流量调度等服务治理诉求。<br>
同时 Dubbo 是高度可扩展的，用户几乎可以在任意功能点去定制自己的实现，以改变框架的默认行为来满足自己的业务需求。</p>
<p>全新服务发现模型</p>
<pre><code>相比于 2.x 版本中的基于接口粒度的服务发现机制，3.x 引入了全新的基于应用粒度的服务发现机制， 新模型带来两方面的巨大优势：

进一步提升了 Dubbo3 在大规模集群实践中的性能与稳定性。新模型可大幅提高系统资源利用率，降低 Dubbo 地址的单机内存消耗（50%），降低注册中心集群的存储与推送压力（90%）， Dubbo 可支持集群规模步入百万实例层次。
打通与其他异构微服务体系的地址互发现障碍。新模型使得 Dubbo3 能实现与异构微服务体系如Spring Cloud、Kubernetes Service、gRPC 等，在地址发现层面的互通， 为连通 Dubbo 与其他微服务体系提供可行方案。
在 Dubbo3 前期版本将会同时提供对两套地址发现模型的支持，以最大程度保证业务升级的兼容性。
</code></pre>
<p>一站式微服务解决方案</p>
<p>Dubbo 提供了从服务定义、服务发现、服务通信到流量管控等几乎所有的服务治理能力，并且尝试从使用上对用户屏蔽底层细节，以提供更好的易用性。</p>
<p>定义服务在 Dubbo 中非常简单与直观，可以选择使用与某种语言绑定的方式（如 Java 中可直接定义 Interface），也可以使用 Protobuf IDL 语言中立的方式。无论选择哪种方式，站在服务消费方的视角，都可以通过 Dubbo 提供的透明代理直接编码。</p>
<p>需要注意的是，在 Dubbo 中，我们提到服务时，通常是指 RPC 粒度的、提供某个具体业务增删改功能的接口或方法，与一些微服务概念书籍中泛指的服务并不是一个概念。</p>
<p>点对点的服务通信是 Dubbo 提供的另一项基本能力，Dubbo 以 RPC 的方式将请求数据（Request）发送给后端服务，并接收服务端返回的计算结果（Response）。RPC 通信对用户来说是完全透明的，使用者无需关心请求是如何发出去的、发到了哪里，每次调用只需要拿到正确的调用结果就行。同步的 Request-Response 是默认的通信模型，它最简单但却不能覆盖所有的场景，因此，Dubbo 提供更丰富的通信模型：</p>
<p>消费端异步请求(Client Side Asynchronous Request-Response)<br>
提供端异步执行（Server Side Asynchronous Request-Response）<br>
消费端请求流（Request Streaming）<br>
提供端响应流（Response Streaming）<br>
双向流式通信（Bidirectional Streaming）<br>
Dubbo 的服务发现机制，让微服务组件之间可以独立演进并任意部署，消费端可以在无需感知对端部署位置与 IP 地址的情况下完成通信。Dubbo 提供的是 Client-Based 的服务发现机制，使用者可以有多种方式启用服务发现：</p>
<p>使用独立的注册中心组件，如 Nacos、Zookeeper、Consul、Etcd 等。<br>
将服务的组织与注册交给底层容器平台，如 Kubernetes，这被理解是一种更云原生的方式</p>
<h2 id="概念与架构">概念与架构</h2>
<h3 id="服务发现">服务发现</h3>
<p>服务发现<br>
服务发现，即消费端自动发现服务地址列表的能力，是微服务框架需要具备的关键能力，借助于自动化的服务发现，微服务之间可以在无需感知对端部署位置与 IP 地址的情况下实现通信。</p>
<p>实现服务发现的方式有很多种，Dubbo 提供的是一种 Client-Based 的服务发现机制，通常还需要部署额外的第三方注册中心组件来协调服务发现过程，如常用的 Nacos、Consul、Zookeeper 等，Dubbo 自身也提供了对多种注册中心组件的对接，用户可以灵活选择。</p>
<p>Dubbo 基于消费端的自动服务发现能力，其基本工作原理如下图：</p>
<figure data-type="image" tabindex="1"><img src="./images/dubbo_architecture.png" alt="dubbo_architecture" loading="lazy"></figure>
<p>服务发现的一个核心组件是注册中心，Provider 注册地址到注册中心，Consumer 从注册中心读取和订阅 Provider 地址列表。 因此，要启用服务发现，需要为 Dubbo 增加注册中心配置：</p>
<p>以 dubbo-spring-boot-starter 使用方式为例，增加 registry 配置</p>
<pre><code class="language-yml"># application.properties
  dubbo
  registry
address: zookeeper://127.0.0.1:2181
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[springboot]]></title>
        <id>https://zuolinlin.github.io/zuo.github.io/post/springboot/</id>
        <link href="https://zuolinlin.github.io/zuo.github.io/post/springboot/">
        </link>
        <updated>2022-04-08T13:32:48.000Z</updated>
        <content type="html"><![CDATA[<h1 id="springboot">springboot</h1>
<h2 id="目录">目录</h2>
<ul>
<li><a href="#SpringBoot2%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF-%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8">SpringBoot2核心技术-基础入门</a>
<ul>
<li><a href="#Spring%E4%B8%8ESpringBoot">Spring与SpringBoot？</a></li>
<li><a href="#SpringBoot2%E5%85%A5%E9%97%A8">SpringBoot2入门</a></li>
<li><a href="#%E8%87%AA%E5%8A%A8%E9%85%8D%E7%BD%AE%E5%8E%9F%E7%90%86">自动配置原理</a></li>
</ul>
</li>
<li><a href="#SpringBoot2%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF-%E6%A0%B8%E5%BF%83%E5%8A%9F%E8%83%BD">SpringBoot2核心技术-核心功能</a>
<ul>
<li><a href="#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">配置文件</a></li>
<li><a href="#Web%E5%BC%80%E5%8F%91">Web开发</a></li>
<li><a href="#Web%E5%BC%80%E5%8F%91">Web开发</a></li>
<li><a href="#%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95">单元测试</a></li>
<li><a href="#%E6%8C%87%E6%A0%87%E7%9B%91%E6%8E%A7">指标监控</a></li>
<li><a href="#%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90">原理解析</a></li>
</ul>
</li>
<li><a href="#SpringBoot2%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF-%E5%9C%BA%E6%99%AF%E6%95%B4%E5%90%88">SpringBoot2核心技术-场景整合</a></li>
</ul>
<h1 id="目录-2">目录</h1>
<h2 id="springboot2核心技术-基础入门">SpringBoot2核心技术-基础入门</h2>
<h3 id="自动配置原理">自动配置原理</h3>
<h4 id="依赖管理">依赖管理</h4>
<p>父项目做依赖管理</p>
<pre><code class="language-xml">&lt;!--依赖管理 --&gt;
&lt;parent&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
    &lt;version&gt;2.3.4.RELEASE&lt;/version&gt;
&lt;/parent&gt;

        &lt;!--他的父项目 --&gt;
&lt;parent&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt;
&lt;version&gt;2.3.4.RELEASE&lt;/version&gt;
&lt;/parent&gt;

        &lt;!--几乎声明了所有开发中常用的依赖的版本号,自动版本仲裁机制 --&gt;

</code></pre>
<p>开发导入starter场景启动器</p>
<pre><code class="language-xml">&lt;!--
1、见到很多 spring-boot-starter-* ： *就某种场景
2、只要引入starter，这个场景的所有常规需要的依赖我们都自动引入
3、SpringBoot所有支持的场景
https://docs.spring.io/spring-boot/docs/current/reference/html/using-spring-boot.html#using-boot-starter
4、见到的  *-spring-boot-starter： 第三方为我们提供的简化开发的场景启动器。
5、所有场景启动器最底层的依赖
--&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;
    &lt;version&gt;2.3.4.RELEASE&lt;/version&gt;
    &lt;scope&gt;compile&lt;/scope&gt;
&lt;/dependency&gt;
</code></pre>
<p>无需关注版本号，自动版本仲裁</p>
<pre><code> 1、引入依赖默认都可以不写版本
 2、引入非版本仲裁的jar，要写版本号。
</code></pre>
<p>可以修改默认版本号</p>
<pre><code class="language-xml">&lt;!--
1、查看spring-boot-dependencies里面规定当前依赖的版本 用的 key。
2、在当前项目里面重写配置
--&gt;
&lt;properties&gt;
    &lt;mysql.version&gt;5.1.43&lt;/mysql.version&gt;
&lt;/properties&gt;
</code></pre>
<h4 id="自动配置">自动配置</h4>
<p>● 自动配好Tomcat<br>
○ 引入Tomcat依赖。<br>
○ 配置Tomcat</p>
<pre><code class="language-xml">
&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt;
    &lt;version&gt;2.3.4.RELEASE&lt;/version&gt;
    &lt;scope&gt;compile&lt;/scope&gt;
&lt;/dependency&gt;
</code></pre>
<p>● 自动配好SpringMVC</p>
<ul>
<li>
<p>引入SpringMVC全套组件</p>
</li>
<li>
<p>自动配好SpringMVC常用组件（功能）</p>
</li>
</ul>
<p>● 自动配好Web常见功能，如：字符编码问题</p>
<ul>
<li>SpringBoot帮我们配置好了所有web开发的常见场景</li>
</ul>
<p>● 默认的包结构</p>
<ul>
<li>主程序所在包及其下面的所有子包里面的组件都会被默认扫描进来</li>
<li>无需以前的包扫描配置</li>
<li>想要改变扫描路径，@SpringBootApplication(scanBasePackages=&quot;com.atguigu&quot;)</li>
</ul>
<p>■ 或者@ComponentScan 指定扫描路径</p>
<pre><code>@SpringBootApplication
等同于
@SpringBootConfiguration
@EnableAutoConfiguration
@ComponentScan(&quot;com.atguigu.boot&quot;)
</code></pre>
<p>● 各种配置拥有默认值</p>
<ul>
<li>默认配置最终都是映射到某个类上，如：MultipartProperties</li>
<li>配置文件的值最终会绑定每个类上，这个类会在容器中创建对象</li>
</ul>
<p>● 按需加载所有自动配置项</p>
<ul>
<li>非常多的starter</li>
<li>引入了哪些场景这个场景的自动配置才会开启</li>
<li>SpringBoot所有的自动配置功能都在 spring-boot-autoconfigure 包里面</li>
</ul>
<p>● ......</p>
<h4 id="容器功能">容器功能</h4>
<h5 id="组件添加">组件添加</h5>
<h6 id="configuration">@Configuration</h6>
<p>● 基本使用<br>
● Full模式与Lite模式</p>
<ul>
<li>示例</li>
<li>最佳实战</li>
</ul>
<p>■ 配置 类组件之间无依赖关系用Lite模式加速容器启动过程，减少判断</p>
<p>■ 配置类组件之间有依赖关系，方法会被调用得到之前单实例组件，用Full模式</p>
<pre><code class="language-java">#############################Configuration使用示例######################################################

/**
 * 1、配置类里面使用@Bean标注在方法上给容器注册组件，默认也是单实例的
 * 2、配置类本身也是组件
 * 3、proxyBeanMethods：代理bean的方法
 *      Full(proxyBeanMethods = true)、【保证每个@Bean方法被调用多少次返回的组件都是单实例的】
 *      Lite(proxyBeanMethods = false)【每个@Bean方法被调用多少次返回的组件都是新创建的】
 *      组件依赖必须使用Full模式默认。其他默认是否Lite模式
 *
 *
 *
 */
@Configuration(proxyBeanMethods = false) //告诉SpringBoot这是一个配置类 == 配置文件
public class MyConfig {

    /**
     * Full:外部无论对配置类中的这个组件注册方法调用多少次获取的都是之前注册容器中的单实例对象
     * @return
     */
    @Bean //给容器中添加组件。以方法名作为组件的id。返回类型就是组件类型。返回的值，就是组件在容器中的实例
    public User user01() {
        User zhangsan = new User(&quot;zhangsan&quot;, 18);
        //user组件依赖了Pet组件
        zhangsan.setPet(tomcatPet());
        return zhangsan;
    }

    @Bean(&quot;tom&quot;)
    public Pet tomcatPet() {
        return new Pet(&quot;tomcat&quot;);
    }
}


################################@Configuration测试代码如下########################################

@SpringBootConfiguration
@EnableAutoConfiguration
@ComponentScan(&quot;com.atguigu.boot&quot;)
public class MainApplication {

    public static void main(String[] args) {
        //1、返回我们IOC容器
        ConfigurableApplicationContext run = SpringApplication.run(MainApplication.class, args);

        //2、查看容器里面的组件
        String[] names = run.getBeanDefinitionNames();
        for (String name : names) {
            System.out.println(name);
        }

        //3、从容器中获取组件

        Pet tom01 = run.getBean(&quot;tom&quot;, Pet.class);

        Pet tom02 = run.getBean(&quot;tom&quot;, Pet.class);

        System.out.println(&quot;组件：&quot; + (tom01 == tom02));


        //4、com.atguigu.boot.config.MyConfig$$EnhancerBySpringCGLIB$$51f1e1ca@1654a892
        MyConfig bean = run.getBean(MyConfig.class);
        System.out.println(bean);

        //如果@Configuration(proxyBeanMethods = true)代理对象调用方法。SpringBoot总会检查这个组件是否在容器中有。
        //保持组件单实例
        User user = bean.user01();
        User user1 = bean.user01();
        System.out.println(user == user1);


        User user01 = run.getBean(&quot;user01&quot;, User.class);
        Pet tom = run.getBean(&quot;tom&quot;, Pet.class);

        System.out.println(&quot;用户的宠物：&quot; + (user01.getPet() == tom));


    }
}


</code></pre>
<h6 id="bean-component-controller-service-repository">@Bean、@Component、@Controller、@Service、@Repository</h6>
<h5 id="componentscan-import">@ComponentScan、@Import</h5>
<pre><code class="language-java">/*
 * 4、@Import({User.class, DBHelper.class})
 *      给容器中自动创建出这两个类型的组件、默认组件的名字就是全类名
 *
 *
 *
 */

@Import({User.class, DBHelper.class})
@Configuration(proxyBeanMethods = false) //告诉SpringBoot这是一个配置类 == 配置文件
public class MyConfig {
}
</code></pre>
<p>[@Import 高级用法：](@Import 高级用法： https://www.bilibili.com/video/BV1gW411W7wy?p=8)</p>
<h6 id="conditional">@Conditional</h6>
<p>条件装配：满足Conditional指定的条件，则进行组件注入<br>
<img src="img_1.png" alt="img_1.png" loading="lazy"></p>
<pre><code class="language-java">=====================测试条件装配==========================

@Configuration(proxyBeanMethods = false) //告诉SpringBoot这是一个配置类 == 配置文件
//@ConditionalOnBean(name = &quot;tom&quot;)
@ConditionalOnMissingBean(name = &quot;tom&quot;)
public class MyConfig {


    /**
     * Full:外部无论对配置类中的这个组件注册方法调用多少次获取的都是之前注册容器中的单实例对象
     * @return
     */

    @Bean //给容器中添加组件。以方法名作为组件的id。返回类型就是组件类型。返回的值，就是组件在容器中的实例
    public User user01() {
        User zhangsan = new User(&quot;zhangsan&quot;, 18);
        //user组件依赖了Pet组件
        zhangsan.setPet(tomcatPet());
        return zhangsan;
    }

    @Bean(&quot;tom22&quot;)
    public Pet tomcatPet() {
        return new Pet(&quot;tomcat&quot;);
    }
}

    public static void main(String[] args) {
        //1、返回我们IOC容器
        ConfigurableApplicationContext run = SpringApplication.run(MainApplication.class, args);

        //2、查看容器里面的组件
        String[] names = run.getBeanDefinitionNames();
        for (String name : names) {
            System.out.println(name);
        }

        boolean tom = run.containsBean(&quot;tom&quot;);
        System.out.println(&quot;容器中Tom组件：&quot; + tom);

        boolean user01 = run.containsBean(&quot;user01&quot;);
        System.out.println(&quot;容器中user01组件：&quot; + user01);

        boolean tom22 = run.containsBean(&quot;tom22&quot;);
        System.out.println(&quot;容器中tom22组件：&quot; + tom22);


    }
</code></pre>
<h5 id="原生配置文件引入">原生配置文件引入</h5>
<h6 id="importresource">@ImportResource</h6>
<pre><code class="language-xml">======================beans.xml=========================
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;
       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
       xmlns:context=&quot;http://www.springframework.org/schema/context&quot;
       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd&quot;&gt;

    &lt;bean id=&quot;haha&quot; class=&quot;com.atguigu.boot.bean.User&quot;&gt;
        &lt;property name=&quot;name&quot; value=&quot;zhangsan&quot;&gt;&lt;/property&gt;
        &lt;property name=&quot;age&quot; value=&quot;18&quot;&gt;&lt;/property&gt;
    &lt;/bean&gt;

    &lt;bean id=&quot;hehe&quot; class=&quot;com.atguigu.boot.bean.Pet&quot;&gt;
        &lt;property name=&quot;name&quot; value=&quot;tomcat&quot;&gt;&lt;/property&gt;
    &lt;/bean&gt;
&lt;/beans&gt;
</code></pre>
<pre><code class="language-java">@ImportResource(&quot;classpath:beans.xml&quot;)
public class MyConfig {}

======================测试=================
        boolean haha = run.containsBean(&quot;haha&quot;);
        boolean hehe = run.containsBean(&quot;hehe&quot;);
        System.out.println(&quot;haha：&quot;+haha);//true
        System.out.println(&quot;hehe：&quot;+hehe);//true
</code></pre>
<h6 id="配置绑定">配置绑定</h6>
<p>如何使用Java读取到properties文件中的内容，并且把它封装到JavaBean中，以供随时使用；</p>
<pre><code class="language-java">
public class getProperties {
     public static void main(String[] args) throws FileNotFoundException, IOException {
         Properties pps = new Properties();
         pps.load(new FileInputStream(&quot;a.properties&quot;));
         Enumeration enum1 = pps.propertyNames();//得到配置文件的名字
         while(enum1.hasMoreElements()) {
             String strKey = (String) enum1.nextElement();
             String strValue = pps.getProperty(strKey);
             System.out.println(strKey + &quot;=&quot; + strValue);
             //封装到JavaBean。
         }
     }
 }
</code></pre>
<h6 id="configurationproperties">@ConfigurationProperties</h6>
<pre><code class="language-java">/**
 * 只有在容器中的组件，才会拥有SpringBoot提供的强大功能
 */
@Component
@ConfigurationProperties(prefix = &quot;mycar&quot;)
public class Car {

    private String brand;
    private Integer price;

    public String getBrand() {
        return brand;
    }

    public void setBrand(String brand) {
        this.brand = brand;
    }

    public Integer getPrice() {
        return price;
    }

    public void setPrice(Integer price) {
        this.price = price;
    }

    @Override
    public String toString() {
        return &quot;Car{&quot; +
                &quot;brand='&quot; + brand + '\'' +
                &quot;, price=&quot; + price +
                '}';
    }
}
</code></pre>
<h6 id="enableconfigurationproperties-configurationproperties">@EnableConfigurationProperties + @ConfigurationProperties</h6>
<h6 id="component-configurationproperties">@Component + @ConfigurationProperties</h6>
<pre><code class="language-java">@EnableConfigurationProperties(Car.class)
//1、开启Car配置绑定功能
//2、把这个Car这个组件自动注册到容器中
public class MyConfig {
}
</code></pre>
<h5 id="自动配置原理入门">自动配置原理入门</h5>
<p>引导加载自动配置类</p>
<pre><code class="language-java">@SpringBootConfiguration
@EnableAutoConfiguration
@ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class),
		@Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) })
public @interface SpringBootApplication{}


======================
    
</code></pre>
<h6 id="springbootconfiguration">@SpringBootConfiguration</h6>
<pre><code>@Configuration。代表当前是一个配置类
</code></pre>
<h6 id="componentscan">@ComponentScan</h6>
<pre><code>指定扫描哪些，Spring注解；
</code></pre>
<h6 id="enableautoconfiguration">@EnableAutoConfiguration</h6>
<pre><code class="language-java">@AutoConfigurationPackage
@Import(AutoConfigurationImportSelector.class)
public @interface EnableAutoConfiguration {}
</code></pre>
<h6 id="autoconfigurationpackage">@AutoConfigurationPackage</h6>
<pre><code>自动配置包？指定了默认的包规则
</code></pre>
<pre><code class="language-java">@Import(AutoConfigurationPackages.Registrar.class)  //给容器中导入一个组件
public @interface AutoConfigurationPackage {}

//利用Registrar给容器中导入一系列组件
//将指定的一个包下的所有组件导入进来？MainApplication 所在包下。

</code></pre>
<h6 id="importautoconfigurationimportselectorclass">@Import(AutoConfigurationImportSelector.class)</h6>
<pre><code>1、利用getAutoConfigurationEntry(annotationMetadata);给容器中批量导入一些组件
2、调用List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes)获取到所有需要导入到容器中的配置类
3、利用工厂加载 Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader)；得到所有的组件
4、从META-INF/spring.factories位置来加载一个文件。
默认扫描我们当前系统里面所有META-INF/spring.factories位置的文件
spring-boot-autoconfigure-2.3.4.RELEASE.jar包里面也有META-INF/spring.factories
</code></pre>
<h6 id="按需开启自动配置项">按需开启自动配置项</h6>
<pre><code>虽然我们127个场景的所有自动配置启动的时候默认全部加载。xxxxAutoConfiguration
按照条件装配规则（@Conditional），最终会按需配置。
</code></pre>
<h6 id="修改默认配置">修改默认配置</h6>
<pre><code class="language-java">        @Bean
		@ConditionalOnBean(MultipartResolver.class)  //容器中有这个类型组件
		@ConditionalOnMissingBean(name = DispatcherServlet.MULTIPART_RESOLVER_BEAN_NAME) //容器中没有这个名字 multipartResolver 的组件
		public MultipartResolver multipartResolver(MultipartResolver resolver) {
            //给@Bean标注的方法传入了对象参数，这个参数的值就会从容器中找。
            //SpringMVC multipartResolver。防止有些用户配置的文件上传解析器不符合规范
			// Detect if the user has created a MultipartResolver but named it incorrectly
			return resolver;
		}
给容器中加入了文件上传解析器；

</code></pre>
<h6 id="springboot默认会在底层配好所有的组件-但是如果用户自己配置了以用户的优先">SpringBoot默认会在底层配好所有的组件。但是如果用户自己配置了以用户的优先</h6>
<pre><code class="language-java">@Bean
	@ConditionalOnMissingBean
	public CharacterEncodingFilter characterEncodingFilter() {
    }

</code></pre>
<p>总结：<br>
● SpringBoot先加载所有的自动配置类  xxxxxAutoConfiguration<br>
● 每个自动配置类按照条件进行生效，默认都会绑定配置文件指定的值。xxxxProperties里面拿。xxxProperties和配置文件进行了绑定<br>
● 生效的配置类就会给容器中装配很多组件<br>
● 只要容器中有这些组件，相当于这些功能就有了<br>
● 定制化配置<br>
○ 用户直接自己@Bean替换底层的组件<br>
○ 用户去看这个组件是获取的配置文件什么值就去修改。<br>
xxxxxAutoConfiguration ---&gt; 组件  ---&gt; xxxxProperties里面拿值  ----&gt; application.properties</p>
<h6 id="最佳实践">最佳实践</h6>
<p>● 引入场景依赖</p>
<pre><code>○ https://docs.spring.io/spring-boot/docs/current/reference/html/using-spring-boot.html#using-boot-starter
</code></pre>
<p>● 查看自动配置了哪些（选做）</p>
<ul>
<li>自己分析，引入场景对应的自动配置一般都生效了</li>
<li>配置文件中debug=true开启自动配置报告。Negative（不生效）\Positive（生效）</li>
</ul>
<p>● 是否需要修改</p>
<ul>
<li>参照文档修改配置项</li>
<li>https://docs.spring.io/spring-boot/docs/current/reference/html/appendix-application-properties.html#common-application-properties</li>
</ul>
<p>■ 自己分析。xxxxProperties绑定了配置文件的哪些。</p>
<ul>
<li>自定义加入或者替换组件<br>
■ @Bean、@Component。。。</li>
<li></li>
<li>自定义器  XXXXXCustomizer</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[枚举]]></title>
        <id>https://zuolinlin.github.io/zuo.github.io/post/mei-ju/</id>
        <link href="https://zuolinlin.github.io/zuo.github.io/post/mei-ju/">
        </link>
        <updated>2022-04-08T13:31:28.000Z</updated>
        <content type="html"><![CDATA[<h1 id="java基础知识">Java基础知识</h1>
<h2 id="目录">目录</h2>
<ul>
<li><a href="#%E6%9E%9A%E4%B8%BE">枚举</a>
<ul>
<li><a href="#%E7%AE%80%E4%BB%8B">简介</a></li>
<li><a href="#%E6%9E%9A%E4%B8%BE%E7%9A%84%E6%9C%AC%E8%B4%A8">枚举的本质</a></li>
<li><a href="#%E6%9E%9A%E4%B8%BE%E7%9A%84%E6%96%B9%E6%B3%95">枚举的方法</a></li>
<li><a href="#%E6%9E%9A%E4%B8%BE%E7%9A%84%E7%89%B9%E5%BE%81">枚举的特征</a>
<ul>
<li><a href="#%E5%9F%BA%E6%9C%AC%E7%89%B9%E5%BE%81">基本特征</a></li>
<li><a href="#%E6%9E%9A%E4%B8%BE%E5%8F%AF%E4%BB%A5%E6%B7%BB%E5%8A%A0%E6%96%B9%E6%B3%95">枚举可以添加方法</a></li>
<li><a href="#%E6%9E%9A%E4%B8%BE%E5%8F%AF%E4%BB%A5%E5%AE%9E%E7%8E%B0%E6%8E%A5%E5%8F%A3">枚举可以实现接口</a></li>
<li><a href="#%E6%9E%9A%E4%B8%BE%E4%B8%8D%E5%8F%AF%E4%BB%A5%E7%BB%A7%E6%89%BF">枚举不可以继承</a></li>
</ul>
</li>
<li><a href="#%E6%9E%9A%E4%B8%BE%E7%9A%84%E5%BA%94%E7%94%A8">枚举的应用</a>
<ul>
<li><a href="#%E7%BB%84%E7%BB%87%E5%B8%B8%E9%87%8F">组织常量</a></li>
<li><a href="#switch%E7%8A%B6%E6%80%81%E6%9C%BA">switch状态机</a></li>
<li><a href="#%E9%94%99%E8%AF%AF%E7%A0%81">错误码</a></li>
<li><a href="#%E7%BB%84%E7%BB%87%E6%9E%9A%E4%B8%BE">组织枚举</a></li>
<li><a href="#%E7%AD%96%E7%95%A5%E6%9E%9A%E4%B8%BE">策略枚举</a></li>
<li><a href="#%E6%9E%9A%E4%B8%BE%E5%AE%9E%E7%8E%B0%E5%8D%95%E7%B2%92%E6%A8%A1%E5%BC%8F">枚举实现单粒模式</a></li>
</ul>
</li>
<li><a href="#%E6%9E%9A%E4%B8%BE%E5%B7%A5%E5%85%B7%E7%B1%BB">枚举工具类</a>
<ul>
<li><a href="#EnumSet">EnumSet</a></li>
<li><a href="#EnumMap">EnumMap</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="目录-2">目录</h1>
<h2 id="枚举">枚举</h2>
<h3 id="简介">简介</h3>
<p>enum 的全称为 enumeration， 是 JDK5 中引入的特性。</p>
<p>在 Java 中，被 enum 关键字修饰的类型就是枚举类型。形式如下：</p>
<pre><code class="language-java">enum ColorEn { RED, GREEN, BLUE }
</code></pre>
<p>枚举的好处：可以将常量组织起来，统一进行管理。</p>
<p>枚举的典型应用场景：错误码、状态机等。</p>
<h3 id="枚举的本质">枚举的本质</h3>
<p>枚举的本质是 java.lang.Enum 的子类。</p>
<p>尽管 enum 看起来像是一种新的数据类型，事实上，enum 是一种受限制的类，并且具有自己的方法。枚举这种特殊的类因为被修饰为 final，所以不能继承其他类。</p>
<p>定义的枚举值，会被默认修饰为 public static final ，从修饰关键字，即可看出枚举值本质上是静态常量。</p>
<h3 id="枚举的方法">枚举的方法</h3>
<p>在 enum 中，提供了一些基本方法：</p>
<p>values()：返回 enum 实例的数组，而且该数组中的元素严格保持在 enum 中声明时的顺序。<br>
name()：返回实例名。<br>
ordinal()：返回实例声明时的次序，从 0 开始。<br>
getDeclaringClass()：返回实例所属的 enum 类型。<br>
equals() ：判断是否为同一个对象。<br>
可以使用 == 来比较enum实例。</p>
<p>此外，java.lang.Enum实现了Comparable和 Serializable 接口，所以也提供 compareTo() 方法。</p>
<h3 id="枚举的特征">枚举的特征</h3>
<p>枚举的特性，归结起来就是一句话：</p>
<pre><code>除了不能继承，基本上可以将 enum 看做一个常规的类。
</code></pre>
<h4 id="基本特征">基本特征</h4>
<p>如果枚举中没有定义方法，也可以在最后一个实例后面加逗号、分号或什么都不加。</p>
<p>如果枚举中没有定义方法，枚举值默认为从 0 开始的有序数值。以 Color 枚举类型举例，它的枚举常量依次为 RED：0，GREEN：1，BLUE：2</p>
<h4 id="枚举可以添加方法">枚举可以添加方法</h4>
<p>1）Java 不允许使用 = 为枚举常量赋值<br>
2）枚举可以添加普通方法、静态方法、抽象方法、构造方法</p>
<p>Java 虽然不能直接为实例赋值，但是它有更优秀的解决方案：为 enum 添加方法来间接实现显式赋值。</p>
<p>创建 enum 时，可以为其添加多种方法，甚至可以为其添加构造方法。</p>
<p>注意一个细节：如果要为 enum 定义方法，那么必须在 enum 的最后一个实例尾部添加一个分号。此外，在 enum 中，必须先定义实例，不能将字段或方法定义在实例前面。否则，编译器会报错。</p>
<h4 id="枚举可以实现接口">枚举可以实现接口</h4>
<pre><code>enum 可以像一般类一样实现接口。
</code></pre>
<pre><code class="language-java">
public interface INumberEnum {
    int getCode();
    String getDescription();
}

public enum ErrorCodeEn2 implements INumberEnum {
    OK(0, &quot;成功&quot;),
    ERROR_A(100, &quot;错误A&quot;),
    ERROR_B(200, &quot;错误B&quot;);

    ErrorCodeEn2(int number, String description) {
        this.code = number;
        this.description = description;
    }

    private int code;
    private String description;

    @Override
    public int getCode() {
        return code;
    }

    @Override
    public String getDescription() {
        return description;
    }
}
</code></pre>
<h3 id="枚举不可以继承">枚举不可以继承</h3>
<p>enum 不可以继承另外一个类，当然，也不能继承另一个 enum 。</p>
<p>因为 enum 实际上都继承自 java.lang.Enum 类，而 Java 不支持多重继承，所以 enum 不能再继承其他类，当然也不能继承另一个 enum。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[消息中间件]]></title>
        <id>https://zuolinlin.github.io/zuo.github.io/post/xiao-xi-zhong-jian-jian/</id>
        <link href="https://zuolinlin.github.io/zuo.github.io/post/xiao-xi-zhong-jian-jian/">
        </link>
        <updated>2022-04-08T13:26:28.000Z</updated>
        <content type="html"><![CDATA[<h1 id="消息中间件">消息中间件</h1>
<h2 id="目录">目录</h2>
<ul>
<li>
<p><a href="#%E6%94%BE%E5%A4%A7100%E5%80%8D%E5%8E%8B%E5%8A%9B%EF%BC%8C%E6%89%BE%E5%87%BA%E4%BD%A0%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%8A%80%E6%9C%AF%E6%8C%91%E6%88%98">放大100倍压力，找出你系统的技术挑战</a></p>
</li>
<li>
<p><a href="#%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6">消息中间件</a></p>
<ul>
<li><a href="#%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E4%BD%9C%E7%94%A8">消息中间件作用</a></li>
<li><a href="#Kafka%E3%80%81RabbitMQ%E4%BB%A5%E5%8F%8ARocketMQ%E5%AF%B9%E6%AF%94">Kafka、RabbitMQ 以及 RocketMQ对比</a></li>
</ul>
</li>
<li>
<p><a href="#RocketMQ">RocketMQ</a></p>
<ul>
<li><a href="#RocketMQ%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86">RocketMQ架构原理</a>
<ul>
<li><a href="#NameServer">NameServer</a></li>
<li><a href="#Broker%E4%B8%BB%E4%BB%8E%E6%9E%B6%E6%9E%84">Broker主从架构</a></li>
<li><a href="#RocketMQ%E7%9A%84%E6%A0%B8%E5%BF%83%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B">RocketMQ的核心数据模型</a></li>
<li><a href="#RocketMQ%E7%9A%84%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2">RocketMQ的集群部署</a>
<ul>
<li><a href="#%E5%B0%8F%E8%A7%84%E6%A8%A1RocketMQ%E7%9A%84%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2">小规模RocketMQ的集群部署</a></li>
<li><a href="#RocketMQ%E7%9A%84%E9%9B%86%E7%BE%A4%E5%8F%AF%E8%A7%86%E5%8C%96%E7%9B%91%E6%8E%A7">RocketMQ的集群可视化监控</a></li>
<li><a href="#RocketMQ%E7%94%9F%E4%BA%A7%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4">RocketMQ生产参数调整</a></li>
<li><a href="#RocketMQ%E7%9A%84%E9%9B%86%E7%BE%A4%E5%8E%8B%E6%B5%8B">RocketMQ的集群压测</a></li>
<li><a href="#RocketMQ%E7%94%9F%E4%BA%A7%E9%9B%86%E7%BE%A4%E7%9A%84%E8%A7%84%E5%88%92">RocketMQ生产集群的规划</a></li>
</ul>
</li>
<li><a href="#RocketMQ%E7%94%9F%E4%BA%A7%E8%80%85%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF%E6%A8%A1%E5%BC%8F">RocketMQ生产者发送消息模式</a></li>
<li><a href="#RocketMQ%E7%9A%84%E6%B6%88%E8%B4%B9%E6%A8%A1%E5%BC%8F">RocketMQ的消费模式</a></li>
<li><a href="#RocketMQ%E7%9A%84%E6%B6%88%E8%B4%B9%E6%A8%A1%E5%BC%8F">RocketMQ的消费模式</a></li>
</ul>
</li>
<li><a href="#RocketMQ%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86">RocketMQ底层原理</a><br>
- <a href="#%E7%94%9F%E4%BA%A7%E8%80%85%E5%BE%80Broker%E9%9B%86%E7%BE%A4%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF%E7%9A%84%E5%BA%95%E5%B1%82%E9%80%BB%E8%BE%91">生产者往Broker集群发送消息的底层逻辑</a><br>
- <a href="#Broker%E6%8E%A5%E5%8F%97%E6%B6%88%E6%81%AF%E5%90%8E%E5%A6%82%E4%BD%95%E5%9C%A8%E5%AD%98%E5%82%A8%E5%88%B0%E7%A3%81%E7%9B%98%E4%B8%8A%E7%9A%84">Broker接受消息后如何在存储到磁盘上的</a><br>
- <a href="#%E5%9F%BA%E4%BA%8EDledger%E6%8A%80%E6%9C%AF%E9%83%A8%E7%BD%B2Broker%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%EF%BC%8C%E5%88%B0%E5%BA%95%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5">基于Dledger技术部署Broker高可用集群，到底如何进行数据同步</a><br>
- <a href="#%E6%B6%88%E8%B4%B9%E8%80%85%E5%88%B0%E5%BA%95%E6%98%AF%E5%9F%BA%E4%BA%8E%E4%BB%80%E4%B9%88%E7%AD%96%E7%95%A5%E9%80%89%E6%8B%A9Master%E6%88%96Slave%E6%8B%89%E5%8F%96%E6%95%B0%E6%8D%AE">消费者到底是基于什么策略选择Master或Slave拉取数据</a><br>
- <a href="#%E6%B6%88%E8%B4%B9%E8%80%85%E6%98%AF%E5%A6%82%E4%BD%95%E4%BB%8EBroker%E6%8B%89%E5%8F%96%E6%B6%88%E6%81%AF%E8%BF%9B%E8%A1%8C%E5%A4%84%E7%90%86%E4%BB%A5%E5%8F%8AACK%E7%9A%84%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%B6%88%E8%B4%B9%E8%80%85%E6%95%85%E9%9A%9C%E4%BC%9A%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86">消费者是如何从Broker拉取消息进行处理以及ACK的？如果消费者故障会如何处理</a><br>
- <a href="#RocketMQ%E6%98%AF%E5%A6%82%E4%BD%95%E5%9F%BA%E4%BA%8ENetty%E6%89%A9%E5%B1%95%E5%87%BA%E9%AB%98%E6%80%A7%E8%83%BD%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E6%9E%B6%E6%9E%84%E7%9A%84">RocketMQ 是如何基于Netty扩展出高性能网络通信架构的？</a><br>
- <a href="#%E5%9F%BA%E4%BA%8Emmap%E5%86%85%E5%AD%98%E6%98%A0%E5%B0%84%E5%AE%9E%E7%8E%B0%E7%A3%81%E7%9B%98%E6%96%87%E4%BB%B6%E7%9A%84%E9%AB%98%E6%80%A7%E8%83%BD%E8%AF%BB%E5%86%99">基于mmap内存映射实现磁盘文件的高性能读写</a><br>
- <a href="#%E4%BA%8B%E5%8A%A1%E6%B6%88%E6%81%AF%E6%9C%BA%E5%88%B6%E7%9A%84%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86">事务消息机制的底层实现原理</a><br>
- <a href="#Broker%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E9%9B%B6%E4%B8%A2%E5%A4%B1%E6%96%B9%E6%A1%88">Broker保证消息零丢失方案</a><br>
- <a href="#Consumer%E6%B6%88%E6%81%AF%E9%9B%B6%E4%B8%A2%E5%A4%B1%E6%96%B9%E6%A1%88">Consumer消息零丢失方案</a><br>
- <a href="#MQ%E6%B6%88%E6%81%AF%E5%B9%82%E7%AD%89%E6%80%A7">MQ消息幂等性</a><br>
- <a href="#MQ%E6%AD%BB%E4%BF%A1%E9%98%9F%E5%88%97">MQ死信队列</a><br>
- <a href="#MQ%E6%B6%88%E6%81%AF%E4%B9%B1%E5%BA%8F">MQ消息乱序</a><br>
- <a href="#MQ%E8%BF%87%E6%BB%A4%E6%9C%BA%E5%88%B6">MQ过滤机制</a><br>
- <a href="#MQ%E5%BB%B6%E8%BF%9F%E6%B6%88%E6%81%AF%E6%9C%BA%E5%88%B6">MQ延迟消息机制</a><br>
- <a href="#MQ%E6%B6%88%E6%81%AF%E7%A7%AF%E5%8E%8B">MQ消息积压</a><br>
- <a href="#%E9%87%91%E8%9E%8D%E7%BA%A7%E7%B3%BB%E7%BB%9F%E9%92%88%E5%AF%B9RocketMQ%E9%9B%86%E7%BE%A4%E5%B4%A9%E6%BA%83%E8%AE%BE%E8%AE%A1%E9%AB%98%E5%8F%AF%E7%94%A8%E6%96%B9%E6%A1%88">金融级系统针对RocketMQ集群崩溃设计高可用方案</a><br>
- <a href="#MQ%E6%B6%88%E6%81%AF%E9%99%90%E6%B5%81">MQ消息限流</a></li>
<li><a href="#RocketMQ%E6%BA%90%E7%A0%81">RocketMQ源码</a></li>
</ul>
</li>
<li>
<p><a href="#RabbitMQ">RabbitMQ</a></p>
<ul>
<li><a href="#RabbitMQ%E6%A6%82%E8%BF%B0">RabbitMQ概述</a></li>
<li><a href="#RabbitMQ%E5%AE%89%E8%A3%85">RabbitMQ安装</a></li>
<li><a href="#%E6%B6%88%E6%81%AF%E5%88%86%E5%8F%91%E6%A8%A1%E5%BC%8F">消息分发模式</a></li>
<li><a href="#%E4%BA%A4%E6%8D%A2%E6%9C%BA%E7%9A%84%E7%B1%BB%E5%9E%8B">交换机的类型</a></li>
<li><a href="#SpringBoot%E6%95%B4%E5%90%88RabbitMQ%E6%95%B4%E5%90%88">SpringBoot整合RabbitMQ整合</a></li>
<li><a href="#TTL%E9%98%9F%E5%88%97%E8%BF%87%E6%9C%9F%E6%97%B6%E9%97%B4">TTL队列过期时间</a></li>
<li><a href="#%E6%AD%BB%E4%BF%A1%E9%98%9F%E5%88%97">死信队列</a></li>
<li><a href="#%E5%86%85%E5%AD%98%E5%92%8C%E7%A3%81%E7%9B%98%E7%A9%BA%E9%97%B4%E7%9B%91%E6%8E%A7">内存和磁盘空间监控</a></li>
<li><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1">分布式事务</a></li>
</ul>
</li>
<li>
<p><a href="#KafKa">KafKa</a></p>
<ul>
<li><a href="#KafKa%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA">KafKa源码分析环境搭建</a></li>
</ul>
</li>
</ul>
<h1 id="消息中间件-2">消息中间件</h1>
<h2 id="放大100倍压力找出你系统的技术挑战">放大100倍压力，找出你系统的技术挑战</h2>
<p>思考一：系统的核心链路，有哪些步骤，各个步骤的性能如何，是否有改进空间？</p>
<pre><code>    下单--预约--核销---退款
     其余的流程可以走MQ异步处理
</code></pre>
<p>思考二：系统中是否有类似后台线程定时补偿的逻辑</p>
<pre><code>订单长时间未支付，要关闭，
预约单长时间无人确认接单，自动分单，
秒杀活动/优惠券设置时间后要自动开始，到点后要自动结束
</code></pre>
<p>思考三：系统中有哪些和第三方系统的耦合？</p>
<pre><code>    耦合微信支付，短信，推送等
      
    考虑MQ解耦
</code></pre>
<p>思考四：核心链路中是否存在哪些关键步骤可能会失败的情况？万一失败了该怎么办？</p>
<pre><code> 例如退款失败后怎么办
</code></pre>
<p>思考五：平时是否有其他系统需要获取你们的数据的情况？他们是如何获取数据的？</p>
<pre><code>是直接跑SQL从你们的数据库里查询？或者是调用你们的接口来获取数据？
是否有这种情况？如果有，对你们有什么影响吗？

可以基于mysql的binlog 日志将数据发送给MQ，别的部门从MQ中消费消息
</code></pre>
<p>思考六：你们的系统是否存在流量洪峰的情况，有时候突然之间访问量增大好几倍，是否对你们的系统产生无法承受的压力？</p>
<h2 id="消息中间件-3">消息中间件</h2>
<h3 id="消息中间件作用">消息中间件作用</h3>
<p>解耦<br>
异步<br>
消峰</p>
<h3 id="kafka-rabbitmq以及rocketmq对比">Kafka、RabbitMQ以及RocketMQ对比</h3>
<p>（1）、Kafka的优势与劣势</p>
<p>kafka 性能很高，基本发消息给kafka都是毫秒级的性能，可用性也很高，kafka支持集群部署的，其中部分宕机是可以继续运行的</p>
<p>但是kafka比较为人诟病的一点，似乎是丢数据方能的问题，因为kafka收到消息后会写入一个磁盘缓冲区里，并没有落地到物理磁盘上，</p>
<p>所以要是机器本身故障，可能会导致磁盘缓冲区数据丢失。</p>
<p>而且kafka另外一个比较大的缺点，就是功能非常单一，主要支持发送消息给他，然后从里面消费消息，其他的就没有什么额外的高级功能了</p>
<p>因此，综上所述，基本行业里的标准，是把kafka在用户行为日志的采集和传输上的，比如大数据团队要收集app上用于的一些行为日志，这种日志就是</p>
<p>kafka收集传输的。</p>
<p>（2）、RabbitMQ的优势与劣势</p>
<p>RabbitMQ的优势是可以保证数据不丢失，也可以保证高可用性，即集群部署的时候。部分机器宕机可以继续运行，然后支持部分高级功能<br>
比如说：死信队列，消息重试之类的</p>
<p>但是他有个缺点最为人诟病的，就是RabbitMQ吞吐量比较低，一般就是每秒几万级别的，所以遇到特别高特别高的并发的情况下，支撑起来是有点困难的</p>
<p>还有一个是它进行集群扩展的时候(就是加机器部署)，还是比较麻烦的</p>
<p>（2）、RocketMQ的优势与劣势</p>
<p>RocketMQ吞吐量也同样很高，单机可以达到10wQPS以上，而且可以保证高可用，性能很高，而且支持配置，保证数据绝对不丢失，可以部署<br>
大规模集群，而且支持各种高级性能，比如说：延迟消息，事务消息，消息回溯，死信队列，消息积压等等</p>
<h2 id="rocketmq">RocketMQ</h2>
<h3 id="rocketmq架构原理">RocketMQ架构原理</h3>
<figure data-type="image" tabindex="1"><img src="images/RockerMQjiagou.png" alt="img_1.png" loading="lazy"></figure>
<p>RocketMQ 这个技术一共包含了四个核心部分</p>
<pre><code>1.第一块就是他的NameServer，这个东西很重要，他要负责管理集群里所有Broker的信息，让使用MQ的系统能感知到集群里有哪些Broke。
2.第二块就是Broke集群的本身信息，必须在多台机器上部署这么一个集群，而且还得用主从架构实现数据多副本和高可用。
3.第三块就是消息生产者
4.第四块就是消息的消费者
</code></pre>
<h4 id="nameserver">NameServer</h4>
<p>NameServer的设计是采用Peer-to-Peer的模式来做的，可以集群化部署，但是里面任何一台机器都是独立运行的，跟其他机器没有任何通信。</p>
<p>要部署RocketMQ，就得先部署NameServer，NameServer支持集群化部署，做到高可用。任何一台机器宕机，NameServer可以继续对外提供服务。</p>
<p>每个Broker启动时都得向所有的NameServer进行注册，也就是说，每个NameServer都有一份集群中所有的Broker信息。</p>
<p>RocketMQ中的生产者和消费者自己主动区NameServer拉取Broker信息的</p>
<p>Broker和NameServer之间通过心跳机制（采用的是TCP长连接），Broker会每隔30s给所有的NameServer发送心跳，告诉NameServer自己还活着。</p>
<p>每次NameServer收到Broker心跳，就可以更新一下它的最近一次心跳时间。</p>
<p>然后NameServer 会每隔10s运行一个任务，区检查各个Broker的最近一次心跳，如果某个Broker超过120s都没发送心跳，那么就认为这个Broker已经挂掉。</p>
<p>生产者和消费者会重新获取NameServer最新的路由信息，并缓存在本地</p>
<h4 id="broker主从架构">Broker主从架构</h4>
<p>为了保证RocketMQ的数据不丢失而且具备一定的高可用性，所以一般将Broker，部署成Master-Slave模式的，也就时一个Master Broker  对应一个Slave Broker</p>
<p>RocketMQ的Master-Slave模式采取的是（Pull模式）Slave Broker不停的发送请求到Master Broker 去拉取消息</p>
<p><strong>写入数据</strong>的时候，肯定是选择<strong>Master Broker</strong> 去写入的</p>
<p><strong>读取数据</strong>的时候，<strong>有可能是Master Broker获取，有可能是Slave Broker 获取</strong>，一切根据当时的情况来定。</p>
<p>如果Slave Broker 挂掉了，那么读写的压力都集中在Master Broker上。</p>
<p>如果Master Broker 挂掉了，在RocketMQ4.5版本之前，Slave Broker无法自定切换为Master Broker,会导致服务不可用。</p>
<p>RocketMQ 4.5之后的版本，RocketMQ支持一种新的机制，叫Dledger，可实现RocketMQ高可用自动切换的效果。</p>
<p>(备注:redis 的Master-Slaver 是基于哨兵模式，异曲同工）</p>
<h4 id="rocketmq的核心数据模型">RocketMQ的核心数据模型</h4>
<p>MQ的核心数据模型 Topic</p>
<p>Topic其实就是一个（逻辑上）数据集合的意思，不同的数据，你的放到不同的Topic里面。</p>
<p>每个Topic分布式存储在Master Broker上，Slave Broker或同步Master Broker 数据。</p>
<p>生产者跟NameServer建立一个TCP长连接，然后定时的从他那里拉取最新的路由信息，包含集群中有哪些Broker，哪些Topic，每个Topic存储在哪些Broker 上</p>
<p>然后生产者找到自己要投递的Topic分布在哪些Broker上，根据负载均衡算法，选择出一台Broker出来，然后跟这个Broker页建立一个TCP长连接，然后通过长连接向Broker发送消息即可。</p>
<p>这里要注意的是生产者一定是投递消息到Master Broker的，然后通过Master Broker会同步数据到他的Slave Broker上</p>
<p>实现一份数据多个副本，保证Master Broker故障之后，数据不丢失，而且可以把Slave Broker切换为Master Broker提供服务。</p>
<p>消费者页生产者类似，他们也会从NameServer建立长连接，然后拉取路由消息，找到自己消息的Topic在哪几台Broker上，就可以跟Broker建立长连接，从里面拉取消息了。</p>
<p>这里唯一要注意的就是消费者系统可能会从Master Broker拉取消息，也可以从Slave Broker拉取消息，都有可能，一切看具体情况。</p>
<h4 id="rocketmq的集群部署">RocketMQ的集群部署</h4>
<h5 id="小规模rocketmq的集群部署">小规模RocketMQ的集群部署</h5>
<h5 id="rocketmq的集群可视化监控">RocketMQ的集群可视化监控</h5>
<h5 id="rocketmq生产参数调整">RocketMQ生产参数调整</h5>
<h5 id="rocketmq的集群压测">RocketMQ的集群压测</h5>
<h5 id="rocketmq生产集群的规划">RocketMQ生产集群的规划</h5>
<h3 id="rocketmq生产者发送消息模式">RocketMQ生产者发送消息模式</h3>
<pre><code>同步发送：生产者发送消息出去之后，登台MQ返回通知，程序在接着向下执行
异步发送：生产者发送消息出去，无需登台MQ返回，直接向下执行，待MQ响应之后，callBack函数，
        如果发送成功，则调用onSuccess函数
        如果发送失败，则调用onException函数
单向发送：生产者发送消息出去之后，代码向下执行，不关注MQ是否返回结果，无论消息发送成功或者发送失败都不管你的事。
</code></pre>
<h3 id="rocketmq的消费模式">RocketMQ的消费模式</h3>
<p>Push消费模式：就是Broker主动把消息发送给你的消费者<br>
Pull消费模式：就是消费者主动发送请求到Broker去拉取消息</p>
<h2 id="rocketmq底层原理">RocketMQ底层原理</h2>
<h3 id="生产者往broker集群发送消息的底层逻辑">生产者往Broker集群发送消息的底层逻辑</h3>
<p>topic数据分片机制：</p>
<figure data-type="image" tabindex="2"><img src="images/topicfenpianjizhi.png" alt="img_1.png" loading="lazy"></figure>
<pre><code>Message Queue 将一个Topic的数据拆分成了很多个数据分片，然后再每个Broker机器上存储一些Message Queue
</code></pre>
<p>生产者写入数据的过程：</p>
<figure data-type="image" tabindex="3"><img src="images/sehngchanzhexierushuju.png" alt="img.png" loading="lazy"></figure>
<p>Broker故障时：</p>
<p>Master Broker故障时，此时正在等待其他Slave Broker自动热切换为Master Broker，但是在这个过程中，这一组Broker是没有Master Broker可以写入的。</p>
<p>通常建议大家打开 Product 中开启一个开关，就是sendLatencyFaultEnable</p>
<p>一旦打开这个开关，那么他就会有一个自动容错机制，比如某次发现一个Broker无法访问，则自动回避访问这个Broker一段时间，过段时间再去访问他。</p>
<h3 id="broker接受消息后如何在存储到磁盘上的">Broker接受消息后如何在存储到磁盘上的</h3>
<pre><code>Broker 数据存储实际才是MQ最核心的环节，他决定了生产者消息写入的吞吐量，决定了消息不能丢失，决定了消费者获取消息的吞吐量，这些都是由他决定的。
</code></pre>
<figure data-type="image" tabindex="4"><img src="images/Brokercunchujizhi.png" alt="img.png" loading="lazy"></figure>
<pre><code>首先，当生产者的消息发送到Broker上的时候，他会把这个消息写入到磁盘上的日志文件，叫做commitLog，直接顺序写入这个文件，（先进入OS Cache Page ，再刷入磁盘）
这个CommitLog是很多个磁盘文件，每个文件限定差不多1GB
Broker收到消息之后就直接追加写入这个文件的末尾，如果一个CommitLog写满了1GB，就会重新创建一个CommitLog

在Broker中，对Topic下的每个MessageQueue都会有一系列的ConsumeQueue文件

格式大致为 $HOME/store/consumeQueue/{topic}/{queueId}/{consumeQueueFileName}
topic：就是指逻辑上的那个Topic
queueId：值某个那个Topic下的某个MessageQueue
consumeQueueFileName：MessageQueue对应的consumeQueue文件名称

当你的Broker收到消息写入到CommitLog之后，其实他同时会将这条数据在CommitLog中的物理位置，也就是一个文件的偏移量（offset），写入到这条消息所属的
MessageQueue对应的consumeQueue文件中去（offset其实的是CommitLog文件消息的地址引用）。

实际上consumeQueue中存储的每条数据不止消息在CommitLog中的offset偏移量，还包含了消息的长度，以及tag，hashCode，一条数据是20个字节，每个consumeQueue保存30W条数据
大概每个文件是5.72M

所以实际上Topic的每个MessageQueue都对应了Broker机器上的多个ConsumeQueue文件，这些文件保存了这个MessageQueue的所有消息在CommitLog文件中的物理位置，也就是offset偏移量。
</code></pre>
<p>为了提升CommitLog文件的写入性能</p>
<pre><code>先写OS操作系统的Page Cache 和顺序写两个机制来提升写入CommitLog文件的性能
</code></pre>
<p>同步刷盘和异步刷盘</p>
<pre><code>异步刷盘模式下，写入到OS Page Cache 缓存成功后，直接提交ACK给生产者，如果刷盘失败，会导致数据丢失，但是吞吐量很高

同步刷盘模式下，写入到OS Page Cache 缓存成功后，必须强制把这台哦消息刷入到底层的物理磁盘，然后才返回ACK给生产者，此时你才知道消息写入成功，
这个模式下，保证Master 数据不会丢失，但是吞吐量下降。
返回ACK后，如果Master挂掉，数据没同步，切换到Slave时，也会出现数据丢失。
</code></pre>
<h3 id="基于dledger技术部署broker高可用集群到底如何进行数据同步">基于Dledger技术部署Broker高可用集群，到底如何进行数据同步</h3>
<figure data-type="image" tabindex="5"><img src="images/Dledger.png" alt="img.png" loading="lazy"></figure>
<p>Dledger技术实际上首先他自己就有一个CommitLog机制，你把数据交给他，他会写CommitLog磁盘文件里去，这是他能干的第一件事情。</p>
<p>如果以及Dledger技术来实现Broker高可用架构，实际上就是用Dledger先替换掉原来的Broker，Dledger自己来来管理CommitLog</p>
<p>那么就是每个Broker上都有一个Dledger组件，</p>
<p>Dledger是基于Raft协议来进行Leader Broker选举的，会从中选举出一个Leader来</p>
<p>Raft协议投票原理：</p>
<pre><code>    比如三台机器，第一轮投票开始，他们都把票投给自己，结果每个人得到的票数一样，第一轮选举失败

    接着每个人进入一个随机休眠，如果是第一个人休息三秒，第二个人休息5秒，四但个人休息4秒

    接着第一个人开始醒过来，继续把票投给自己，然后发送自己的选票给别人，下一个人醒来时，发现自己没有票

    会把票投给那个有票的人，接着第三个人醒来同理

    依靠这个休眠机制一般都能选出一个Leader
</code></pre>
<p>Dledger是基于Raft协议进行多副本同步</p>
<p>Raft协议多副本同步机制：</p>
<pre><code>    首先Leader Broker上的Dledger收到一条数据之后，会标记uncommitted 状态
    然后他会通过自己的DledgerServer组件把这个uncommitted数据发送给Following Broker的Dledger
    接着Follower Broker的DledgerServer收到uncommitted消息之后，必须返回一个ack给Leader Broker的DledgerServer
    然后如果Leader Broker收到超过半数的Follower Broker返回的ack之后，会将消息标记为committed状态
    然后Leader Broker上的DledgerServer就会发送commited消息给Follower Broker机器的DledgerServer，让他们也把消息标记为committed状态
</code></pre>
<p>如果Leader Broker发生宕机了</p>
<p>如果Leader Broker 发生宕机了，剩下的两个Follower Broker会重新发起选举，他们还是会采用Raft协议的算法，去选举出来一个新的Leader Broker继续对外提供服务，<br>
而且会对没有完成数据同步的进行一些恢复性操作，保证数据不会丢失。</p>
<h3 id="消费者到底是基于什么策略选择master或slave拉取数据">消费者到底是基于什么策略选择Master或Slave拉取数据</h3>
<p>集群模式：一个消费者组获取一条消息，只会交给组内的一台机器取处理，而不是每台机器都可以获取到这条消息的<br>
广播模式：那么对于消费者组获取到一条消息，组内的每台机器都可以获取到这条消，但是相对而言，广播模式用的很少，常见基本都是使用集群消费模式<br>
一个Topic的多个MessageQueue会均匀分摊给消费者组内的多个机器取消费，这里的一个原则就是，<br>
一个MessageQueue只能被一个消费机器取消费，但是一个消费机器，可以负责多个MessageQueue的消息处理</p>
<p>Push模式和Pull模式<br>
实际上这两个模式是一样的，都是消费者机器主动发送请求到Broker机器上取拉取一批消息下来。</p>
<p>Push消费模式本质底层也是基于这种消费者主动拉取的模式实现的，只不过他的名字叫Push而已，意思是Broker会尽可能的实时的把消息<br>
交给消费者机器来处理，他的消息实时性会更好。<br>
一般我们使用RocketMQ的时候，消费者模式通常都是基于Push模式来做的，因为Pull模式的代码写起来更加复杂和繁琐，<br>
而且Push模式底层本身就是基于消息拉取来做的，只不过实时性更好。</p>
<p>push模式实现思路：当消费者发送请求到Broker取拉取消息的时候，如果有新消息可以消费那么立马就会返回一批消息到消费机器取处理。<br>
处理完之后会接着立刻发送请求到Broker机器去拉取下一批消息。</p>
<p>所以消费机器再push模式下会处理完一批消息，立马发送请求去拉取下一批消息，消息处理的实时性非常好，看起来就跟Broker<br>
一直不停的推送消息到消费者机器一样。</p>
<p>push模式下有一个请求挂起和长轮询的机制</p>
<p>当你的请求发送到Broker，结果他发现没有新的消息给你处理的时候，就会请求线程挂起，默认挂起15s，然后这个期间<br>
他会有后台线程每隔一会儿就会检查一下是否有新的消息给你，另外如果再这个挂起过程中，如果有新的消息到达了就会主动唤起挂起的线程<br>
，然后把消息返回给你。</p>
<p>其实消费消息的时候，本质就是根据你要的消费的MessageQueue以及开始消费的位置，去找对应的ConsumeQueue读取里面对应位置的消息<br>
再commitLog中的物理offset偏移量，然后到CommitLog中更具offset读取消息数据返回给消费者机器。</p>
<p>ConsumeQueue 文件同样也是基于 os page cache 来进行优化的</p>
<p>也就是说，对于Broker机器磁盘上的大量的ConsumeQueue文件，再写入的时候也会优先进入os cache,</p>
<p>而且os 自己有一个优化机制，就是读取一个磁盘文件的时候，他就会把磁盘文件的一些数据缓存到os cache中。</p>
<p>CommitLog 是基于os cache + 磁盘一起读取的</p>
<p>当你去拉取消息的时候，可以轻松从os cache 里读取少量的consumeQueue文件里的offset，这个性能是极高的，<br>
但是当你去committedLog文件里去读取完整消息的时候，会有两种可能</p>
<p>第一种可能：如果你读取的是那种刚刚写入commitLog的数据，那么大概率还停留再 os cache,<br>
此时你可以顺利的直接从 os cache 中读取commitLog中的数据<br>
第二种可能：你读取的是比较早之前写入的commitLog的数据，哪些数据很早就被刷入磁盘了，已经不在os cache 里了，<br>
那么此时你读取就只能从磁盘文件里读取了</p>
<h3 id="消费者是如何从broker拉取消息进行处理以及ack的如果消费者故障会如何处理">消费者是如何从Broker拉取消息进行处理以及ACK的？如果消费者故障会如何处理</h3>
<p>当消费者处理完这批消息之后，消费者机器就会提交我们的目前一个消费进度到Broker上去，然后Broker就会存储我们的消费进度</p>
<p>下次这个消费者组只要再次拉取这个ConsumeQueue的消息，就可以从Broker记录的消费位置开始拉取，不用从头开始拉取了。</p>
<p>如果消费者族中机器宕机或扩展机器，这个时候其实会进入一个rebalance的环节，也就是说重新给各个消费者机器分配他们要处理的MessageQueue</p>
<h3 id="rocketmq是如何基于netty扩展出高性能网络通信架构的">RocketMQ是如何基于Netty扩展出高性能网络通信架构的</h3>
<p><img src="images/Reactormoxing.png" alt="img.png" loading="lazy"><br>
首先作为Broker而言，他会有一个Reactor主线程，而且这个线程负责监听一个网络端口</p>
<p>假设我么有一个Producer他现在想跟Broker建立TCP长连接，此时Broker上的这个Reactor主线程，他会在端口上监听到这个Producer建立连接的请求</p>
<p>接着这个Reactor主线程就专门负责这个Producer按照TCP协议规定的一系列步骤和规范，建立好一个长连接</p>
<p>Producer里面会有一个SocketChannel，Broker里也会有一个SocketChannel，这两个SocketChannel就代表他们两建立好这个长连接</p>
<p>接着Producer会通过这个SocketChannel去发送消息给Broker。</p>
<p>Reactor线程池，默认是3个线程</p>
<p>Reactor主线程建立好每个连接SocketChannel，都会交给这个Reactor线程池里的其中一个线程去监听请求</p>
<p>基于Worker线程池完成一系列准备工作</p>
<p>线程池会转交请求给Worker线程池，进行一系列预处理</p>
<p>Worker线程池默认有8个线程，此时Reactor线程收到的这个请求会交给Worker线程池中的一个线程进行处理，完成ssl加密，编码解码，连接空闲检查，网络连接管理诸如此类的事情。</p>
<p>预处理之后，请求转交给业务线程池</p>
<p>比如对于处理发送请求而言，就会把请求转发给SendMessage线程池。SendMessage是可以配置的配置的越高，处理消息的吞吐量越高</p>
<p>所以最终的效果就是：</p>
<ul>
<li>Reactor主线程在端口上监听Producer建立连接的请求，建立长连接</li>
<li>Reactor线程池并发的监听多个请求是否到达</li>
<li>Worker线程池求情并发的对多个请求进行预处理</li>
<li>业务线程池并发的对多个请求进行磁盘读写业务操作</li>
</ul>
<p>这样一套网络通信架构，最终实现的效果就是可以高并发、高吞吐的对大量的网络连接发送过来的大量请求数据进行处理，保证Broker实现高吞吐</p>
<h3 id="基于mmap内存映射实现磁盘文件的高性能读写">基于mmap内存映射实现磁盘文件的高性能读写</h3>
<p>传统文件IO操作的多次数据拷贝问题</p>
<p>会先将磁盘文件拷贝到内核IO缓冲区，再将数据拷贝到用户进程的私有空间</p>
<p>为了读取磁盘文件发生了两次拷贝，对磁盘读写的性能是由影响的</p>
<p>RocketMQ 是利用mmap技术配合Page Cache技术进行文件读写优化的</p>
<p>首先RocketMQ底层对CommitLog、ConsumeQueue之类的磁盘文件的读写操作，基本上都是采用mmap的技术来实现的</p>
<p>基于JDK NIO包下的MappedByteBuffer的map()函数先将磁盘文件映射到内存里来</p>
<p>MappedByteBuffer的map()底层就是基于mmap技术实现的</p>
<p>mmap技术在进行文件映射的时候，一般由大小限制，在1.5GB-2GB间</p>
<p>所以RocketMQ才能让CommitLog单个文件在1GB，ConsumeQueue文件在5.72MB，不会太大</p>
<p>接下来就可以对这个已经映射到内存的磁盘文件进行读写操作，写的时候他会直接进入Page Cache中，</p>
<p>然后过一段时间之后，由OS线程异步刷入磁盘中</p>
<p>你加载的数据块的临近其他数据块也一起加载到Page Cache里去</p>
<p>Broker针对磁盘文件高性能读写机制的优化：<br>
1.<strong>内存预加载机制</strong>：Broker会针对磁盘中各种CommitLog、ConsumeQueue文件预先分配好MappedFile，<br>
也就是提前对一些可能接下来要读写的磁盘文件，提前使用MappedByteBuffer执行map()函数完成映射，这样<br>
后续读写文件的时候，就可以直接执行了。<br>
2.<strong>文件预热</strong>：在提前对一些文件完成映射之后，因为映射不会直接将数据加载到内存里来，那么后续在读取<br>
尤其是CommitLog、ConsumeQueue的时候，其实有可能会频繁的从磁盘文件里加载数据到内存中去。</p>
<p>所以其实在执行完map()函数之后，会进行madvise()系统调用，就是提前尽可能多的把磁盘文件加载到内存里去。</p>
<p>通过上述优化，才真正实现了一个效果<br>
就是写磁盘文件的时候都是进入Page Cache的，保证写入的高性能；同时尽可能的多的通过map+madvise<br>
的映射后预热机制，把磁盘文件里的数据尽可能多的加载到Page Cache里来，后续对ConsumeQueue、CommitLog进行读取<br>
的时候，才能尽可能的从内存中读取数据。</p>
<h3 id="事务消息机制的底层实现原理">事务消息机制的底层实现原理</h3>
<figure data-type="image" tabindex="6"><img src="images/trancitionmessage.png" alt="img.png" loading="lazy"></figure>
<pre><code>1.生产者发送half消息，这条half消息会定位到这个Topic的一个MessageQueue，
通过MessageQueue消息写入CommitLog文件，同时消息的offset会写入到MessageQueue对应的consumeQueue

2.但是此时这个half消息并没有发送到消费端对应的ConsumeQueue中，而是自己内部的RMQ_SYS_TRANS_HALF_TOPIC,
对于事务消息下的half消息，RocketMQ是写入内部Topic的ConsumeQueue，而不是指定消费端ConsumeQueue
此时别的系统无法在Topic对应的ConsumeQueue中查看到这套half消息，此时返回half响应给生产者。

3.此时RocketMQ内部会有一个定时任务去扫描RMQ_SYS_TRANS_HALF_TOPIC中的half消息，如果此时你超过了一定时间
还是half消息，此时会回调生产者系统的接口，让你判断这个half消息是rollback还是commit

4.如果rollback，那么此时就需要对消息进行回滚。他的本质就是用一个OP操作来标记half消息的状态
RocketMQ内部有一个OP_TOPIC，此时可以写入一条rollback OP记录在这个Topic里，标记某个half消息是rollback
假设你一直没有执行rollback/commit，RocketMQ会对调生产者系统接口取判断half消息的状态，
但是他最多就是回调15，如果15次之后，你都没有法告知他half消息的状态，此时就自动把消息标记为rollback

5.如果订单系统提交了commit操作，此时RocketMQ就会在OP_TOPIC里写入一条记录，标记half消息已经是commit状态了。
接着就需要把放在RMQ_SYS_TRANS_HALF_TOPIC中的half消息吸入人到消费端 Topic 的ConsumeQueue里去，然后下游系统就可以看到这条消息消费了
</code></pre>
<p>解决发送消息零丢失的方案:一定要使用事务消息方案</p>
<h3 id="broker保证消息零丢失方案">Broker保证消息零丢失方案</h3>
<figure data-type="image" tabindex="7"><img src="images/rocketmq_%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1.png" alt="img.png" loading="lazy"></figure>
<p>解决方案总结：</p>
<pre><code>    1.把Broker刷盘策略调整为同步刷盘，那么绝对不会因为机器宕机而丢失数据 
    2.采用主从架构的Broker集群，那么一条消息写入成功，就意味着多个Broker机器都写入了
    此时任何一台机器的磁盘故障，数据也不会丢失
</code></pre>
<h3 id="consumer消息零丢失方案">Consumer消息零丢失方案</h3>
<p>在默认的Consumer消费模式下，必须是你处理完一批消息了，才会返回CONSUME_SUCCESS这个标识消息都处理结束了，去提交offset到Broker去。</p>
<p>在这种情况下，正常来说是不会丢失消息的，即使你一个Customer宕机了，他会把没处理完的消息交给其他Customer去处理</p>
<p>但是我们要警惕一点，就是我们在代码中不能对消息进行异步处理</p>
<p>对全链路消息丢失方案的总结：</p>
<p>发送消息到MQ的零丢失：<br>
1.同步发送消息+反复多次重试<br>
2.事务消息机制</p>
<p>MQ收到消息之后的零丢失：<br>
同步刷盘策略+主从架构同步机制</p>
<p>消费消息的零丢失：</p>
<p>采用RocketMQ的消费者，天然就可以保证你处理完消息之后，才会提交消息的offset到Broker，<br>
记住千万别采用多线程异步处理消息的方式即可</p>
<p>消息零丢失，会导致性能下降，MQ的吞吐量下降</p>
<p>一般我们建议，对于金钱、交易以及核心数据相关的系统和核心链路，可以使用消息零丢失方案</p>
<h3 id="mq消息幂等性">MQ消息幂等性</h3>
<p>一般来说，对于MQ的重读消息问题而言，我们往MQ里面重复发送一样的消息其实是可以接受的，因为MQ里由多条重复消息，<br>
他不会对核心链路造成影响，但是我们关键要保证的，是你从MQ里获取消息进行处理，必须保证消息不能重复处理。</p>
<p>这样的话，要保证消息的幂等性，我们优先推荐的其实还是业务判断法，直接根据你的数据存储中的记录来判断这个消息是否处理过<br>
如果处理过了，那就别在处理了。因为我们要知道，基于redis的消息发送状态的方案，在一些极端情况下还是没法完全保证幂等性。</p>
<h3 id="mq死信队列">MQ死信队列</h3>
<figure data-type="image" tabindex="8"><img src="images/rocket_%E9%87%8D%E8%AF%95%E9%98%9F%E5%88%97.png" alt="img.png" loading="lazy"></figure>
<p>对于消息处理异常的，可以返回RECONSUME_LATER状态</p>
<p>所以实际上，如果我们因为数据库宕机等问题，导致这批消息处理异常的，此时没法处理这批消息，我们就返回一个RECONSUME_LATER状态</p>
<p>他的意思就是，我现在没法完成这批消息的处理，麻烦你稍后过段时间再次给我这批消息我重试一下！</p>
<p>RocketMQ会有一个针对你这个CustomerGroup的重试队列，如果你返回了RECONSUME_LATER状态，他会把你的这批消息放到你的这个消费者组的重试队列中去。</p>
<p>然后过一段时间之后，重试队列的消息再次给我们，我们再次进行处理，默认最多重试16次，每次重试的时间间隔是不一样的，配置如下所示：<br>
messageDelayLevel:1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h</p>
<p>如果联系16次还是无法处理消息</p>
<p><img src="images/rocket_%E7%A7%81%E4%BF%A1%E9%98%9F%E5%88%97.png" alt="img.png" loading="lazy"><br>
这个时候消息会进入死信队列，后面针对私信队列中的消息进行单独处理就可以了。</p>
<h3 id="mq消息乱序">MQ消息乱序</h3>
<p>在使用MQ的时候，出现消息乱序是是非常正常的一个问题，因为我们原本有顺序的消息，<br>
完全有可能会分到不同的MessageQueue中去，然后不同机器上部署的Customer可能会用混乱的顺序从<br>
不同的MessageQueue里获取消息然后处理。</p>
<p>所以要解决消息乱序的问题其实非常简单：</p>
<p>让同一个业务数据进入到同一个MessageQueue里去<br>
Customer有序的处理一个订单的binlog，</p>
<p>对于有序消息的方案中，如果你遇到消息处理失败的场景，就必须返回SUSPEND_CURRENT_QUEUE_A_MOMENT这个状态<br>
意思是先等一会儿，一会儿再继续处理这批消息，而不能把他放到重试队列中去，然后直接处理下一批。</p>
<h3 id="mq过滤机制">MQ过滤机制</h3>
<p>RocketMQ支持比较丰富的数据过滤语法</p>
<p>1.数值比较，比如: &gt; ,&gt;= ,&lt; ,&lt;= ,BETWEEN ,= ;<br>
2.字符比较，比如： = ，&lt;&gt;,IN;<br>
3.IS NULL或者IS NOT NULL;<br>
4.逻辑符号 AND ,OR , NOT ;<br>
5.数值，1f127853<br>
6.字符，比如'abc'必须用单引号包裹起来<br>
7.NULL,特殊的常量<br>
8.布尔值 TRUE或者FALSE</p>
<h3 id="mq延迟消息机制">MQ延迟消息机制</h3>
<figure data-type="image" tabindex="9"><img src="images/rocket_%E5%AE%9A%E6%97%B6%E6%89%AB%E6%8F%8F%E4%B8%BA%E6%94%AF%E4%BB%98%E7%9A%84%E8%AE%A2%E5%8D%95.png" alt="img.png" loading="lazy"></figure>
<p>这种方式很不好，导致一个订单要扫描很多遍才发现他超过了30min未支付<br>
<img src="images/rocket_%E5%BB%B6%E8%BF%9F%E6%B6%88%E6%81%AF.png" alt="img.png" loading="lazy"><br>
因此针对这种场景，MQ里的延迟消息就会出场了，而且在实际项目中，MQ的延迟消息使用往往是很多的。</p>
<p>所谓延迟消息，意思就是说，我们订单系统在创建一个订单之后，可以发送一条消息到MQ里去，<br>
我们指定这条消息是延迟消息，比如要等待30分钟之后，才能被订单扫描服务给消费到。</p>
<p>这种方式就比你用后台线程扫描订单的方式要好多了，一个是对每个订单你只会在他创建的30分钟查询他一次，<br>
不会反复扫描多次。</p>
<p>另外就是如果你的订单数量很多，你完全可以让订单扫描的服务多部署几台机器，<br>
然后对于MQ中的Topic可以多指定一个MessageQueue，这样每个订单扫描服务的机器作为一个Customer都会处理<br>
一部分订单查询的任务。</p>
<p>MQ的延迟消息是非常常用并且非常有用的功能。</p>
<p>其实发送延迟消息的核心，就是设置消息的delayTimeLevel，也就是延迟级别</p>
<p>RocketMQ默认支持的延迟级别如下：1s,5s,10s,1m,2m,3m,4m,5m,6m,7m,8m,9m,10m,20m,1h,2h</p>
<p>如果设置为3，意思是延迟10s,你发送出去的消息，会过10s被消费者获取到</p>
<p>其实Consumer是支持设置从哪里开始消费消息的，常见有两种，一种是从Topic的第一条数据开始消费（CONSUMER_FROM_LAST_OFFSET），<br>
另一种是从最后一次消费过的消息之后开始消费（CONSUMER_FROM_FIRST_OFFSET）</p>
<p>一般来说，我们都会选择（CONSUMER_FROM_LAST_OFFSET），这样你刚才就从topic的第一条消息开始消费，但是每次重启，你都是从上一次消费到的位置<br>
继续往后进行消费的。</p>
<h3 id="mq消息积压">MQ消息积压</h3>
<p>高峰期消息积压，消费者无法尽快处理</p>
<p>增加消费者加大消费的吞吐量</p>
<h3 id="金融级系统针对rocketmq集群崩溃设计高可用方案">金融级系统针对RocketMQ集群崩溃设计高可用方案</h3>
<p>对于跟钱相关的问题，针对这种场景，我们通常会在发送MQ那个系统中设计高可用降级方案，这个降级的思路是,<br>
在你需要发送MQ的地方进行try catch 捕获，如果发现发送消息到MQ有异常，此时就进行重试。</p>
<p>如果发现连续重试超过一定次数还是失败，说明MQ集群彻底崩溃了，此时你必须把消息写入本地储存或者NoSQL存储中去，但是一定要保证消息写入的顺序。</p>
<p>只要在这个方案下，即使MQ集群彻底崩溃了，你系统的消息也不会丢失，对于跟金钱相关的系统，广告系统来说，这个高可用的设计方案还是非常有必要的</p>
<h3 id="mq消息限流">MQ消息限流</h3>
<p>一般会根据MQ压测的结果来，通过压测看看MQ最多能抗下多少QPS，然后最好限流</p>
<p>一般来说，限流算法可以采用令牌桶算法：也就是说每秒发放多少个令牌，然后只允许多少个请求通过</p>
<h2 id="rocketmq源码">RocketMQ源码</h2>
<h1 id="rabbitmq">RabbitMQ</h1>
<h2 id="rabbitmq概述">RabbitMQ概述</h2>
<figure data-type="image" tabindex="10"><img src="images/RabbitMQ%E7%9B%B8%E5%85%B3%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.png" alt="" loading="lazy"></figure>
<p>Producer ：生产者，生成消息并把消息发送给 RabbitMQ 。</p>
<p>Consumer ：消费者，从 RabbitMQ 中接收消息。</p>
<p>Exchange ：交换器，具有路由的作用，将生产者传递的消息根据不同的路由规则传递到对应的队列中。交换器具有四种不同的类型，每种类型对应不同的路由规则。</p>
<p>Queue ：队列，实际存储消息的地方，消费者通过订阅队列来获取队列中的消息。</p>
<p>Binding ：绑定交换器和队列，只有绑定后消息才能被交换器分发到具体的队列中，用一个字符串来代表Binding Key。</p>
<p>消息是如何由生产者传递到消费者：</p>
<p>1、生产者 Producer 生成消息 msg ，并指定这条消息的路由键 Routing Key ，然后将消息传递给交换器 Exchange 。</p>
<p>2、交换器 Exchange 接收到消息后根据 Exchange Type 也就是交换器类型以及交换器和队列的 Binding 绑定关系来判断路由规则并分发消息到具体的队列 Queue 中。</p>
<p>3、消费者 Consumer 通过订阅具体的队列，一旦队列接收到消息便会将其传递给消费者。</p>
<p>这里的 Routing Key 和 Binding 我是按照自己的理解解释的，与某些参考资料是有出入的，读者理解就好。</p>
<p>当然完成上述三个步骤还缺少两个关键的东西：</p>
<p>Connection ：连接，不论生产者还是消费者想要使用 RabbitMQ 都必须首先建立到RabbitMQ 的 TCP 连接。</p>
<p>Channel ：信道，建立完TCP 连接后还必须建立一个信道，消息都是在信道中传递和操作的。<br>
<img src="images/amqp%E4%BF%A1%E9%81%93.png" alt="信道" loading="lazy"></p>
<p>、上图形象的展示了连接和信道之间的关系，一个连接中可以建立多个信道，而且每个信道之间都是完全隔离的，同时我们需要记住的是创建和销毁 TCP 连接是很消耗资源的，而信道则不是，所以能够通过创建多个信道来隔离环境的不要通过创建多个连接。</p>
<h2 id="消息分发模式">消息分发模式</h2>
<ul>
<li>简单模式</li>
<li>工作模式</li>
<li>fanout模式</li>
<li>direct模式</li>
<li>topic模式</li>
<li>RPC模式</li>
</ul>
<h2 id="交换机的类型">交换机的类型</h2>
<p>交换器类型</p>
<p>交换器具有路由分发消息的作用，其有四种不同的类型，每种类型对应不同的路由规则：</p>
<p>fanout ：广播，将消息传递给所有该交换器绑定的队列。</p>
<p>direct ：直连，将消息传递给 Routing Key 与 Binding Key完全一致的队列中，可以有多个队列。</p>
<p>topic ：模糊匹配，Binding Key 是一个可以用符号 . 分隔单词的字符串，模糊匹配下，符号 * 用于匹配任意一个单词，符号  # 用于匹配零个或多个单词。</p>
<p>headers ：这个比较特殊，是根据消息中具体内容的 header 属性来作为路由规则的，这种类型对资源消耗太大，一般很少使用，前面三种类型就够了。</p>
<h2 id="rabbitmq安装">RabbitMQ安装</h2>
<pre><code>注意：如果docker pull rabbitmq 后面不带management，启动rabbitmq后是无法打开管理界面的，所以我们要下载带management插件的rabbitmq.

docker pull rabbitmq:3.8.6-management

mkdir -p /data/rabbitmq

#机器1运行:
docker run -d --restart=always --name rabbitmq1 --hostname rabbitmq1 --add-host=&quot;rabbitmq1&quot;:172.18.163.76 --add-host=&quot;rabbitmq2&quot;:172.18.163.77 --add-host=&quot;rabbitmq3&quot;:172.18.163.78 -p 5672:5672 -p 15672:15672 -p 4369:4369 -p 25672:25672 -v /data/rabbitmq/:/var/lib/rabbitmq -e RABBITMQ_DEFAULT_VHOST=mrb_vhost -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_NODENAME=rabbitmq1 -e RABBITMQ_DEFAULT_PASS=dTybUNtvyu90 -e RABBITMQ_ERLANG_COOKIE=dTybUNtvyu90 docker.io/rabbitmq:3.8.6-management
#进入容器:
docker exec -it rabbitmq1 bash
#容器内执行:
rabbitmqctl stop_app
rabbitmqctl reset
rabbitmqctl start_app
rabbitmqctl set_user_tags admin administrator
rabbitmqctl set_policy -p mrb_vhost ha &quot;^&quot; '{&quot;ha-mode&quot;:&quot;all&quot;,&quot;ha-sync-mode&quot;:&quot;automatic&quot;}'

#机器2运行:
docker run -d --restart=always --name rabbitmq2 --hostname rabbitmq2 --add-host=&quot;rabbitmq1&quot;:172.18.163.76 --add-host=&quot;rabbitmq2&quot;:172.18.163.77 --add-host=&quot;rabbitmq3&quot;:172.18.163.78 -p 5672:5672 -p 15672:15672 -p 4369:4369 -p 25672:25672 -v /data/rabbitmq/:/var/lib/rabbitmq -e RABBITMQ_DEFAULT_VHOST=mrb_vhost -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_NODENAME=rabbitmq2 -e RABBITMQ_DEFAULT_PASS=dTybUNtvyu90 -e RABBITMQ_ERLANG_COOKIE=dTybUNtvyu90 docker.io/rabbitmq:3.8.6-management
#进入容器:
docker exec -it rabbitmq2 bash
#容器内执行:
rabbitmqctl stop_app
rabbitmqctl reset
rabbitmqctl join_cluster rabbitmq1@rabbitmq1
rabbitmqctl start_app

#机器3运行:
docker run -d --restart=always --name rabbitmq3 --hostname rabbitmq3 --add-host=&quot;rabbitmq1&quot;:172.18.163.76 --add-host=&quot;rabbitmq2&quot;:172.18.163.77 --add-host=&quot;rabbitmq3&quot;:172.18.163.78 -p 5672:5672 -p 15672:15672 -p 4369:4369 -p 25672:25672 -v /data/rabbitmq/:/var/lib/rabbitmq -e RABBITMQ_DEFAULT_VHOST=mrb_vhost -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_NODENAME=rabbitmq2 -e RABBITMQ_DEFAULT_PASS=dTybUNtvyu90 -e RABBITMQ_ERLANG_COOKIE=dTybUNtvyu90 docker.io/rabbitmq:3.8.6-management
#进入容器:
docker exec -it rabbitmq3 bash
#容器内执行:
rabbitmqctl stop_app
rabbitmqctl reset
rabbitmqctl join_cluster rabbitmq1@rabbitmq1
rabbitmqctl start_app
</code></pre>
<h2 id="springboot整合rabbitmq整合">SpringBoot整合RabbitMQ整合</h2>
<p>生产者</p>
<pre><code class="language-java">    @Autowired
    private RabbitTemplate rabbitTemplate;
     rabbitTemplate.convertAndSend(exchange, routingKey, msg, new CorrelationData(IdUtil.objectId()));
</code></pre>
<p>消费者</p>
<pre><code>队列与交换机的绑定可以在生产者，也可以在消费者，也可以同时绑定，建议在消费者端绑定，因为消费者端先启动
绑定的方式可以使用配置文件的方式，可以使用注解的方式
</code></pre>
<pre><code class="language-java">@Slf4j
@Component
@RabbitListener(bindings = @QueueBinding(
        value = @Queue(value = FanoutQueue.PROJECT_VIEW),
        exchange = @Exchange(value = FanoutExchange.PROJECT, type = ExchangeTypes.FANOUT)
))
public class ProjectViewListener {

</code></pre>
<h2 id="ttl队列过期时间">TTL队列过期时间</h2>
<p>概述</p>
<pre><code>过期时间TTl表示可以对消息设置预期时间，在这个时间内都可以被消费者接受获取，过了时间之后，消息将自动被删除
目前有两种方式设置：
第一种方法是通过队列属性设置，队列中所有的消息都有相同的过期时间 
第二张是对消息进行单独设置，每条消息的TTL可以不同
</code></pre>
<p>对队列设置过期时间</p>
<pre><code class="language-java"> 
    x-messsage-ttl:设置这个属性单位是毫秒 
   

</code></pre>
<p>对消息设置过期时间</p>
<pre><code class="language-java">MessagePostProcessor messagePostProcessor = new MessagePostProcessor(){
    
    @Override
public Message messagePostProcessor(Message message){
        message..getMessagePostProcessor.setExpiration(&quot;5000&quot;);
        message..getMessagePostProcessor.setContentEncoding(&quot;UTF-8&quot;);
  }
}

        rabbitTemplate.convertAndSend(exchange, routingKey, msg, new CorrelationData(IdUtil.objectId()),messagePostProcessor);
</code></pre>
<p>过期队列，和过期消息的区别</p>
<pre><code>过期队列，时间到了之后，会将消息进行移除，但是不是直接移除，而是放到一个死信队列里面来。
过期消息，时间到了之后，会将消息进行移除，不会放在任何一个地方。
</code></pre>
<h2 id="死信队列">死信队列</h2>
<p>概述</p>
<pre><code>DLX：Dead Letter Exchange，可以称之为死信交换机，也可以称之为死信邮箱，当一个消息在一个队列中变成死信之后，它能被重新发送到另一个交换机中，这个
交换机就是DLX，绑定DLX的队列称之为私信队列
DLX交换机是正常的交换机和一般交换机没有区别，它能在任何队列上被指定，实际上就是
设置某一队列的属性，当这个队列存在死信时，RabbitMQ会自动将这个消息发布到DLX上去，
进而被路由到另一个队列即死信队列。
</code></pre>
<p>消息变成死信的原因：</p>
<pre><code>消息被拒绝
消息过期
消息达到最大长度
</code></pre>
<p>设置死信队列：只需要在定义队列的时候设置队列参数：x-dead-letter-exchange，指定交换机即可。</p>
<h2 id="内存和磁盘空间监控">内存和磁盘空间监控</h2>
<h1 id="kafka">KafKa</h1>
<h2 id="kafka源码分析环境搭建">KafKa源码分析环境搭建</h2>
<p>1.按照jdk<br>
2.安装scala<br>
直接下载傻瓜式安装就可以了。<br>
接着配置SCALA_HOME和PATH两个环境变量<br>
3.安装Gradle，项目构建<br>
4.zookeeper是必须是必须适用的</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[spring]]></title>
        <id>https://zuolinlin.github.io/zuo.github.io/post/spring/</id>
        <link href="https://zuolinlin.github.io/zuo.github.io/post/spring/">
        </link>
        <updated>2022-04-08T13:24:44.000Z</updated>
        <content type="html"><![CDATA[<h1 id="spring">spring</h1>
<h2 id="目录">目录</h2>
<ul>
<li><a href="#%E7%B2%BE%E9%80%89%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%9ASpringIOC%E5%AE%B9%E5%99%A8%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BF%87%E7%A8%8B">精选面试题：SpringIOC容器初始化过程</a>
<ul>
<li><a href="#IOC%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84">IOC是如何工作的？</a></li>
<li><a href="#Resource%E5%AE%9A%E4%BD%8D">Resource定位</a></li>
<li><a href="#%E8%BD%BD%E5%85%A5BeanDefinition">载入BeanDefinition</a></li>
<li><a href="#%E5%B0%86BeanDefinition%E6%B3%A8%E5%86%8C%E5%88%B0%E5%AE%B9%E5%99%A8">将BeanDefinition注册到容器</a></li>
</ul>
</li>
<li><a href="#SpringIOC%E4%BE%9D%E8%B5%96%E6%9F%A5%E6%89%BE">SpringIOC依赖查找</a>
<ul>
<li><a href="#%E6%A0%B9%E6%8D%AEBean%E5%90%8D%E7%A7%B0%E6%9F%A5%E6%89%BE">根据Bean名称查找</a></li>
<li><a href="#%E6%A0%B9%E6%8D%AEBean%E7%B1%BB%E5%9E%8B%E6%9F%A5%E6%89%BE">根据Bean类型查找</a></li>
<li><a href="#%E4%BE%9D%E8%B5%96%E6%9F%A5%E6%89%BE%E4%B8%AD%E7%9A%84%E7%BB%8F%E5%85%B8%E5%BC%82%E5%B8%B8%EF%BC%9ABean%E6%89%BE%E4%B8%8D%E5%88%B0%EF%BC%9FBean%E4%B8%8D%E6%98%AF%E5%94%AF%E4%B8%80%E7%9A%84%EF%BC%9F">依赖查找中的经典异常：Bean找不到？Bean不是唯一的？</a></li>
</ul>
</li>
<li>[源码剖析：Spring IOC容器Bean初始化流程？](#源码剖析：Spring IOC容器Bean初始化流程？)</li>
<li><a href="#SpringIOC%E4%BE%9D%E8%B5%96%E6%9F%A5%E6%B3%A8%E5%85%A5">SpringIOC依赖查注入</a></li>
<li><a href="#%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90%EF%BC%9A@Autowire%E6%B3%A8%E8%A7%A3%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5%E7%9A%84%E5%8E%9F%E7%90%86">源码剖析：@Autowire注解依赖注入的原理</a></li>
<li><a href="#%E7%B2%BE%E9%80%89%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%9A@Autowire%E5%92%8C@Resource%E5%8C%BA%E5%88%AB">精选面试题：@Autowire和@Resource区别</a></li>
<li>[Spring IOC依赖来源有哪些？](#Spring IOC依赖来源有哪些？)</li>
<li>[精选面试题：@Value是如何将外部化配置注入到spring bean中？](#精选面试题：@Value是如何将外部化配置注入到spring bean中？)</li>
<li>[精选面试题：spring IOC如何解决循环依赖问题？](#精选面试题：spring IOC如何解决循环依赖问题？)</li>
<li><a href="#JDK%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E5%92%8CCGLIB%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86">JDK动态代理和CGLIB动态代理</a></li>
<li>[_Aspect、Join Points、Pointcuts 和 Advice 语法](#Aspect、Join Points、Pointcuts和Advice语法)</li>
<li>[_源码分析：Spring AOP JDK动态代理实现原理](#_源码分析：Spring AOP JDK动态代理实现原理)</li>
<li>[精选面试题：Spring AOP和AspectJ AOP存在哪些区别？](#精选面试题：Spring AOP和AspectJ AOP存在哪些区别？)</li>
<li><a href="#%E5%BC%95%E5%85%A5logback%E6%A1%86%E6%9E%B6%E8%BF%9B%E8%A1%8C%E6%97%A5%E5%BF%97%E6%89%93%E5%8D%B0">引入logback框架进行日志打印</a></li>
<li><a href="#@Aspect%E6%B3%A8%E8%A7%A3%E6%98%AF%E6%80%8E%E4%B9%88%E8%BF%90%E4%BD%9C%E8%B5%B7%E6%9D%A5%E7%9A%84">@Aspect注解是怎么运作起来的</a></li>
<li>[_代码实战：基于Spring AOP机制，实现全局异常处理](#_代码实战：基于Spring AOP机制，实现全局异常处理)</li>
<li>[基于Spring AOP机制拦截处理登陆信息](#基于Spring AOP机制拦截处理登陆信息)</li>
<li>[精选面试题：Spring AOP的设计模式有哪些？](#精选面试题：Spring AOP的设计模式有哪些？)</li>
<li><a href="#%E5%85%88%E6%9D%A5%E7%9C%8B%E7%9C%8B%E5%8E%9F%E7%94%9FJDBC%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%BA%93%E6%93%8D%E4%BD%9C">先来看看原生 JDBC 如何进行数据库操作</a></li>
<li><a href="#%E5%9F%BA%E4%BA%8EDruid%E8%BF%9E%E6%8E%A5%E6%B1%A0%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C%EF%BC%8C%E6%8F%90%E5%8D%87%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD">基于 Druid 连接池进行数据操作，提升系统性能</a></li>
<li><a href="#SpringEvent%E4%BA%8B%E4%BB%B6%E9%80%9A%E7%9F%A5%E6%9C%BA%E5%88%B6%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86">Spring Event事件通知机制底层原理</a></li>
<li>[玩转Spring Cache中@Cacheable注解的底层原理](#玩转Spring Cache中@Cacheable注解的底层原理)</li>
<li><a href="#%E7%B2%BE%E9%80%89%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%9A%E4%BB%80%E4%B9%88%E6%83%85%E5%86%B5%E5%AF%BC%E8%87%B4Spring%E4%BA%8B%E5%8A%A1%E5%A4%B1%E6%95%88%EF%BC%9F">精选面试题：什么情况导致Spring事务失效？</a></li>
</ul>
<h1 id="目录-2">目录</h1>
<h2 id="精选面试题springioc容器初始化过程">精选面试题：SpringIOC容器初始化过程</h2>
<h3 id="ioc是如何工作的">IOC是如何工作的</h3>
<pre><code class="language-java">
ApplicationContext appContext=new ClassPathXmlApplicationContext(&quot;/bean.xml&quot;);
        User user=(User)appContext.getBean(&quot;user&quot;);

</code></pre>
<pre><code>如上述代码所示，通过ApplicationContext 创建Spring容器，该容器会读取配置文件 &quot;/bean.xml&quot;,
并统一管理由该文件中定义好的Bean实例对象，如果要获取某个bean实例，使用getBean方法就行了，
假设将user配置在beans.xml文件中，之后就不需要使用new User()的方式创建对象，
而是通过ApplicationContext 容器来获取User的实例
</code></pre>
<p>剖析创建IOC容器要经历的几个阶段： Resource定位、载入BeanDefinition、将BeanDefinition注册到容器</p>
<h4 id="resource定位">Resource定位</h4>
<pre><code>Resource定位是Spring中用于封装I/O操作的接口。在创建Spring容器时，会去访问xml配置文件，还可以通过文件类型
、二进制留、URL等方式访问资源。这些都可以理解为Resource。其体系结构如下如所示：
</code></pre>
<ul>
<li>
<p>FileSystemResource：以文件绝对路径访问资源</p>
</li>
<li>
<p>ClassPathResource：以类路径的方式访问资源</p>
</li>
<li>
<p>ServletContextResource：web应用根目录的方式访问资源</p>
</li>
<li>
<p>UrlResource：访问网络资源的实现类</p>
</li>
<li>
<p>ByteArrayResource:访问字节数组资源的实现类</p>
<pre><code>那么这些类型在Spring中是如何访问的呢？Spring 提供了ResourceLoader接口用于实现不同的Resource加载策略，
该接口的实例对象中可以获取一个resource对象。
如下代码所示,在ResourceLoader接口中只定义了两个方法：
</code></pre>
</li>
</ul>
<pre><code class="language-java">
public ResourceLoader{
        //通过提供的资源location参数获取Resource 实例
        Resource getResource(String location);

// 获取ClassLoader，通过ClassLoader可将资源载入到JVM
        ClassLoader getClassLoader(String location);

        }


</code></pre>
<p>注：</p>
<pre><code>   ApplicationContext德所有实现类都实现ResourceLoader接口，因此可以直接调用getResource(location)获取Resource 对象，。
   不同的ApplicationContext实现类使用getResource方法获取资源类型不同。
   例如：
    FileSystemXmlApplicationContent.getResource获取的就是FileSystemResource实例
    ClassPathXmlApplicationContext.getResource获取的就是ClassPathResource实例
    XmlWebApplicationContent.getResource获取的就是ClassPathResource实例
     另外像不需要通过xml直接使用注解@Configuration方式加载资源的AnnotationConfigApplicationContext等等
</code></pre>
<p>在资源定位过程完成以后，就为资源文件中的bean的载入创造了I/O操作的条件。</p>
<h4 id="载入beandefinition">载入BeanDefinition</h4>
<p>如何读取资源中德数据------BeanDefinition的载入过程中的描述</p>
<pre><code>BeanDefinition是一个数据结构，BeanDefinition是根据Resource对象中bean来生成的。bean会在SpringIoc容器内部以BeanDefinition的形式存在，
Ioc容器对Bean的管理和依赖注入的实现是通过BeanDefinition来完成的。BeanDefinition就是Bean在Ioc容器中的存在形式。

由于Spring的配置文件主要是XML格式，一般而言会使用AbstractXmlApplicationContext类进行文件读取，如图，该类定义了一个名为
loadBeanDefinition(DefaultListableBeanFactory beanFactory)的方法来获取BeanDefinition

方法内会new 一个BeanDefinitionReader对象，然后将生成的实例传入loadBeanDefinition方法
</code></pre>
<pre><code class="language-java">
import java.io.IOException;

public class AbstractXmlApplicationContext {

    //获取BeanDefinition
    protected void loadBeanDefinition(DefaultListableBeanFactory beanFactory) throws BeanException, IOException {

        XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory);
        beanDefinitionReader.setEnvironment(this.getEnvironment());
        beanDefinitionReader.setResourceLoader(this);
        beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this));
        //用于获取BeanDefinition
        this.loadBeanDefinitions(beanDefinitionReader);
    }

    protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeanException, IOException {
        //获取所有定位到的resource资源定位（用户定义）
        Resource[] configResources = getConfigResources();
        if (configResources != null) {
            //载入resource
            reader.loadBeanDefinitions(configResources);
        }
        //获取所有本地配置文件的位置（容器本身）
        String[] configLocations = getConfigLocations();
        if (configLocations != null) {
            //载入resource
            reader.loadBeanDefinitions(configLocations);
        }
    }
}

</code></pre>
<p>顺着看reader中loadBeanDefinitions方法，该方法override了AbstractBeanDefinitionReader类，父接口的BeanDefinitionReader。<br>
方法体中，将所有资源全部加载，并且交给AbstractBeanDefinitionReader的实现子类处理这些resource。</p>
<pre><code class="language-java">
public class AbstractBeanDefinitionReader extends BeanDefinitionReader {

    @Override
    public int loadBeanDefinitions(Resource... resources) throws BeanDefinitionStoreException {
        Assert. assert (resources,&quot;Resource array must not be null&quot;);
        int counter = 0;
        for (Resource resource : resources) {
            //将所有资源全部加载，交给AbstractBeanDefinitionReader的实现子类处理这个resource
            counter += loadBeanDefinitions(resource);
        }
        return counter;
    }

}



</code></pre>
<p>再BeanDefinitionReader 接口中定义了 int loadBeanDefinitions(Resource resource) 方法</p>
<pre><code class="language-java">public interface BeanDefinitionReader {

    int loadBeanDefinition(Resource Resource) throws BeanDefinitionStoreException;

    int loadBeanDefinition(Resource... Resource) throws BeanDefinitionStoreException;
}

</code></pre>
<p>此时回到XmlBeanDefinitionReader上来,它主要是针对XML方式的Bean进行读取，XmlBeanDefinitionReader 主要是实现了AbstractBeanDefinitionReader抽象类，<br>
而该类继承与BeanDefinitionReader，主要实现的方法也是来自于BeanDefinitionReader的loadBeanDefinitions(Resource) 方法</p>
<figure data-type="image" tabindex="1"><img src="images/XmlBeanDefinitionReader%E7%BB%A7%E6%89%BF%E5%85%B3%E7%B3%BB%E5%9B%BE.png" alt="img_1.png" loading="lazy"></figure>
<p>如图所示，读取Bean之后就是加载Bean的过程了，XmlBeanDefinitionReader中的loadBeanDefinitions方法主要来处理加载Bean的工作。<br>
首先对资源经行验证，然后从资源对象中加载Document对象，使用了documentLoader中的loadDocument方法，<br>
然后跟上registerBeanDefinition对文档对应的Resource进行注册，也就是将XML文件中Bean 转换成容器中BeanDefinition。</p>
<pre><code class="language-java">    public class AbstractBeanDefinitionReader {

    protected int doLoadBeanDefinitions(InPutSource inputSource, Resource Resource) throws BeanDefinitionStoreException {

        try {
            //获取指定资源的验证方法
            int validation = getValidationModeFotResource(Resource);
            //将资源对象中加载Document对象，大致过程为：将Resource资源文件的内容读入到document
            //DocumentReader 在容器中读取XML文件过程中有着举足轻重的作用！
            // XmlBeanDefinitionReader实例化时会创建一个DefaultDocumentLoader型私有属性，继而调用loadDocument方法
            // inputSource --要加载的文档输入源
            this.documentLoader.loadDocument(inputSource,
                    this.entityResolver,
                    this.errorHandler, validationMode,
                    this.namespaceAware);

            //将document文件的bean封装成BeanDefinition，并注册到容器
            return registerBeanDefinition(doc, resource);
        } catch {
            //
        }
    }


}



</code></pre>
<p>如上图所示，接下来就是registerBeanDefinitions方法了，它主要是针对Spring Bean 语义进行转换，变成BeanDefinition类型。</p>
<p>首先获取DefaultBeanDefinitionDocumentReader实例，然后获取容器中的bean数量，通过documentReader中的registerBeanDefinition</p>
<p>方法进行注册和转换工作</p>
<pre><code class="language-java">
public class XmlBeanDefinitionReader {

    public int registerBeanDefinitions(Document doc, Resource resource) throws BeanDefinitionStoreException {
        //获取到DefaultBeanDefinitionDocumentReader实例
        BeanDefaultDocumentReader documentReader = createBeanDefinitionDocumentReader();
        //获取容器中bean的数量
        int countBefore = getRegister().getBeanDefinitionCount();
        documentReader.registerBeanDefinitions(doc, createReaderContext(resource));
        return getRegister().getBeanDefinitionCount() - countBefore;

    }

}


</code></pre>
<p>顺着上面的思路继续往下，在DefaultBeanDefinitionDocumentReader中的registerBeanDefinitions方法如下图所示，其获取document的根据根节点然后顺势访问所有子节点。同时把处理<br>
BeanDefinition的过程委托给BeanDefinitionParserDelegate对象完成。</p>
<pre><code class="language-java">
import org.w3c.dom.Element;

public class DefaultBeanDefinitionDocumentReader implements BeanDefinitionDocumentReader {

    public void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) {
        this.readerContext = readerContext;
        logger.debug(&quot;Loading bean definitions &quot;)
        // 获取doc 的root 节点 ，通过该节点能够访问所有子节点
        Element root = doc.getDocumentElement();
        // 处理beanDefinition的过程委托给beanDefinitionParseDelegate实例对象完成
        BeanDefinitionParseDelegate delegate = createHelper(readerContext, root);
        preProcessXml(root);
        //核心方法 代理
        parseBeanDefinitions(root, delegate);
        postProcessXml(root);

    }

    /**
     * BeanDefinitionParseDelegate类主要负责BeanDefinition的解析
     *这里涉及到JDK和CGLIB动态代理的知识
     * BeanDefinitionParseDelegates代理完全对符合Spring语义规则的处理。
     * 比如&lt;bean&gt;&lt;/bean&gt;、&lt;import&gt;&lt;/import&gt;、&lt;alias&gt;&lt;/alias&gt;等的检测
     * 就是如下
     * BeanDefinitionParseDelegate代理类中的parseBeanDefinitions方法，用来对XML文件中节点进行解析。
     * 通过遍历&lt;import&gt;&lt;/import&gt;标签节点调用importBeanDefinitionResource方法对其进行处理，然后利用遍历
     * bean节点调用processBeanDefinition对其调用。
     *
     */

    protected void parseBeanDefinitions(Element root, BeanDefinitionParseDelegate delegate) {
        if (delegate.isDefaultNamespace) {
            NodeList nl = root.getChildNodes();
            // 遍历所有节点，做对应解析工作
            // 如遍历到&lt;import&gt; 标签节点就调用importBeanDefinitionResource(ele)方法对应处理
            // 遍历到&lt;bean&gt; 标签就调用processBeanDefinition(ele ,delegate)方法对应处理
            for (int i = 0; i &lt; nl.getElementCount(); i++) {
                Node node = nl.get(i);
                if (node instanceof Element) {
                    Element ele = (Element) node;
                    if (delegate.isDefaultNamespace(ele)) {
                        parseDefaultElement(ele, delegate);
                    } else {
                        // 处理用户自定义节点的处理方法
                        delegate.parseCustomElement(ele);
                    }
                }
            }
        } else {
            delegate.parseCustomElement(root);
        }
    }
}


</code></pre>
<p>如图 再看parseBeanDefinitions方法中调用的parseDefaultElement方法，顾名思义它是对节点元素进行处理。 从方法体的语句可以看出它对<br>
import标签、alias标签、bean标签进行处理。每类标签对应不同的 BeanDefinition的处理方法。</p>
<pre><code class="language-java">
private void parseDefaultElement(Element ele,BeanDefinitionParserDelegate delegate){

        // 解析&lt;import&gt;标签
        if(delegate.nodeNameEquals(ele,IMPORT_ELEMENT)){
        importBeanDefinitionResource(ele);
        }
        // 解析 &lt;alias&gt; 标签
        else if(delegate.nodeNameEquals(ele,ALIAS_ELEMENT)){
        processAliasRegistrations(ele);
        }
        // 解析bean标签
        else if(delegate.nodeNameEquals(ele,BEAN_ELEMENT)){
        processBeanDefinition(ele,delegate);
        }
        // 解析beans 标签
        else if(delegate.nodeNameEquals(ele,NESTED_BEANS_ELEMENT)){
        // recurse
        doRegisterBeanDefinitions(ele);
        }

        }

</code></pre>
<p>在parseDefaultElement 调用了众多方法中，我们选取processBeanDefinition方法给大家讲解。该方法是用来处理Bean的。<br>
首先通过delegate的parseBeanDefaultElement方法传入节点信息，获取该bean 对应的name和alias。<br>
然后通过BeanDefinitionReaderUtils中的registerBeanDefinitions方法对其进行容器注册，也就是将Bean实例 注册到容器中进行管理。最后发送注册事件。</p>
<pre><code class="language-java">
protected void processBeanDefinition(Element ele,BeanDefinitionParserDelegate delegate){
        // 该对象持有beanDefinition的name和alias ，可以使用该对象完成beanDefinition向容器注册
        BeanDefinitionHolder bHolder=delegate.parseBeanDefinitionElement(ele);
        if(bHolder!=null){
            bHolder  =  delegate.decorateBeanDefinitionElement(ele , bHolder);
            try{
                // 注册最终被修饰bean实例 ，下文注册beanDefinition到容器会讲解该方法
                BeanDefinitionReaderUtils.registerBeanDefinition(bHolder ,getReaderContext().getRegister());
           }catch(BeanDefinitionStoreException ex){
                getReaderContext.error(&quot;Faild to register bean definition with me &quot; +
                    bHolder.getBeanName() +&quot;&quot; ,ele ,ex);
            );

            // 发送注册事件
           getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bHolder));
        }
    }
}


</code></pre>
<p>到此完成了BeanDefinition的加载工作</p>
<h4 id="将beandefinition注册到容器">将BeanDefinition注册到容器</h4>
<p>在加载Bean之后，就需要将其注册到容器中进行管理。如图所示，Bean会被解析成BeanDefinition并于BeanName、Alias一同封装到BeanDefinitionHolder类中。<br>
之后beanFactory.registerBeanDefinitions(beanName ,bHolder.getBeanDefinition())方法;</p>
<p>注册到DefaultListableBeanFactory.beanDefinitionMap中。如果客户端需要获取bean对象，<br>
Spring容器会根据注册的BeanDefinition信息进行实例化。</p>
<pre><code class="language-java">
        public static registerBeanDefinition(BeanDefinitionHolder dfHolder, BeanDefinitionRegistry beanFactory)throws BeansException {
           // 注册BeanDefinition ！！！
            String beanName = bdHolder.getBeanName();
            beanFactory.registerBeanDefinition(beanName, bdHolder.getBeanDefinition());
            //如果有别名也注册进去， register aliases for bean name ， if any
                String[] aliases = bdHolder.getAliases();
                if (aliases != null){
                    for (int i = 0 ; i &lt; aliases.length ; i++){
                        beanFactory.registerAlias(beanName, aliases[i]);
                    }   
                }
        }
</code></pre>
<p>DefaultListableBeanFactory实现了上面调用BeanDefinitionRegistry接口的registerBeanDefinition(beanName ,bHolder.getBeanDefinition()) 方法。<br>
如下如所示，这一部分的主要逻辑是向DefaultListableBeanFactory对象的beanDefinitionMap存放beanDefinition，<br>
也就是说beanDefinition都存放在beanDefinitionMap中进行管理。<br>
当容器进行bean初始化时，在bean的生命周期分析里必然会在这个beanDefinitionMap中获取beanDefinition实例</p>
<pre><code class="language-java">
public void registerBeanDefinition(String beanName ,BeanDefinition beanDefinition) throws BeanDefinitionStoreException{
    
        if(beanDefinition instanceof AbstractBeanDefinition){
            try{
            ((AbstractBeanDefinition) beanDefinition).validate();
           }catch(BeanDefinitionValidationException ex){
                throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription(),beanName,
                , &quot;Validation of bean definition failed&quot; ,ex); 
          }
       }
        // beanDefinitionMap是个ConcurrentHashMap类型数据，用于存放beanDefinition，它的key 是beanName
        Object oldBeanDefinition =  this.beanDefinitionMap.get(beanName);
        if(oldBeanDefinition != null){
            if(!this.allowBeanDefinitionOverriding()){
                throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription(),beanName,
                &quot;Cannot register bean definition[&quot; + beanDefinition +&quot;]&quot; +beanName + &quot; there's already [&quot; + oldBeanDefinition +&quot;] bound&quot; )
          }
            else{
                if (logger.IsInfoEnabled()){
                    logger.info(&quot;Overriding bean definition for bean &quot; + beanName + &quot;: replacing [&quot; + oldBeanDefinition +&quot;] with&quot; + beanDefinition +&quot;]&quot; );
           }
        }
      }else{
            this.beanDefinitionNames.add(beanName);
        }
        //将获取到的BeanDefinition 放入Map 中，容器操作使用bean时，通过这个HashMap找到具体的BeanDefinition
        this.beanDefinitionMap.put(beanName, BeanDefinition);
        removeSingleton(beanName);
    }


</code></pre>
<h2 id="springioc依赖查找">SpringIOC依赖查找</h2>
<p>总所周知，Spring IOC是用来创建和管理Bean实例的，Spring IOC容器针对Bean创建实例以后，会被其他类使用，当一个实例调用另一个实例的时候，就会对调用的实例产生依赖，<br>
虽然这个依赖的实例由Spring IOC容器创建并管理，但如果要使用这个Bean实例需要通过某种途径，其中一种就成为依赖查询。依赖查询的种类很多，这里介绍最简单的两种查询方式：<br>
根据Bean名称查询，根据Bean类型查询。</p>
<h3 id="根据bean名称查找">根据Bean名称查找</h3>
<p>由于getBean(Class)最终会调用resolveNameBean 方法完成Bean的查找，因此源码分析围绕resolveNameBean展开，如下图所示，将resolveNameBean方法两张图，后面我们会针对图中的步骤<br>
逐步展开</p>
<pre><code class="language-java">  private &lt;T&gt; NameBeanHolder&lt;T&gt; resolveNameBean(ResolvableType requestType,Object[]args,boolean nonUniqueAsNull) throws BeansException {

    // 根据类型查找，不会实例化bean 
       String[] candidateNames = getBeanNameForType(requestType);
        // 多种类型  如何过滤
        
        if(candidateNames.length &gt;0 ){
            List&lt;String&gt; autoWireCandidates = new ArrayList&lt;String&gt;(candidateNames.length);
            for(String beanName : autoWireCandidates){
                //如果容器中定义了beanDefinition.autowireCandidate = false(默认为true) 则剔除
                //①没有定义该beanDefinition 或②beanDefinition.autowireCandidate = true 时合法
                // 什么场景下出现：没有定义BeanDefinition，但根据类型查找该beanName 
                if(!containsBeanDefinition(beanName) || getBeanDefinition(beanName).isAutowireCandidate()){
                    autowireCandidate.add(beanName);                    
                }       
            }
            
            if(!autowireCandidates.isEmpty()){
                candidateNames = StringUtils.toStringArray(autowireCandidates);
            }
        }
        // 单个candidateNames，则调用getBean(beanName) 实例化该Bean
        if (candidateNames.length == 1 ){
            String beanName = candidateNames[0];
            return new NameBeanHolder&lt;&gt;(beanName ,(T)getBean(beanName,requestType.toClass(),args))
        // 多个candidateNames，先尝试是否标注Primary属性，在尝试类上的@primary注解
        }else{
            Map&lt;String ,Object&gt;  candidates = new LinkedHashMap&lt;String,Object&gt;(candidateNames.length);
            for(String beanName:candidateNames){
                if(containsSingleton(beanName) &amp;&amp; args == null){
                   Object beanInstance = getBean(beanName);
                  candidates.put(beanName, beanInstance instanceof NullBean ? null : beanInstance);
                  
            }else{
                    candidates.put(beanName ,getType(beanName))
          }
      }
            //4.a 查找Primary Bean ，即beanDefinition.primary = true
            String candidateName = determinePrimaryCandidate(candidates,requiredType.toClass);
            // 比较bean的优先级 @javax.annotation.Priority
            if(candidateName == null){
                determineHighestPriorityCandidate(candidates,requiredType.toClass);
        }
            //4.b 过滤只要一个符合条件，getBean(candidateName) 实例化
        if (candidateName != null ){
            Object beanInstance = candidates.get(candidateName);
            if (beanInstance == null || beanInstance instanceof Class){
                beanInstance = getBean(candidateName,requestType.class ,args);
           }
            return new NamedBeanHolder&lt;&gt;(candidateName, (T)beanInstance);
        }
        //4.c  多个bean ，抛出NoUniqueDefinitionException异常
        if (!nonUniqueAsNull){
            throws new NoUniqueDefinitionException(requestType,candidates.keySet());
        }
    }
}

</code></pre>
<p>我们根据resolveNamedBean代码注释中的步骤解释如下：<br>
1.根据Bean类型查找Spring Ioc容器中所有符合条件的Bean名称，可能是以和或者多个。getBeanNameForType只会读取BeanDefinition信息，这里并没有对bean实例化。<br>
2.查找完类型会将其放到candidateNames中，这里过滤条件时没有定义Bean对应的BeanDefinition或者BeanDefinition的autowireCandidate为true的情况下。<br>
如果满足上诉情况就将其加入到autowireCandidates<br>
3.如果容器中只注册了一个这种类型的Bean，也就是candidateNames长度为1的时候，直接实例化该Bean后返回即可。<br>
4.当Spring IOC 容器注册多个Bean的时候需要完成一下几个步骤：<br>
5.查找primary Bean，即beanDefinition.primary = true.如果有多个，则抛出NoUniqueDefinitionException异常<br>
6.比较Bean的优先级。Spring 默认的比较器是AnnotationAwareOrderComparator，比较Bean上@javax.annitation.Priority 的优先级，<br>
值越小，优先级越高。同样的如果最高级别的多个，则抛出NoUniqueDefinitionException异常<br>
7.多个Bean，则抛出NoUniqueDefinitionException异常。</p>
<h3 id="根据bean类型查找">根据Bean类型查找</h3>
<p>与获取单个Bean实例类型过程相似的是，获取集合bean类型实例有多个candidateNames是不用过滤，全部返回即可，如下图所示，getBeanOfType同样调用getBeanNameForType获取所有bean类型匹配的beanNames，<br>
然后调用getBean(beanName)实例化所有的Bean.</p>
<pre><code class="language-java">
    @Override
    public &lt;T&gt; Map&lt;String, T&gt; getBeanOfType(@Nullable Class&lt;T&gt; type) throws BeanException {
            
        return getBeanOfType(type ,true ,true);
    }

    public &lt;T&gt; Map&lt;String, T&gt; getBeanOfType(@Nullable Class&lt;T&gt; type ,boolean includeNonSingles,boolean allowEagarInit) throws  BeanException {
       String[] beanNames = getBeanNameForType(type,includeNonSingles,allowEagarInit);
        Map&lt;String ,T&gt; result = new LinkedHashMap&lt;String, T&gt;(beanNames.length);
        for(String beanName : beanNames){
           Object beanInstance = getBean(beanName);
           if(!(beanInstance instanceof NullBean)){
               result.put(beanName, (T)beanInstance);
            }
        }
        return result;
    }

</code></pre>
<p>获取集合Bean类型<br>
如果需要获取集合Bean类型需要使用getBeanNameForType方法，Sprin内部根据类型匹配所有的beanNames。<br>
getBeanNameForType不会初始化Bean，根据其BeanDefinition 或者FactoryBean#getObjectType获取其类型</p>
<pre><code class="language-java">    
    public String[] getBeanNamesForType(@Nullable Class&lt;T&gt; type ,boolean includeNonSingletons ,boolean allowEagerInit){
    //1.查询结果不使用缓存
    //2.configurationFrozen表示是否冻结BeanDefinition,不允许修改，因此查询结果可能有误
    //一旦调用refresh 方法，则configurationFrozen = true，也就是容器启动过程中会走if语句
    // type = null,查找所有bean
    // !allowEagarInit 表示不允许提前初始化FactoryBean
    if(!isCOnfigurationFrozen || type == null !allowEagarInit) {
       return doGetBeanNamesForType(ResolvableType.forRawClass(type) ,includeNonSingletons,allowEagarInit);
    }
    //2.允许使用缓存，此容器已经启动完成，bean已加载，BeanDefinition不允许被修改
     Map&lt;Class&lt;?&gt;,String[]&gt; cache = includeNonSingletons ?this.allBeanNamesByType:this.singletonsBeanNamesByType;
    String[] resolvedBeanNames = cache.get(type);
    if(resolvedBeanNames != null){
        return resolvedBeanNames;
     }
      resolvedBeanNames =  doGetBeanNamesForType(ResolvableType.forRawClass(type) ,includeNonSingletons,true);
    if(ClassUtils.isCacheSafe){
        cache.put(type,resolvedBeanNames);
    } 
    return resolvedBeanNames;
}


</code></pre>
<p>如上图所示，方法getBeanNamesForType中调用了doGetBeanNamesForType方法。</p>
<p>参数type表示要查找的类型，includeNonSingletons表示是否包含非单例bean，allowEagarInit表示是否提前部分初始化<br>
FactoryBean。</p>
<p>1、查询结果不使用缓存，则需要满足以下场景：</p>
<ul>
<li>
<p>configurationFrozen = false ，表示Spring容器在初始化阶段，可以对BeanDefinition进行调整，当然缓存结果<br>
毫无意义。在refresh后会设置为true，表示可以对结果进行缓存。</p>
</li>
<li>
<p>type =null?</p>
</li>
<li>
<p>allowEagerInit = false;表示不允许实例化FactoryBean来获取Bean类型，那就只能通过FactoryBean上的<br>
泛型来获取其类型了，也可能导致查询结果不准确。</p>
</li>
</ul>
<p>2、允许使用缓存，当getBeanNamesForType方法根据类型查找匹配beanNames结果时，这个匹配过程并不十分准确。<br>
因为方法不会通过提前实例化Bean的方法获取其类型，只会根据BeanDefinition或者FactoryBean#getObjectType<br>
获取其类型。如果不实例化对象，有些场景可能并不能获取对象类型。因此这个使用缓存。在容器启动完成只会Bean已经加载完毕<br>
BeanDefinition就不允许修改。</p>
<h3 id="依赖查找中的经典异常bean找不到bean不是唯一的">依赖查找中的经典异常：Bean找不到？Bean不是唯一的？</h3>
<p>上节课我们通过源码方式介绍了：获取单个Bean类型实例，获取集合Bean类型实例；获取集合Bean类型名称。<br>
本节课，会给大家依赖查找中经典异常问题的讨论：Bean找不到的情况以及Bean不是唯一的情况。<br>
看看这种情况如何处理。今天主要介绍Spring 依赖查找，有如何内容：</p>
<ul>
<li>BeansException的子接口</li>
<li>NoSuchBeanDefinitionException不存在查找的Bean</li>
<li>NoUniqueDefinitionException 容器中存在多个同类型的Bean</li>
</ul>
<h4 id="beansexception">BeansException</h4>
<p>如图一所示，通常来说在操作Bean的时候会出现异常，这些异常都来自BeansException的子接口。<br>
例如查找的Bean不存在于Ioc容器，类型用来查找时IOC容器存在多个Bean实例；</p>
<p>当Bean对应的类型非具体类时；当Bean初始化过程出现异常；当BeanDefinition配置元数据时出现异常。这些异常都会在不同的场景中出现。</p>
<table>
<thead>
<tr>
<th>异常类型</th>
<th>触发条件(举例)</th>
<th>场景举例</th>
</tr>
</thead>
<tbody>
<tr>
<td>NoSuchBeanDefinitionException</td>
<td>当查找的Bean不在IOC容器时</td>
<td>BeanFactory#getBean</td>
</tr>
<tr>
<td>NoUniqueDefinitionException</td>
<td>依赖查找时IOC容器中存在多个Bean实例</td>
<td>BeanFactory#getBean</td>
</tr>
<tr>
<td>BeanInstantiationException</td>
<td>当Bean所对应的类型非具体类时</td>
<td>BeanFactory#getBean</td>
</tr>
<tr>
<td>BeanCreationException</td>
<td>当Bean初始化过程中</td>
<td>Bean初始化方法执行异常时</td>
</tr>
<tr>
<td>BeanDefinitionStoreException</td>
<td>当BeanDefinition配置元数据非法时</td>
<td>XML配置资源无法打开时</td>
</tr>
</tbody>
</table>
<p>这里我们着重讨论NoSuchBeanDefinitionException 和NoUniqueDefinitionException 两种异常的使用场景</p>
<h3 id="源码剖析spring-ioc容器bean初始化流程">源码剖析：Spring IOC容器Bean初始化流程？</h3>
<p>通过SpringIOC容器初始化Bean的流程，在之前的章节中介绍了SpringIOC容器时如何从配置文件中读取Bean信息，并且将其转换为BeanDefinition，并且注册到SpringIOC<br>
容器中进行统一管理的。今天讲的是，在SpringIOC容器中的Bean实例是如何被创建出来的，也就是解决如何从SpringIOC取出Bean进行使用。</p>
<p>创建Bean的时序图</p>
<figure data-type="image" tabindex="2"><img src="../framework/images/Bean%E5%88%9B%E5%BB%BA%E6%97%B6%E5%BA%8F%E5%9B%BE.png" alt="创建Bean时许图" loading="lazy"></figure>
<p>由于一个Bean的创建经历了若干个类，调用了很多方法，为了将这个过程理解清楚，这里根据图1的时序图给大家展开描述。<br>
这里涉及到几个主要调用的类，我们会根据重点，按照图上红色数字标号进行讲解。</p>
<p>如图一所示，按照红色标号的顺序如下：</p>
<p>1.首先通过ClassPathXmlApplicationContext获取要创建的Bean，这里的方法是getBean。</p>
<p>2.在AbstractBeanFactory中会通过调用doGetBean方法完成获取Bean的操作。</p>
<p>3.此时DefaultSingletonBeanResigstry会执行getSingleton通过单例的方式返回Bean。</p>
<p>4.在获取Bean以后会通过createBean方法调用AbstractAutowireCapableBeanFactory中的方法对Bean进行初始化。<br>
这里重要的三个方式分别是：doCreateBean、createBeanInstance、instantiateBeanBean。从图上可以看出Bean初始化都是在这个类中完成的。</p>
<p>5.紧接着通过instantiate通过调用SimpleInstantiationStrategy中的instantiateWithMethodInjection做初始化的操作。</p>
<p>6.CglibSubclassingInstantiationStrategy作为CGLIB代理实现策略类，通过CglibSubclassCreator.instantiate方法进行初始化，并且返回<br>
AbstractAutowireCapableBeanFactory。此时AbstractAutowireCapableBeanFactory会接着populateBean和ApplyPropertyValues方法对Bean进行填充，以及设置属性值。</p>
<p>7.期间会通过调用beanDefinitionValueResolver中的resolveValueIfNecessary方法完成对BeanDefinition中的值填充。</p>
<h3 id="springioc依赖查注入">SpringIOC依赖查注入</h3>
<p>依赖注入的方式：构造器注入，set注入</p>
<p>SpringIOC 依赖注入也称为自动转载，当两个类存在依赖关系，通过配置Bean元素的autowire属性自动装配<bean>标签的方式获取依赖类的实例。<br>
这里介绍两种常见的依赖注入方式:</p>
<ul>
<li>
<p>根据Bean名称注入</p>
<p>根据Bean名称注入，使用byName标签，此选项将检查容器并根据名字查找与属性完全一致的bean,并将其与属性自动装配。</p>
</li>
<li>
<p>根据Bean类型注入</p>
<p>根据Bean类型注入，使用byType标签，如果容器中存在一个与指定属性类型相同的Bean,那么将其与该属性自动装配；<br>
如果存在多个该类型Bean,那么抛出异常，并指出不能使用byType方式进行自动状态；<br>
如果没有找到向匹配的Bean，则什么事情都不会发生。</p>
</li>
</ul>
<h3 id="源码剖析autowire注解依赖注入的原理">源码剖析：@Autowire注解依赖注入的原理</h3>
<h4 id="autowire的定义">@Autowire的定义</h4>
<pre><code>通常来说，我么会使用@Autowire进行依赖注入的操作，如图1所示通过@Autowire注解的源码定看出，它被标注可以作用于：构造函数（Constructor）、方法（Method）、参数（Parameter）、字段（Field）
注解（Annotation），也就是针对这些描述都可以使用@Autowire的方式实现自动的依赖注入。
</code></pre>
<pre><code class="language-java">

@Target({ElementType.CONSTRUCTOR, ElementType.METHOD, ElementType.PARAMETER, ElementType.FIELD, ElementType.ANNOTATION_TYPE})
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface Autowired {

	/**
	 * Declares whether the annotated dependency is required.
	 * &lt;p&gt;Defaults to {@code true}.
	 */
	boolean required() default true;

}



</code></pre>
<h4 id="autowire的工作原理">@Autowire的工作原理</h4>
<p>@Autowire注解是由AutowireAnnotationBeanProcessor实现的，如图2所示，查看该类的源码会发现它实现了MergedBeanDefinitionPostProcessor接口。</p>
<pre><code class="language-java">
public class AutowiredAnnotationBeanPostProcessor extends InstantiationAwareBeanPostProcessorAdapter
		implements MergedBeanDefinitionPostProcessor, PriorityOrdered, BeanFactoryAware {


</code></pre>
<p>如图所示，同时AutowiredAnnotationBeanPostProcessor实现了MergedBeanDefinitionPostProcessor中的postProcessMergedBeanDefinition方法，<br>
@Autowired注解正是通过这个方法实现注入类型类型的预解析，将需要依赖注入的属性信息封装到InjectionMetadata类中。</p>
<pre><code class="language-java">
	@Override
	public void postProcessMergedBeanDefinition(RootBeanDefinition beanDefinition, Class&lt;?&gt; beanType, String beanName) {
		InjectionMetadata metadata = findAutowiringMetadata(beanName, beanType, null);
		metadata.checkConfigMembers(beanDefinition);
	}


</code></pre>
<p>如下图所示，InjectionMetadata类中包含了需要注入的元素(injectElements)及元素注入的目标类(tagetClass)</p>
<pre><code class="language-java">    
public class InjectionMetadate{
    private static final Log logger = LogFactory.getLog(InjectionMetadata.class);

    private final Class&lt;?&gt; targetClass;

    private final Collection&lt;InjectedElement&gt; injectedElements;

    @Nullable
    private volatile Set&lt;InjectedElement&gt; checkedElements;
}

</code></pre>
<p>既然AutowiredAnnotationBeanPostProcessor是实现Autowire主力类，那么如何执行它实现依赖注入呢？<br>
Spring容器在启动的时候会执行AbstractApplicationContext类的refresh方法，refresh方法中<br>
registerBeanPostProcessor(beanFactory)完成了对AutowireAnnotationBeanPostProcessor的注册，当执行finishBeanFactoryInitialization(beanFactory)方法<br>
对非延迟初始化的单例bean进行初始化时，会执行AbstractAutowireCapableBeanFactory类的doCreateBean方法。</p>
<pre><code class="language-java">    public void refresh() throws BeansException, IllegalStateException {
        synchronized(this.startupShutdownMonitor) {
            this.prepareRefresh();
            ConfigurableListableBeanFactory beanFactory = this.obtainFreshBeanFactory();
            this.prepareBeanFactory(beanFactory);

            try {
                this.postProcessBeanFactory(beanFactory);
                this.invokeBeanFactoryPostProcessors(beanFactory);
                this.registerBeanPostProcessors(beanFactory);
                this.initMessageSource();
                this.initApplicationEventMulticaster();
                this.onRefresh();
                this.registerListeners();
                this.finishBeanFactoryInitialization(beanFactory);
                this.finishRefresh();
            } catch (BeansException var9) {
                if (this.logger.isWarnEnabled()) {
                    this.logger.warn(&quot;Exception encountered during context initialization - cancelling refresh attempt: &quot; + var9);
                }

                this.destroyBeans();
                this.cancelRefresh(var9);
                throw var9;
            } finally {
                this.resetCommonCaches();
            }

        }
    }

</code></pre>
<p>如图6所示doCreateBean方法中回去调用applyMergedBeanDefinitionPostProcessor方法</p>
<pre><code class="language-java">
 synchronized(mbd.postProcessingLock){
    
    if(!md.postProcessed){
      try {
          applyMergedBeanDefinitionPostProcessors(mbd,beanType,beanName);
          
         }catch{
           throw new BeanCreationException(
                   mbd.getResourceDescription(),beanName,&quot;Post-processing of merge bean definition failed&quot;,ex);
         }
      mbd.postProcessed = true;
     }
 }

</code></pre>
<p>如图所示，查看postProcessMergedBeanDefinition方法具体的实现类，会发现调用的是<br>
AutowireAnnotationBeanPostProcessor类postProcessMergedBeanDefinition方法，这个方法中完成了对注入元素<br>
注解的预解析。</p>
<p>最后还是吧眼光移回doCreateBean方法中会遍历所有注册过的BeanPostProcessor接口实现类的实例，如果实例属于<br>
InstantiationAwareBeanPostProcessor类型的，则会执行实例类的postProcessPropertyValues方法</p>
<pre><code class="language-java">
    if(hasInstAwareBpps || needsDepCheck){
        PropertyDecriptore[] filteredPds = filterPropertyDescriptorForDependencyCheck(bw,mbd.allowCaching);
        if (hasInstAwareBpps) {
            for (BeanPostProcessor bp : getBeanPostProcessors()) {
                if(bo instanceof InstantiationAwareBeanPostProcessor) {
                    InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp
                    pvs = ibp.postProcessPropertyValues(pvs,filtredPds,bw.getWrappedInstance(),beanName);
                    if (pvs == null){
                        return ;    
                    }
                }       
            }
            if(needsDepCheck){
                checkDependcies(beanName,mbd,filteredPds,pvs);
            }
        }
        
        applyPropertyValues(beanName,mbd,bw,pvs);
    }

</code></pre>
<p>如下图所示，进入到postProcessPropertyValues方法内部。由于AutowiredAnnotationBeanPostProcessor继承了InstantiationAwarePostBeanPostProcessorAdapter<br>
,而AutowiredAnnotationBeanPostProcessor间接实现了InstantiationAwareBeanPostProcessor接口，所以这里会执行到<br>
AutowiredAnnotationBeanPostProcessor类的postProcessPropertyValues方法。其中会调用metadata.inject(bean ,beanName,pvs)代码进行注入操作。</p>
<pre><code class="language-java">
public void  inject(Object target ,String beanName ,PropertyValues pvs) throws Throwable {
          Collection&lt;InjectedElement&gt; elementsToInterate = this.checkedElements != null? this.checkElements:this.injectedElements);
          if(!elementsToInterate.isEmpty()){
              boolean debug =  logger.isDebugEnabled();
              for (InjectedElement element : elementsToInterate) {
                    if(debug) {
                        logger.debug(&quot;Processing injected element of bean &quot; + beanName +&quot;:&quot; +element)
                }           
              }
              element.inject(target,beanName,pvs);
        }        
          
}
</code></pre>
<p>上面两种方式的注入过程都差不多，根据需要注入的元素的描述信息，按类型或者名称查找需要的依赖值，如果依赖没有找到实例先实例化依赖，然后使用反射进行赋值。<br>
下图是field注入的场景，</p>
<pre><code class="language-java">
 if (value != null) {
     ReflectionUtils.makeAccessible(field);
     field.setValue(bean ,value);
 }

</code></pre>
<p>field 注入</p>
<pre><code class="language-java">
if (arguments != null){
     try{
        ReflectionUtils.makeAccessible(field);
        method.invoke(bean ,arguments);
    }catch(InvocationTargetException ex){
         throw ex.getTargetException();
    }
}
</code></pre>
<h3 id="精选面试题autowire和resource区别">精选面试题：@Autowire和@Resource区别</h3>
<h4 id="resource的注入方式">@Resource的注入方式</h4>
<p>上一节课介绍的是@Autowire的注入方式，在对@Resource和@Autowire做区别之前先介绍一下@Resource的常用注入属性name和type，这里根据组合分为4种情况：</p>
<ul>
<li>指定name和type：通过name找到唯一bean，找不到抛出异常；如果type和字段类型不一致，也会抛出异常。</li>
<li>指定name：通过name找到唯一的bean，找不到则抛出异常</li>
<li>指定type：通过type找唯一的Bean，如果不唯一，则抛出异常：NoSuchBeanDefinitionException。</li>
<li>不指定name和type：通过字段名作为key去查找；找不到则通过字段类型去查找，如果不唯一，则抛出异常：NoSuchBeanDefinitionException</li>
</ul>
<p>@Resource装配的顺序如下：</p>
<p>1.如果同时指定了name和type吗，则会从Spring上下文种找到唯一匹配的bean进行装配，找不到则抛出异常。<br>
2.如果指定了name，则从Spring上下文种查找名称(id)匹配的bean进行装配，找不到则抛出异常<br>
3.如果指定了type，则从Spring上下文种找到指定类型匹配的唯一Bean进行装配，找不到或者找到多个都抛出异常<br>
4.如果既没指定name，也没有指定type，则自动按照byName方式进行装配，如果没有匹配，则回退一个原始类型进行匹配，如果匹配则自动装配。</p>
<p>@Resource与@Autowire区别<br>
通过对@Resource的介绍发现实现自动装配的方式与@Autowire大同小异，那么到底他们之间有什么区别呢？我们从以下几个方面来看：</p>
<ul>
<li>@Autowire是Spring定义的注解 ，而@Resource是JSR-250定义的注解</li>
<li>@Autowire默认按照ByType自动装配，而@Resource默认byName自动装配</li>
<li>@Autowire只包含一个参数：required，表示是否开启自动装配,默认是true。而@Resource包含七个参数，其中最重要的参数是name和type。</li>
<li>@Autowire如果使用byName，需要使用@Qualified一起配合。而@Resource如果指定name,则用byName自动装配，如果指定了byType，则用byType自动装配。</li>
<li>@Autowire能够用在构造器，方法，参数，成员变量和注解上。而@Resource能用在类、成员变量、方法上。</li>
</ul>
<p>另外他们的装配顺序不同</p>
<p>@Autowire装配顺序，按照类型查找Bean，如果没有找到直接抛出异常，如果找到了，判断是否找到一个bean，如果是只有一个bean进行自动装配。<br>
如果存在多个bean的情况判断是否配置了Qualifier，如果配置了那么按照Qualifier参数查找bean,如果找到进行自动装配，否则抛出异常。<br>
如果没有配置Qualifier按照名称查找bean，找到了就自动装配，否则抛出异常。</p>
<figure data-type="image" tabindex="3"><img src="../framework/images/spring-autowire.png" alt="Autowired自动装配" loading="lazy"></figure>
<p>@Resource同时指定了name和type装配顺序如下图所示</p>
<p>如果指定了name和type的情况，回去查找name和type唯一的bean如果没有找到抛出异常，如果找到了自动装配。</p>
<figure data-type="image" tabindex="4"><img src="../framework/images/spring-resource-name-type.png" alt="Resource装配" loading="lazy"></figure>
<p>当Resource指定了name的情况，会查找name唯一匹配的bean，如果没有找到抛出异常，如果找到了自动装配。</p>
<figure data-type="image" tabindex="5"><img src="../framework/images/spring-resource-name.png" alt="Resource装配" loading="lazy"></figure>
<p>当Resource指定了type的情况，会查找type唯一匹配的bean，如果没有找到抛出异常，如果找到了自动装配。</p>
<figure data-type="image" tabindex="6"><img src="../framework/images/spring-resource-type.png" alt="Resource装配" loading="lazy"></figure>
<p>当Resource既没指定name又没指定type时，首先按照名称查找bean，如果没有找到在按照类型查找bean，如果没有找到或者找到多个就抛出异常，如果找到一个就进行自动装配。</p>
<figure data-type="image" tabindex="7"><img src="../framework/images/spring-resource-noname-notype.png" alt="Resource装配" loading="lazy"></figure>
<h3 id="spring-ioc依赖来源有哪些">Spring IOC依赖来源有哪些？</h3>
<p>前面两节课介绍了Spring IOC的依赖查找和依赖注入的两种方式，实际上针对这两种方式都对应了依赖来源。今天针对SpringIOC的依赖展开介绍，包括如下内容：</p>
<ul>
<li>自定义Bean作为依赖项</li>
<li>容器内建Bean对象作为依赖来源</li>
<li>容器内建依赖作为依赖来源</li>
</ul>
<p>容器内建的Bean对象并不是我们创建的而是Spring内部实例化产生的，说白了就是Spring内部自己需要使用从而创建的Bean。同时也可以通过beanFactory.getBean()的方式进行依赖查找。</p>
<ul>
<li>Environment对象，在外部化配置和Profiles场景使用</li>
<li>java.util.Properties对象，被用到Java系统属性场景。</li>
<li>Java.util.Map对象，可以使用到操作系统环境变量的场景</li>
<li>MessageSource对象，作为国际化文案</li>
<li>LifecycleProcessor对象，用来处理LifecycleBean处理器</li>
<li>ApplicationEventMulticaster对象，用来处理Spring时间广播器</li>
</ul>
<p>容器内建依赖作为依赖项</p>
<p>容器内建依赖也称作为非Bean对象，为什么这么说呢？因为这类对象并不是Spring 的Bean，比如说Bean Factory就是这类对象，<br>
但是这类对象必须时Spring启动必须的组件，也就是基础组件。基于以上原因容器内建依赖是不能通过beanFactory.getBean()的方式依赖查找出来的。</p>
<p>对于容器内建依赖具有一下特征：无生命周期管理，无法实现延迟初始化Bean，无法通过依赖查找。此类对象包括：<br>
beanFactory、ResourceLoader、ApplicationEventPublisher、ApplicationContext从名字上看都是系统级别的对象。<br>
如果一定要对这些变量注册，可以使用ConfigurableListableBeanFactory种的registerResolvableDependency方法。</p>
<h3 id="精选面试题value是如何将外部化配置注入到spring-bean中">精选面试题：@Value是如何将外部化配置注入到spring bean中？</h3>
<p>前两节课介绍了SpringIOC依赖查找和依赖注入的两种方式，实际上针对这两种方式都对应了依赖来源。今天给大家讲解@Value如何将外部化配置<br>
注入到Spring Bean中，包括如下内容：</p>
<ul>
<li>@Value获取配置的介绍</li>
<li>基于配置文件注入</li>
<li>基于非配置文件注入</li>
<li>默认值注入</li>
</ul>
<h4 id="value获取配置的简介">@Value获取配置的简介</h4>
<p>@Value 是一种获取外部配置的方式，是SpringBoot中的一种用法，在SpringBoot允许将配置文件外部化，这样能够在不同的环境下<br>
使用相同的代码。可以使用properties文件，yml文件环境变量和命令行参数来外部化配置。使用@Value注解，可以直接将属性值注入到beans中。<br>
然后通过Spring的Environment抽象或者通过@ConfigurationProperties绑定到结构化对象来访问，这里我们介绍3种文件注入的方式，<br>
分别是基于配置文件注入，基于非配置文件注入，默认值注入。</p>
<h4 id="基于配置文件注入">基于配置文件注入</h4>
<pre><code>基于配置文件注入，顾名思义起源头来自于配置文件。这些配置文件注入applcation.properties或者自定义*.properties.
例如：application.properties配置文件种定义属性值的形式如下；
user.name=admin
假设存在自定义配置文件my.properties,配置文件中定义了如下属性
user.password=123456 
如果需要做在类中使用这两个配置值如何做呢，看如下编码：
</code></pre>
<pre><code class="language-java">    @PropertySource(&quot;classpath:my.properties&quot;)
    @RestController
    public class ValueController{
    
     /**
      *
       *获取application.properties中的配置属性
       */
     @Value(&quot;$(user.name)&quot;)
     private String name;
    /**
     *
     *获取my.properties中的配置属性
     */
    @Value(&quot;$(user.password)&quot;)
    private String password;
}

</code></pre>
<pre><code>从上面代码可以看出从application.properties配置中获取user.name的信息填充到变量name中，
在从自定义配置文件中获取my.properties中user.properties填充到password变量中。

值得注意的是，如果自定义my.properties文件，需要在某个类中通过PropertySource引入改配置文件额，而application.properties中的属性会自动被加载。


通过@Value注入单个舒心的同时，也可以注入数组和列表形式。如果存在一下配置：
tools=car,train,airplane

那么可以通过如下代码进行注入：
</code></pre>
<pre><code class="language-java">
    /**
     * 
     * 注入数组，自动根据&quot;,&quot;分割
     */
    @Value(&quot;${tools}&quot;)
    private String[] tools;
    /**
     *
     * 注入列表，自动根据&quot;,&quot;分割
     */
    @Value(&quot;${tools}&quot;)
    private List&lt;String&gt; tools;
</code></pre>
<pre><code>Spring默认情况下会以&quot;,&quot;进行分割，将配置文件中的tool包含内容切割城&quot;car&quot;，&quot;train&quot;，&quot;airplane&quot;组成的数组或者列表保存到数组或者列表中。
</code></pre>
<h4 id="基于非配置文件注入">基于非配置文件注入</h4>
<pre><code>  基于非配置文件注入，需要用到SPEl(Spring Expression Language)即Spring表达式语言对@Value进行修饰而传递配置信息

  下面就来看几个应用场景

  注入普通字符串，相当于直接给属性默认值，代码如下：
</code></pre>
<pre><code class="language-java">
    /**
     *注入普通字符串，代码如下：
     */
    @Value(&quot;测试&quot;)
    private String wechatSubscription;

    /**
     *注入普通字符串，代码如下：
     */
    @Value(&quot;#{systemProperties['os.name']}&quot;)
    private String systemPropertiesName;

    /**
     *注入表达式结果，代码如下：
     */
    @Value(&quot;#{T(java.lang.math).random()*100}&quot;)
    private String systemPropertiesName;
    /**
     *注入其它Bean属性:注入config对象属性tool，代码如下：
     */
    @Value(&quot;#{config.tool}&quot;)
    private String systemPropertiesName;
    
    /**
     *注入列表形式（自动根据&quot;｜&quot;分割），代码如下：
     */
    @Value(&quot;#{'${word}'}.split('\\|')&quot;)
    private List&lt;String&gt; numList;


    /**
     *注入URL资源，代码如下：
     */
    @Value(&quot;http://www.baidu.com&quot;)
    private List&lt;String&gt; numList;

</code></pre>
<h4 id="默认值注入">默认值注入</h4>
<pre><code>说了配置文件和非配置文件的#Value注入，这里再加入默认值的注入方式，顾名思义就是在配置为空或者没有设置具体值的时候，使用默认值填充目标对象。
这里我们整理了集中情况，大家参考。
</code></pre>
<pre><code class="language-java"> /**
  *如果属性中为配置ip，则使用默认值，代码如下：
  */

 @Value(&quot;${ip:127.0.0.1}&quot;)
 private String ip;

/**
 *如果系统中未获取到port的值，则使用8888，代码如下：
 * 其中${}中直接使用：对未定义或者未空的值进行默认值设置，而#{}则需要使用&quot;？：&quot;对未设置的属性进行默认值设置。
 */

@Value(&quot;#{systemProperties['port']？:8888}&quot;)
private String port;

 
</code></pre>
<h3 id="精选面试题spring-ioc如何解决循环依赖问题">精选面试题：spring IOC如何解决循环依赖问题？</h3>
<h4 id="什么是循环依赖">什么是循环依赖</h4>
<pre><code>SpringIOC中循环依赖其实就是循环引用，两个或者两个以上的Bean互相持有对方，最终形成闭环。如图所示，如A依赖于B，
B依赖于C，C又依赖于A。这种一个场景，初始化A的时候需要出初始化B的初始化，而完成B的初始化又需要完成初始化C的初始化，
最后C又依赖于A，如此这般A永远无法完成初始化操作。这种对象相互依赖形成闭关的关系称为循环依赖。
</code></pre>
<figure data-type="image" tabindex="8"><img src="../framework/images/xunhuanyilai.png" alt="循环依赖" loading="lazy"></figure>
<pre><code>在SpringIOC的使用场景中有两类循环依赖是无解的：
</code></pre>
<ul>
<li>构造器循环依赖：构造器要调用构造函数new出一个对象来，而参数有依赖另外一个对象。创建类A依赖于类B，new的时候去创建类B发现类B不存在就会抛出BeanCurrentlyCreationException异常</li>
<li>prototype原型bean的初始化过程中不论是通过构造器参数循环依赖还是通过set方式产生循环依赖也会抛出异常.</li>
</ul>
<p>然而针对singleton bean的循环依赖的场景可以通过三级缓存的方式解决,下面就根据该解决方案展开说明.</p>
<h4 id="spring-ioc处理循环依赖的思路">spring ioc处理循环依赖的思路</h4>
<p>再整理spring ioc处理singleton bean循环依赖的思路之前先来复习一下bean的生命周期,其包括三个步骤:</p>
<ul>
<li>实例化,执行bean的构造方法,bean中的依赖对象还未赋值.</li>
<li>设置属性,给bean中依赖的对象赋值,若被依赖的对象尚未初始化,则先进行该对象的生命周期(递归).</li>
<li>初始化:执行bean的初始化方法,回调方法等.</li>
</ul>
<p>解决循环依赖的思路就藏在这三个步骤中,在实例化于设置属性两个不走之间引入缓存机制,将已经建好的实例但是并没有设置属性的bean放入缓存中,缓存中是没有属性设置的实例对象,假设A对象和B对象相互依赖,<br>
A对象的创建需要引用B对象,而B对象的创建也需要A对象.在创建A对象的时候可以将其放入缓存中,当B对象创建的时候直接从缓存里引用A对象,<br>
(此时的A对象只完成了实例化,并没有经行设置属性的操作,因此不是完成的A对象,我们称之为半成品A对象),当B对象利用这个半成品的A对象完成实例创建以后(三个步骤都完成),在被A对象引用进去,则A对象也完成了创建.</p>
<p>上文提到的缓存在这里做个解释,我们将其分为三级,每级缓存都起到不同的作用,如下表格所示</p>
<table>
<thead>
<tr>
<th>源码</th>
<th>级别</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>singletonObjects</td>
<td>一级缓存</td>
<td>用于存放完全初始化好的bean,从该缓存中读取的bean可以直接使用</td>
</tr>
<tr>
<td>earlySingletonObjects</td>
<td>二级缓存</td>
<td>存放原始的bean对象(尚未填充属性),用于解决循环依赖</td>
</tr>
<tr>
<td>singletonFactories</td>
<td>三级缓存</td>
<td>存放bean的工厂对象,用于解决循环依赖</td>
</tr>
</tbody>
</table>
<p>解决循环依赖的整个过程是:</p>
<p>先从一级缓存里取出bean实例,如果没有对应的bean实例,二级缓存里取,如果二级缓存里也没有bean实例,singletonFactories<br>
三级缓存里获取.由于三级缓存存放着产生bean实例的工厂类,因此也可以通过该工厂类产生bean实例.</p>
<p>这里可以调用工厂类调用的getObjects方法返回早期暴露对象引用,也就是我们所说的半成品bean,也可以成为earlySingletonObjects.<br>
并且将这个半成品的bean放到二级缓存里,在三级缓存里删除该bean.什么时候这个半成品填充了属性之后,就被移动到一级缓存中.<br>
也就是被称为可以使用完成初始化实例bean了,处理循环依赖的过程宣告完毕,下面通过一个例子让大家更好的理解这个思路.</p>
<h4 id="处理循环依赖举例">处理循环依赖举例</h4>
<p>根据上面的思路,这里假设A和B互相依赖,如图所示,在A创建实例的时候使用getBean方法,通过createBeanInstance方法对A进行实例化.此时的A只是被实例化出来了,<br>
也把创建B实例的工厂类通过addSingletonFactory方法添加到三级缓存中.上面的思路中提到了这个放到三级缓存中的工厂类使用用来生成bean实例用的.</p>
<p>接着往下,当通过populateBean填充实例A的属性的时候发现,A依赖B.此时开始通过getBean方法创建B的实例,依旧通过createBeanInstance方法对bean进行实例化,<br>
也就是把创建B实例的工厂类通过addSingletonFactory方法添加到三级缓存.在使用populateBean方法填充B的属性时,发现B依赖A,此时通过getBean方法对A进行实例化.</p>
<p>这个时候就出现了循环依赖的情况了,getBean方法先从一级缓存中获取A实例,发现没有,再去二级缓存中找,还是找不到,没办法,只能到三级缓存中A实例创建工厂取创建A实例.前面<br>
三个步骤中A已经将工厂类通过addSingletonFactory方法存放到三级缓存中,于是调用A的工厂类创建A的实例,并且将其放到二级缓存中返回给B用来填充B的属性,当B完成属性填充<br>
以后产生了B的实例,返回给populateBean(A)使用,此时A获取了B实例(完成属性填充的B的实例).</p>
<p>所以,A也可以完成属性填充从而产生A的初始化以后的实例并且将其放入到一级缓存中.由于B之前使用的时A的实例是没有做属性填充的,也就是半成品A的实例,<br>
因此此时充一级缓存中获取成品A实例完成B对象的初始化.</p>
<h3 id="jdk动态代理和cglib动态代理">JDK动态代理和CGLIB动态代理</h3>
<h4 id="aop的基本概念">AOP的基本概念</h4>
<p>AOP(Aspect Orient Programming),直译过来就是面向切面编程.AOP是一种编程思想,是面向对象编程(OOP)的一种补充.面向对象编程将程序抽象成各个层次的对象,<br>
而面向切面编程是将程序抽象晨光各个切面.</p>
<p>如图一所示,切面实现了横切关注点,也就是跨多个应用对象的逻辑,假设业务服务1,2,3各自实现业务不仅相同,但是有些系统级别的应用是可以抽象的,例如蓝色箭头中标注的<br>
日志,安全,事务.</p>
<p>所谓切面,相当于应用对象的横切点,可以黄其抽象为单独的模块.其1中的切面就是日志,安全,事务,换句话说可以将日志,安全,事务抽象成独立的模块.</p>
<h4 id="为什么要使用aop">为什么要使用AOP</h4>
<p>在实际开发过程中,业务代码中会不断重复的代码,例如:记录日志,判断权限等.遇到这种场景就可以将这些业务无关的代码进行抽象成一个方法.这种问题就可以利用AOP来解决.</p>
<p>AOP可以保证开发者不修改源代码的前提下,为业务组件添加通用功能.其本质是通过AOP框架将通用代码植入到业务组件方法中.</p>
<p>按照AOP框架修改源代码的时机,可以将其分为两类:</p>
<p>静态AOP实现,AOP框架在编译阶段对程序源代码进行修改,生成静态AOP代理类(生成的*.class文件已经被改掉了,需要使用特定的编译器),比如AspectJ.</p>
<p>动态AOP实现,AOP框架在运行阶段对动态生成代理对象(在内存中以JDK动态代理和CGLIB动态代理生成AOP代理类),比如Spring AOP.</p>
<p>如图所示,这里列出了AOP的几大类别,其中国以动态AOP的两个类别使用最为广泛,他们就是JDK动态代理和CGLIB动态代理.接下来就让我们一起来看看他们是如何实现AOP的吧.</p>
<table>
<thead>
<tr>
<th>类别</th>
<th>机制</th>
<th>原理</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>静态AOP</td>
<td>静态织入</td>
<td>在编译起，切面直接以字节码的形式编译到目标字节码文件中</td>
<td>对系统无性能影响</td>
<td>灵活度不够</td>
</tr>
<tr>
<td>动态AOP</td>
<td>JDK动态代理</td>
<td>在运行期，目标类加载后，为接口动态生成代理类，将切面织入到代理类中</td>
<td>相对于静态AOP更加灵活</td>
<td>切入的关注点需要实现接口，对系统有一点性能影响</td>
</tr>
<tr>
<td>动态字节码生成</td>
<td>CGLIB</td>
<td>在运行期目标类加载后，动态生成目标类的子类，将切面逻辑加入到子类中</td>
<td>没有接口也可以织入</td>
<td>扩展类的实例用final修饰时，则无法进行织入</td>
</tr>
<tr>
<td>自定义类加载器</td>
<td></td>
<td>在运行期目标类加载前，将切面逻辑驾到目标字节码里</td>
<td>可以对绝大部分类进行织入</td>
<td>代码中如果使用其它类加载器，则这些类将不会被织入</td>
</tr>
<tr>
<td>字节码转换</td>
<td></td>
<td>在运行期，所有类加载器加载字节码前进行拦截</td>
<td>可以对所有类进行织入</td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="jdk动态代理">JDK动态代理</h4>
<p>AOP框架有很多，Spring中的AOP是通过动态代理来实现的。这里就介绍JDK动态代理的实现原理。JDK动态代理是在运行期，目标类加载后，为接口动态生成代理类，将切面织入代理类类中。</p>
<p>这里通过一个例子类讲解JDK动态代理的实现原理。如下图所示，定义Service接口，其中有一个help方法。用ServiceImpl去实现Service接口，<br>
在help方法中实现打印买书的输出。这个ServiceImpl就是我们要代理的目标类。</p>
<pre><code class="language-java">public interface Service {

    public void help();
}


public class ServiceImpl implements Service {
    @Override
    public void help() {
        System.out.println(&quot;买书&quot;)
    }
}
</code></pre>
<p>接下来实现代理类，如图所示，dynamicProxy作为动态代理类实现了invocationHandler接口，其中定义了Service的接口作为<br>
代理类dynamicProxy构造方法的输入参数，在Override的invoke方法中通过参数Method来代理要执行的代理类中的方法，<br>
method.invoke的输入参数为目标类Service。</p>
<p>method.invoke是用来执行代理类中的help方法，如果这个假设成立的话，在这个方法执行前面执行的<br>
System.out.println(&quot;买书之前&quot;)，会在买书方法之前执行，同理System.out.println(&quot;买书之后&quot;)会在买书方法之后执行</p>
<pre><code class="language-java">public class DynamicProxy implements InvocationHandler{
    //代理的真实对象
    private Object service;
    
    //构造方法，给代理的真是对象赋值
    public DynamicProxy(Object service){
        this.service = service;
    }
    
    public Object invoke(Object object, Method method ,Object[] args) throws  Throwable{
        
        //真是方法执行之前
        System.out.println(&quot;买书之前&quot;);
        //调用真是方法
        method.invoke(service, args);
        //真是方法执行之后
        System.out.println(&quot;买书之后&quot;);
        return  null;
    }
    
    
}

</code></pre>
<p>回过头我们测试一些安JDK的代理类是否工作，如图所示，在main函数中通过proxy的newProxyInstance方法传入ClassLoader的类加载器，同时传入Service的接口，<br>
以及代理类DynamicProxy(以InvocationHandler的形式)。然后执行代理类的help方法。</p>
<pre><code class="language-java">
public class {
    
    public static void main(String[] args) {
        //要代理的对象
        Service service = new ServiceImpl();
        //要代理哪个真实对象，就将该对象传进去，最后是通过该真实对象来调用其方法
        InventoryHandler handler = new DynamicProxy(service);
        // 添加一下的几段代码，就可以将代理生成的字节码保存起来
        try {
            Service serviceProxy = Proxy.newProxyInstance(service.getClass().getClassLoader(),service.getClass().getInterface(),handler);
            service.help();
        }catch (NoSuchFieldException e){
            e.printStackTrace();
        }catch(IllegalAccessException e){
            e.printStackTrace();
        }
    }    
    
}

</code></pre>
<h4 id="cglib动态代理">CGLIB动态代理</h4>
<p>CGLIB(Code Generation Library)是一个开源项目，高性能高质量的Code生成类库，它可以在运行期扩展java类与实现Java接口。其原理是在运行期间，通过字节码的方式<br>
在目表类的子类中织入对应的代码完成代理。</p>
<p>还是通过一个例子来看，依旧定义一个Biz的目标类，里面有一个help方法打印买书，我们要对这个方法进行织入，在这个方法的前后打印买书之前和买书之后的打印语句。<br>
定义一个BizTntercepter作为代理类，实现MethodIntercepter的接口，并且Override intercept方法。</p>
<p>介绍一下方法的参数，Object是生成的子类对象，Method是要代理的目标类的方法，object[]是参数，MethodProxy子类生成代理方法。通过methodProxy.invokeSuper方法执行生成子类的代理方法，<br>
第一个输出的是目标类生成的子类，第二个输出的是参数，该方法就是调用目标类biz中的help方法。</p>
<p>最后就是执行测试类的方法，在main函数中new一个enhandcer，setSuperclass执行目标类，通过setCallBack方法指定代理类，最后通过使用create方法生成Biz的实例，并且执行对应的help方法。</p>
<pre><code class="language-java">
public class Biz{
    /**
     *  目标类
     * @author zxy
     * @date 2021/9/15 10:22
     */
    public void help(){
        System.out.println(&quot;买书&quot;);
    }
}


/**
 * 代理类
 * @author zxy
 * @date 2021/9/15 10:24
 */
public class BizInterceptor implements MethodInterceptor{
    //Object 是生成类的子类对象，Method是要代理目标类的方法，Object[]是参数，MethodProxy子类生成的代理方法
    @Override
    public Object intercept(Object o ,Method method ,Object[] object ,MethodProxy methodProxy) throws Throwable {
        System.out.println(&quot;买书之前&quot;);
        methodProxy.invokeSuper(o,object);
        System.out.println(&quot;买书之后&quot;);
        return null;
    }
    
    
}


public BizCglibClient{
    
    public static void main(String[] args){
        EnHancer enHancer = new EnHancer();
        enHancer.setSuperClass(Biz.class);
        enHancer.setCallback(new BizInterceptor());
        Biz biz = enHancer.create();
        biz.help();
    }
}

</code></pre>
<h4 id="jdk动态代理和cglib动态代理区别">JDK动态代理和CGLIB动态代理区别</h4>
<p>1.介绍完两种代理模式，这里稍微总结一下两者的区别。</p>
<ul>
<li>JDK动态代理只能对实现了接口的类生成代理，而不能针对类</li>
<li>CGLIB动态代理是针对类实现代理，主要是对指定的类生成的子类，覆盖其中的方法（继承）</li>
</ul>
<p>2.Spring AOP选择JDK还是CGLIB的依据</p>
<ul>
<li>当Bean实现接口时，Spring就会用JDK的动态代理</li>
<li>当Bean没有实现接口时，Spring使用Cglib来实现</li>
<li>默认是JDK代理，可以强制使用CGLIB(在Spring配置中加入&lt;aop:aspectj-autoproxy proxy-target-class=&quot;true&quot; /&gt;)</li>
</ul>
<p>3.JDK动态代理和CGLIB性能对比</p>
<p>使用CGLIB实现动态代理，CGLIB底层采用ASM字节码生成框架，使用字节码技术生成代理类，在JDK1.6之前使用Java反射效率要高，<br>
唯一需要注意的是，CGLIB不能对声明final方法进行代理，因为CGLIB原理是动态生成代理类的子类。</p>
<h3 id="aspect-join-points-pointcuts和advice语法">Aspect、Join Points、Pointcuts和Advice语法</h3>
<h4 id="spring-aop核心概念综述">Spring AOP核心概念综述</h4>
<p>Aspect、JoinPoints、Pointcuts、以及Advice这些都属于AOP的核心概念，SpringAOP是通过这些概念的组合完成代码植入工作的。</p>
<p>如图一所示，这张图将SpringAOP的概念进行了总体描述，先从右的箭头说起，这个箭头包含了一些小方块，我们称之为JoinPoints，它是可以用来进行增强的方法点，<br>
例如上节课中Service类中的help方法，说白了就是对什么方法进行增强。</p>
<p>看完了JoinPoints之后，再往上看，有一个向下的箭头，上面标注着Pointcut,我们称之为切入点。切入点是告诉我们在方法的什么位置进行增强，<br>
比如在方法执行之前增强，还是执行之后增强，或者两者皆有。Advice是最上面的方块，它表示需要增强的功能，上节课的例子中在“买书之前”或者“买书之后”的打印语句。</p>
<p>这里的“买书之前”和“买书之后”就是增强功能，再例如我们如果编写日志也可以放在也放在增强方法里面完成。最后Advice和Pointcut的组合就是切面Aspect。</p>
<figure data-type="image" tabindex="9"><img src="../framework/images/spring-aop.png" alt="SpringAOP" loading="lazy"></figure>
<h3 id="_源码分析spring-aop-jdk动态代理实现原理">_源码分析：Spring AOP JDK动态代理实现原理</h3>
<h4 id="invoke实现aop中具体的逻辑">invoke：实现AOP中具体的逻辑</h4>
<p>从前面的课程我们知道，JDK中AOP的实现是基于java.lang.reflect包中的Proxy和InvocationHandler两个接口来实现的。<br>
对于InvocationHandler的创建，我们需要重写三个方法：</p>
<ul>
<li>构造函数：将目标代理对象传入。</li>
<li>invoke：实现AOP中具体的逻辑。</li>
<li>getProxy：获取产生的代理类。</li>
</ul>
<p>而在Spring框架中，JDK方式也实现了上诉过程，在源码分析上面我们也将其分为invoke和getProxy两大部分给大家介绍。</p>
<p>JDK动态代理中是通过proxyFactory.getProxy()获取代理的，如下面代码所示，通过createAopProxy()生成对应的proxyFactory<br>
然后在调用其中的getProxy方法。</p>
<pre><code class="language-java">
public Object getProxy(ClassLoader classLoader){
    return createAopProxy().getProxy(classLoader);
}
</code></pre>
<p>createAopProxy()中决定的实现类为JdkDynamicAopProxy。如下面的代码所示，它实现了AopProxy和InvocationHandler。<br>
由于我们会实现InvocationHandler，并且Override其中的invoke方法，然后通过InvocationHandler的getProxy方法<br>
获取代理类，通过对代理类调用process方法执行AOP的相关操作。</p>
<p>那么首先来看看Override的invoke方法的源码，如图所示，在invoke方法中我们需要关注红框标注的部分</p>
<ul>
<li>在获取目标类以后，通过目标类获取要执行方法的拦截器链，这里的变量为chain。</li>
<li>如果chain为空说明没有在方法上拦截，那么直接调用切点方法就行了。</li>
<li>如果chain不为空说明方法上有拦截，于是将拦截器封装在ReflectiveMethodInvocation，并执行，这部分也是AOP需要关注点。</li>
</ul>
<h3 id="精选面试题spring-aop和aspectj-aop存在哪些区别">精选面试题：Spring AOP和AspectJ AOP存在哪些区别？</h3>
<h4 id="aop的基本概念-2">AOP的基本概念</h4>
<p>AOP，它是面向对象编程的一种补充，主要应用于处理一些具有横切性质的系统级服务，如日志收集，事务管理安全检查缓存，对象池管理等等。</p>
<p>AOP实现的关键在于AOP框架自动创建AOP代理，AOP代理则可分为静态代理和东岱代理两大类，其中静态代理是指用AOP框架提供的命令指令进行编译，<br>
从而在编译阶段就可生成AOP代理类，因此也称为编译时增强；而动态代理则在运行时借助JDK动态代理，CGLIB等再内存中临时生成AOP动态代理类，因此也被称为运行时增强。</p>
<p>面向切面编程是一种编程范式，旨在通过允许横切关注点的分离，提高模块化。AOP提供切面来讲跨域对象关注点模块化。</p>
<p>AOP要实现的是在我们写代码的基础上进行一定的包装，如在方法执行前或者方法执行后或者在执行出现异常后这些地方进行拦截处理或叫做增强处理。</p>
<p>AOP的几个要素：</p>
<ul>
<li>Pointcut：是一个一个基于正则表达式，有点绕，就是说他本身是一个表达式，但是它是基于正则表达式语法的。通常一个Pointcut，会选取程序中的某些我们感兴趣的执行点，或者谁程序执行点的集合。</li>
<li>JoinPoints：通过Pointcut选取出来的集合中的具体的一个执行点，我们就叫JoinPoints。</li>
<li>Advice：在选取出来的JoinPoints上要执行的操作、逻辑</li>
<li>Aspect：就是我们关注的模块化，这个关注点可能会横切多个对象和模块，事务管理是横切关注点的很好的例子。它是一个抽象的概念。从软件的角度来说是指应用程序在不同模块的某一个领域或方面。由Pointcut和Advice组成。</li>
<li>Weaving：把切面应用到目标对象来创建新的Advised对象的过程。</li>
</ul>
<h4 id="springaop和aspectj的区别">SpringAOP和AspectJ的区别</h4>
<p>相信作为Java开发者我们都熟悉Spring这个框架，在spring框架中有一个主要的功能就是AOP，提到AOP就往往会想到AspectJ，下面对AOP和AspectJ做一个简单的比较：</p>
<p>(1)SpringAOP</p>
<ul>
<li>基于动态代理来实现，默认如果使用了接口，用JDK提供的动态代理实现，如果是方法则使用CGLIB实现</li>
<li>SpringAOP需要依赖IOC容器来管理，并且只能作用于Spring容器，使用纯Java代码实现</li>
<li>在性能上，由于SpringAOP是基于动态来实现的，在容器启动时需要生成代理实例，在方法调用上也会增加栈的深度，使得SpringAOP的性能没有AspectJ好。</li>
</ul>
<p>(2)AspectJ<br>
AspectJ来自Eclipse基金会，其实现机制属于静态织入，通过修改代码来实现，有如下几个织入的时机：</p>
<ul>
<li>编译期织入：如类A使用了AspectJ添加了一个属性，类B引用了它，这个场景就需要编译期的时候就进行织入，否则就没办法编译类B。</li>
<li>编译后织入：也就是已经生成了.class文件或者已经打包成jar包了，这个时候我们需要增强处理的话，就要用编译后织入。</li>
<li>类加载后织入：指的是在加载类的时候进行织入，要实现这个时期的织入，有几种方法：<br>
1.自定义类加载器干这个，这个应该是最容易想到的办法，在被织入类加载到JVM之前对它进行加载，这样就可以在加载的时候定义行为了。<br>
2.在JVM启动的时候指定AspectJ提供的agent-javaagent:.....jar。</li>
</ul>
<p>AspectJ可以做SpringAOP做不了的事情，它是AOP编程的完全解决方案，SpringAOP则致力于解决企业级开发中最普遍的AOP(方法织入)，而不是成为像AspectJ一样的AOP方案。<br>
因为AspectJ在运行之前就完成了织入，所以说它生成的类没有额外运行时开销的</p>
<h4 id="springaop和aspectj对照表">SpringAOP和AspectJ对照表</h4>
<table>
<thead>
<tr>
<th>SpringAop</th>
<th>AspectJ</th>
</tr>
</thead>
<tbody>
<tr>
<td>在纯Java中实现</td>
<td>使用Java编程语言的扩展实现</td>
</tr>
<tr>
<td>不需要单独的编译过程</td>
<td>除非设置LTW，否则需要AspectJ编译器（ajc）</td>
</tr>
<tr>
<td>只能使用运行时织入</td>
<td>运行时织入不可用，支持编译时、编译后和加载时织入</td>
</tr>
<tr>
<td>功能不强，进支持方法级织入</td>
<td>更强大，可以编织字段方法构造函数，静态初始值，设定值，最终类方法等</td>
</tr>
<tr>
<td>只能再由Spring容器管理的bean上实现</td>
<td>可以在所有域对象上实现</td>
</tr>
<tr>
<td>仅支持方法执行切入点</td>
<td>支持所有切入点</td>
</tr>
<tr>
<td>代理是由目标对象创建的，切面切面应用在这些代理上</td>
<td>在执行应用程序之前，各方面直接在代码中织入</td>
</tr>
<tr>
<td>比AspectJ慢多了</td>
<td>更好的性能</td>
</tr>
<tr>
<td>易于学习容易</td>
<td>比SpringAOP来说更复杂</td>
</tr>
</tbody>
</table>
<h3 id="引入logback框架进行日志打印">引入logback框架进行日志打印</h3>
<h4 id="logback介绍">logback介绍</h4>
<p>logback是一个Java领域的日志框架。它被认为是Log4J的继承人，logback和log4J属于同一个作者，看上去logback是log4J的升级版。<br>
logback分为三个模块，logback-core，logback-classic和logback-access，其中logback-core是核心其他两个模块依赖core，这个<br>
logback-classic是log4J的改善版本，并且原生实现了SELF4J门面。模块logback-access可以集成于Servlet容器。比如tomcat和Jetty。<br>
你可以基于logback-core自己创建其他模块。</p>
<p>如图一所示，在项目的包引用中，需要三个jar包分别是self4j-api，logback-core，logback-classic。</p>
<h4 id="logback的配置介绍">logback的配置介绍</h4>
<p>由于logback针对整个系统的日志进行定义，因此需要有一个文件这些定义进行配置，这就是logback的配置文件，这里我们以logback.xml文件为例给大家介绍。</p>
<p>(1)根节点<configuration><br>
<configuration>元素包含下面三个属性：<br>
scan：此属性设置为true时，配置文件发生改变，将会被重新加载<br>
scanPeriod：配置文件修改间隔，当scan为true时此属性生效， “60 seconds”表示配置文件每个60s检查一次是否被修改。<br>
debug:当此属性设置为true时，将打印出logback内部日志信息。实时查看logback运行状态。默认值为false。<br>
(2)子节点<property><br>
该节点是configuration的子节点，用来定义变量值，它有两个属性name和value通过<property>定义的值会被插入到logger上下文中，可以<br>
使用“${}”来使用变量。</p>
<p>name：变量的名称<br>
value：变量定义的值</p>
<p>(3)子节点<appender><br>
同样appender节点也是configuration的子节点，logback将写入日志事件的任务委托给一个名为appender的组件。<br>
它有两个属性name和class。name指定appender名称，class指定类的全限定名用于实例化。<br>
<appender>元素可以保函零个挥着一个<layout>元素，零个或者多个<encoder>元素，零个或者多个<filter><br>
除了这些公共元素之外，<appender>元素可以包含任意与appender类的JavaBean属性相一致的元素，常见结构如下</p>
<pre><code class="language-java">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;configuration debug=&quot;false&quot;&gt;
&lt;!--定义日志文件的存储地址 勿在 LogBack 的配置中使用相对路径--&gt;
&lt;property name=&quot;LOG_HOME&quot; value=&quot;/data/logs&quot;/&gt;
&lt;!-- 控制台输出 --&gt;
&lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt;
&lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt;
&lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符--&gt;
&lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg %n&lt;/pattern&gt;
&lt;/encoder&gt;
&lt;/appender&gt;

&lt;!-- 按照每天生成日志文件 --&gt;
&lt;appender name=&quot;FILE&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;
&lt;file&gt;${LOG_HOME}/mrb-store.log&lt;/file&gt;
&lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;
&lt;fileNamePattern&gt;${LOG_HOME}/mrb-store.log.%d{yyyy-MM-dd}.log&lt;/fileNamePattern&gt;
&lt;maxHistory&gt;15&lt;/maxHistory&gt;
&lt;/rollingPolicy&gt;
&lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt;
&lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符--&gt;
&lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n&lt;/pattern&gt;
&lt;/encoder&gt;
&lt;/appender&gt;

&lt;!-- 应用的日志(错误级别)文件 --&gt;
&lt;appender name=&quot;ERROR&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;
&lt;file&gt;${LOG_HOME}/mrb-store-error.log&lt;/file&gt;
&lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;
&lt;fileNamePattern&gt;${LOG_HOME}/mrb-store-error-%d{yyyy-MM-dd}.log&lt;/fileNamePattern&gt;
&lt;maxHistory&gt;15&lt;/maxHistory&gt;
&lt;/rollingPolicy&gt;
&lt;encoder&gt;
&lt;pattern&gt;%date %level [%thread] %logger{40}[%file-%M %line] %msg%n
&lt;/pattern&gt;
&lt;/encoder&gt;

&lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt;
&lt;level&gt;ERROR&lt;/level&gt;
&lt;onMatch&gt;ACCEPT&lt;/onMatch&gt;&lt;!-- 只接收错误级别的日志 --&gt;
&lt;onMismatch&gt;DENY&lt;/onMismatch&gt;
&lt;/filter&gt;
&lt;/appender&gt;

&lt;!-- 数据库相关操作日志 --&gt;
&lt;appender name=&quot;MYSQL_LOG&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;
&lt;file&gt;${LOG_HOME}/mrb-store-sql.log&lt;/file&gt;
&lt;encoder&gt;
&lt;pattern&gt;%date %level [%thread] %logger{40}[%file-%M %line] %msg%n&lt;/pattern&gt;
&lt;/encoder&gt;
&lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;
&lt;fileNamePattern&gt;${LOG_HOME}/mrb-store-sql-%d{yyyy-MM-dd}.log&lt;/fileNamePattern&gt;
&lt;maxHistory&gt;15&lt;/maxHistory&gt;
&lt;/rollingPolicy&gt;
&lt;/appender&gt;

&lt;logger name=&quot;com.mrb.store.mapper&quot; level=&quot;debug&quot; additivity=&quot;false&quot;&gt;
&lt;appender-ref ref=&quot;MYSQL_LOG&quot;/&gt;
&lt;/logger&gt;

&lt;!-- 屏蔽swagger打印的debug info日志 --&gt;
&lt;logger name=&quot;springfox.documentation.spring.web&quot; level=&quot;WARN&quot;/&gt;

&lt;springProfile name=&quot;dev&quot;&gt;
&lt;root level=&quot;INFO&quot;&gt;
&lt;appender-ref ref=&quot;STDOUT&quot;/&gt;
&lt;appender-ref ref=&quot;FILE&quot;/&gt;
&lt;appender-ref ref=&quot;ERROR&quot;/&gt;
&lt;/root&gt;
&lt;/springProfile&gt;

&lt;springProfile name=&quot;test,audit,prod&quot;&gt;
&lt;root level=&quot;INFO&quot;&gt;
&lt;appender-ref ref=&quot;FILE&quot;/&gt;
&lt;appender-ref ref=&quot;ERROR&quot;/&gt;
&lt;/root&gt;
&lt;/springProfile&gt;

&lt;/configuration&gt;
</code></pre>
<p>(4)RollingFileAppender(轮转日志)<br>
RollingFileAppender是一类appender其功能比较强大，因此被广泛应用。它将日志输出到log.txt，在满足了特定的条件之后，将日志输出到另一个文件。</p>
<p>与RollingFileAppender进行交互的有两个重要的子组件，第一个是RollingPolicy,它负责日志轮转的记录策略。另一个是TriggeringPolicy.<br>
它负责日志轮转的记录时机，如图4所示。</p>
<p><file>:被写入的文件名，可以是相对目录，也可以是绝对目录，如果上级不存在会自动创建，没有默认值。<br>
<append>:如果是true，日志被追加到文件结尾，如果是false，情况现存文件，默认是true。<br>
<encoder>:记录事件进行格式化。包含两个功能，一是把日志信息转换成字节数组，二是把字节数组写入到输入流。<br>
PatternLayoutEncoder是唯一有用且默认的encoder，有一个<pattern>节点，用来设置日志的输入格式。使用“%”加转换符的方式<br>
，如果要输出%，则必须对“\”对“%”进行定义。<br>
<prudent>:当为true时，不支持FilxedWindowRollingPolicy。支持TimeBasedRollingPolicy，但是有两个限制，1.不支持文件压缩，2.不能设置为file属性，必须留空。<br>
<rollingPolicy>：当发生滚动时，决定RollingFileAppender的行为涉及文件的移动和重命名，属性class定义具体的滚动策略类。<br>
(5).日志过滤器<br>
Filter作为appender的子节点，如图所示，<br>
<filter>将日志信息过滤，常用levelFilter-等级过滤器来过滤日志信息，有三个元素<br>
<level>INFO</level> :指定特定等级<br>
<onMatch>ACCEPT</onMatch>：如果匹配则接受输出<br>
<onMismatch>DENY&lt;/onMismatch:如果不匹配则过滤掉</p>
<p>(6)logger子节点<br>
其用来设置某一个包或者具体的某一个类的日志打印级别，以及指定<appender>.<br>
<logger>仅有一个name属性，一个可选的level和一个可选的addtivity属性。<br>
可以包括零个或者多个<appender-ref>元素，标识这个appender将会添加到这个logger<br>
name:用来指定受此logger约束的某一个包或者具体的类，<br>
level:用来打印日志级别，大小写无关，TRACK，DEBUG，INFO，WARN，ERROR，ALL，OFF还有一个特殊的值InHERITEN或者同义词NULL。<br>
addtivity：是否向上级logger传递打印信息。默认是true。</p>
<p>logger节点可以包含零个或者多个<appender-ref>元素，标识这个appender将会添加到这个logger。</p>
<p>(7)Root节点<br>
它也是一个logger节点元素，不过它是一个根logger，也就是所有logger的上级节点，只有一个level属性，因为name已经被命名为“root”，且已经是最上级了。</p>
<p>level：用来设置打印级别，大小写无关：TRACE，DEBUG，INFO，WARN，ERROR，ALL，OFF。不能设置为INHERITED或者同义词NULL，默认是DEBUG。</p>
<figure data-type="image" tabindex="10"><img src="../framework/images/logback.png" alt="SpringAOP" loading="lazy"></figure>
<h3 id="aspect注解是怎么运作起来的">@Aspect注解是怎么运作起来的</h3>
<h4 id="aspect执行过程">@Aspect执行过程</h4>
<p>@Aspect切面类注解属于Spring2.0以后定义的标签注解，在配置文件ApplicationContext.xml中以<br>
aop:aspect-autoproxy的方式开启。其原理是通过代码追踪，在AopNamespaceHandler中找到了对这个标签的解析器<br>
AspectJAutoProxyBeanDefinitionParser类。<br>
先看看AspectJAutoProxyBeanDefinitionParser中的parse方法如何对@AspectJ标签进行解析工作的，如下图所示，<br>
parse方法里面对调用registerAspectJAnnotationAutoProxyCreatorIfNecessary</p>
<p>方法对解析上下文以及搭上注释的元素进行注册。顺势看注册方法紫红调用AopConfigUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary<br>
方法创建BeanDefinition，并且把BeanDefinition注册组件里。</p>
<pre><code class="language-java">public  BeanDefinition parse(Element element ,ParserContext parserContext){
        
    AopNamespace.registerAspectJAnnotationAutoProxyCreatorIfNecessary(parserContext,element)
        
}

public static registerAspectJAnnotationAutoProxyCreatorIfNecessary(ParserContext parserContext , Element element){
        BeanDefinition BeanDefinition = AopConfigUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary(parserContext,element);
        useClassProxyIfNecessary(parserContext.getRegistry() , sourceElemet) ;
        registerComponentIfNecessary(beanDefinition ,parserContext);
}



</code></pre>
<p>从前面的课程知道BeanDefinition是Spring容器对Bean进行存储和管理的模式，接着看registerAspectJAnnotationAutoProxyCreatorIfNecessary<br>
是符合生成BeanDefinition的。如图所示，registerAspectJAnnotationAutoProxyCreatorIfNecessary方法直接调用getResource<br>
registerOrEscalateApcAsRequired方法产生BeanDefinition。其主要操作就是建立RootBeanDefinition的根，<br>
然后设置其源，角色以及对BeanDefinition的注册。</p>
<pre><code class="language-java">
public static BeanDefinition registerAspectJAnnotationAutoProxyCreatorIfNecessary(BeanDefinitionRegistry registry ,Object source){
    return registerOrEscalateApcAsRequired(AnnotationAwareAspectJAutoProxyCreator.class ,registry.source);
}

private static registerOrEscalateApcAsRequired(Class&lt;?&gt; cls ,BeanDefinitionRegistry registry,Object source){
    Assert.nonNull(register ,&quot;BeanDefinitionRegistry must not be null&quot;);
    if(registry.containsBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME)){
       BeanDefinition apcDefinition = registry.getBeanDefinition(AUTO_PROXY_CREATOR_BEAN);
       if(cls.getName().equals(apcDefinition.getBeanClassName())){
         int currentPriority =  findPriorityForClass(apsDefinition.getBeanClassName())
         int requiredPriority = findPriorityForClass(cls)
        if(currentPriority &lt; requiredPriority){
            apcDefinition.setBeanClassName(cls.getName())
         }
        }
       return null;
    }
    //封装一个代理
    RootBeanDefinition beanDefinition = new RootBeanDefinition(cls);
    beanDefinition.setSource(source);
    beanDefinition.getPropertyValues().add(&quot;order&quot;,ORDER.HIGHEST_PRECEDENCE);
    beanDefinition.serRole(BeanDefinition.ROLE_INFRASTEUCTURE);
    registry.register(AUTO_PROXY_CREATOR_BEAN_NAME ,beanDefinition);
    return beanDefinition;
}

</code></pre>
<p>经过以上处理AnnotationAwareAspectJAutoProxyCreator类就被创建处理了，而这个类的功能就是完成AOP增强的，它集成了父类AbstractAutoProxyCreator，同时其父类实现了BeanPostProcessor接口，它在实现改接口后，<br>
Spring加载Bean时会在其实例话前调用其postProcessBeforeInstantiation方法，接下来看这个方法，如图三所示，postProcessBeforeInstantiation方法中，先添加目标Bean，然后获取有@Aspect注解类中的增强方法，<br>
最后拦截器specificInterceptors对源目标类创建代理增强。</p>
<pre><code class="language-java">public Object postProcessBeforeInstantiation(Class&lt;?&gt; beanClass ,String beanName) throws Exception {
   Object cacheKey = getCacheKey(beanClass , beanName);
   if(beanName == null || !this.targetSourceBeans.constains(beanName)){
       if(this.adviseBeans.constains(cacheKey)){
           return null;
        }
       if(isInfrastructrue(beanClass) || shouldSkip(beanClass,beanName)){
           this.adviseBeans.put(cacheKey, Boolean.FALSE);
        }
    }
   if(beanName != null){
      TargetSource targetSource =   getCustomTargetSource(beanClass, beanName);
      if(targetSource != null){
          //获取存在与aspect注解类中的增强方法；
        Object[] specificInterceptords = getadvicesAndAdvisorsForBean(beanClass, beanName, targetSource);
        // 根据增强器创建代理
        Object proxy =  createProxy(beanClass ,beanName ,specificInterceptors,targetSource);
        this.proxy.put(createKey,proxy.getCLass());
        return proxy;
        }
    }
        return null;
}


</code></pre>
<p>按照上面的代码描述是由getadvicesAndAdvisorsForBean来生成拦截器的，它包括如下工作：<br>
获取beanFactory中所有的@Aspect注解的类</p>
<p>对标记类进行增强提取，提取之前需求对切入信息（before，after，around等）进行增强，其中<br>
before增强被为AspectJmethodBeforeAdvice<br>
afrer增强为AspectJAfterAdvice<br>
around增强为AspectJAfterThrowingAdvice增强器<br>
增强器封装后，被代理接口调用时，会触发增强器中的方法。</p>
<p>接下来看看postProcessBeforeInstantiation中出现的createProxy方法，它是用来创建拦截器代理的。如图所示，<br>
在createProxy方法中会被接受拦截器specificInterceptors和目标源。方法体中创建代理工厂，接下来就是通过buildAdvicetors方法生成<br>
advisors也就是增强的方法，对其进行遍历，并且加入到代理工厂中。然后要设置要代理的类，通过proxyFactory中getProxy方法获取代理类信息</p>
<pre><code class="language-java">
public Object createProxy(Class&lt;?&gt; beanClass ,String beanName ,Object[] specificInterceptors ,TargetSource targetSource){
        //创建道理工厂
        ProxyFactory proxyFactory=new ProxyFactory();
        proxyFactory.copyFrom(this);
        //决定对应给定的bean是否应该使用targetClass而不是他的接口代理，主要看xml中的配置：
        //检查proxyTargetClass设置以及preserverTargetClass属性
        if(!proxyFactory.isProxyTargetClass()){
        if(!shouldProxyTargetClass(beanClass,beanName)){
        //类CGLIB代理
        proxyFactory.setProxyTargetClass(true);
        }else{
        //添加代理接口，动态代理
        evaluateProxyInterfaces(beanClass,proxyFactory);
        }
        }
        Adisor[]adisors=buildAdvisors(beanName,specificInterceptors);
        for(Advisor advisor:adisors){
        //加入增强器，AspectJmethodBeforeAdvice等
        proxyFactory.addAdvisor(advisor);

        }
        //设置要代理的类
        proxyFactory.setProxyTargetClass(targetSource);
        customeieProxyFactory(proxyFactory);
        proxyFactory.setFrozen(this.freezeProxy);
        if(advisorsPreFiltered){
        proxyFactory.setPreFiltered(true);
        }
        return proxyFactory.getProxy(getProxyClassLoader())


        }

</code></pre>
<p>在createProxy方法最后会调用getProxy方法，createAopProxy().getProxy(classLoader);在被调用的getProxy方法实现JDK动态代理，这个在上周JDK代理的源码中提到过，该方法对代理类进行实例化并且返回。</p>
<pre><code class="language-java">public Object getProxy(ClassLoader classLoader){
    return createProxy().getProxy(classLoader);
        }
        
public getProxy(ClassLoader classLoader){
    if(logger.isDebugEnabled()){
        logger.debug(&quot;&quot;)
        }
   CLass&lt;?&gt;[] proxiedInterfaces =  AopProxyUtils.comoleteProxiedInterfaces(this.advised);
    findDefinitionEqualsAndHashCodeMethods(proxiedInterfaces);
    return Proxy
}

</code></pre>
<p>当返回代理类之后会执行该代理类当invoke方法，最后来看看invoke方法的源码，这个方法在上周JDK动态代理课中讲过，主要是对拦截连进行循环增强，也就是将拦截器一个个串行执行，从而达到对目标方法的增强。</p>
<pre><code class="language-java">public Object invoke(Object proxy ,Method method ,Object[] args) throws Exception{
    MethodInvocation invocation =;
    
    Object oldProxy = null;
    
    boolean setProxyContext = false;
    TargetSource targetSource =  this.advised.targetSource;
    Class&lt;?&gt; targetClass = null;
    try {
        if(!this.equalsDefined &amp;&amp; AopUtils.isEquealMethod(method)){
            ---
        }
        if(!this.hashCodeDefined &amp;&amp; AOpUtils.isEquealMethod(method)){
            ---
        }
        if(this.advised.opaque &amp;&amp; method.getDeclaryingClass().isInterface() &amp;&amp;){
            
        }
        Object retVal ;
        target =  targetSource.getTarget();
        //获取方法拦截器链，其实就是把增强器封装成拦截器的形式串行调用的；
        List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method ,targetClass);
        }
        if(chain.isEmpty()){
            retVal = AOpUtils.invokeJoinPointUseingRefletion(target ,method,args);
        }else{
        invocation =  new ReflectiveMethodInvocation(proxy ,target ,method,args,targetClass,chain);
        retVal = invocation.processd();
        }
        CLass&lt;?&gt; retureType = method.getReturnType();
        if(retVal != null &amp;&amp; retVal == target &amp;&amp; returnType.isInstance(proxy) &amp;&amp; !RawTargetAccess.class.isAssignableFrom(method.getDeclaryingClass())){
            retVal = proxy ;
        }
        else if(retval == null &amp;&amp; returnType != Void.Type &amp;&amp; returnType.isPrimitive() &amp;&amp; returnType.){
            throws new AopInvocationException(&quot;&quot;);
        }
        
        return retVal;
}



</code></pre>
<h3 id="_代码实战基于spring-aop机制实现全局异常处理">_代码实战：基于Spring AOP机制，实现全局异常处理</h3>
<h4 id="controlleradvice与exceptionhandler注解">ControllerAdvice与ExceptionHandler注解</h4>
<p>如果说需要对系统内部异常采取统一的处理，其思路还是通过Spring AOP 去截获异常信息，进行统一处理然后再返回给客户端。在使用Spring AOP编写异常处理类之前先介绍几个注释。</p>
<p>ControllerAdvice，顾名思义是一个增强的 Controller，增强的概念在SpringAOP中就是我们需要织入方法的具体内容，回到本节内容就是需要进行异常处理的地方。</p>
<p>使用ControllerAdvice可以实现三个方面的功能：全局异常处理、全局数据绑定和全局数据预处理。在我们的项目中，主要关注全局异常处理的功能。</p>
<p>如图1所示，在建立的MyGlobalExceptionHandler类上标记了ControllerAdvice注解，表示对Controller的请求进行增强<br>
。<br>
在类中的customException方法上面通过一个ExceptionHandler注解，参数使用Exception.class表示对Exception类型的异常进行处理。</p>
<p>这里可以根据需要定义其他异常，甚至是自定义的异常类。然后通过new ModelAndView初始化出ModelAndView，并且对其进行修改通过addObject方法和setViewName加入一些错误信息，从而通过return 语句将这个ModelAndView返回给Controller的调用者</p>
<pre><code class="language-java">
/**
 * 全局异常处理器
 */
@Slf4j
@RestControllerAdvice
public class ExceptionHandlerAdvice {

    /**
     * 基础异常
     */
    @ExceptionHandler(BaseException.class)
    public Response&lt;Void&gt; baseException(BaseException e) {
        return Response.fail(e.getMessage());
    }

    @ExceptionHandler(NoHandlerFoundException.class)
    public Response&lt;Void&gt; handlerNoFoundException(Exception e) {
        log.error(e.getMessage());
        return Response.fail(Code.System.NOT_FOUND);
    }

    @ExceptionHandler(AccessDeniedException.class)
    public Response&lt;Void&gt; handleAuthorizationException(AccessDeniedException e) {
        log.error(e.getMessage());
        return Response.fail(Code.System.FORBIDDEN);
    }

    @ExceptionHandler(AccountExpiredException.class)
    public Response&lt;Void&gt; handleAccountExpiredException(AccountExpiredException e) {
        log.error(e.getMessage(), e);
        return Response.fail(e.getMessage());
    }

    @ExceptionHandler(UsernameNotFoundException.class)
    public Response&lt;Void&gt; handleUsernameNotFoundException(UsernameNotFoundException e) {
        log.error(e.getMessage(), e);
        return Response.fail(e.getMessage());
    }

    @ExceptionHandler(Exception.class)
    public Response&lt;Void&gt; handleException(Exception e) {
        log.error(e.getMessage(), e);
        return Response.fail(Code.System.ERROR_SERVER);
    }

    /**
     * 自定义验证异常
     */
    @ExceptionHandler(BindException.class)
    public Response&lt;Void&gt; validatedBindException(BindException e) {
        log.error(e.getMessage(), e);
        String message = e.getAllErrors().get(0).getDefaultMessage();
        return Response.fail(message);
    }

    @ExceptionHandler(MrbException.class)
    public Response&lt;Void&gt; handleAllException(MrbException e, HttpServletRequest req) {
        log.error(&quot;tips:{} request method:{} uri:{}&quot;, e.getMessage(), req.getMethod(), req.getRequestURI());
        if (e.getErrorCode() != null &amp;&amp; e.getErrorCode() != Code.System.FAIL) {
            return Response.fail(e.getErrorCode(), null, e.getErrorCode().getLabel());
        }
        return Response.fail(Code.System.FAIL, null, e.getMessage());
    }

    @ExceptionHandler(HystrixBadRequestException.class)
    public Response&lt;Void&gt; handleAllException(HystrixBadRequestException e, HttpServletRequest req) {
        log.error(&quot;tips:{} request method:{} uri:{}&quot;, e.getMessage(), req.getMethod(), req.getRequestURI());
        MrbException exception = (MrbException) e.getCause();
        if (exception.getErrorCode() != null &amp;&amp; exception.getErrorCode() != Code.System.FAIL) {
            return Response.fail(exception.getErrorCode(), null, exception.getErrorCode().getLabel());
        }
        return Response.fail(Code.System.FAIL, null, e.getMessage());
    }

    @ExceptionHandler(HttpRequestMethodNotSupportedException.class)
    public Response&lt;Void&gt; handleFeignException(HttpRequestMethodNotSupportedException e, HttpServletRequest req) {
        log.error(&quot;request method:{} uri:{}&quot;, req.getMethod(), req.getRequestURI(), e);
        return Response.fail(Code.System.ERROR_METHOD, null, e.getMessage());
    }

    @ExceptionHandler(value = {MismatchedInputException.class, HttpMessageNotReadableException.class})
    public Response&lt;Void&gt; handleFeignException(Exception e, HttpServletRequest req) {
        log.error(&quot;request method:{} uri:{}&quot;, req.getMethod(), req.getRequestURI(), e);
        return Response.fail(Code.System.ERROR_PARAMS, null, Code.System.ERROR_PARAMS.getLabel());
    }

    @ExceptionHandler(value = {RetryableException.class})
    public Response&lt;Void&gt; handleRetryableException(RetryableException e, HttpServletRequest req) {
        log.error(&quot;request method:{} uri:{}&quot;, req.getMethod(), req.getRequestURI(), e);
        return Response.fail(Code.System.ERROR_PARAMS, null, &quot;请求超时&quot;);
    }

    /**
     * 参数校验异常处理
     *
     * @param e
     * @param req
     * @return Response&lt;java.util.Map &lt; java.lang.String, java.lang.String&gt;&gt;
     * @author zhangdecheng
     * @date 2020/9/18 17:40
     */
    @ExceptionHandler(value = MethodArgumentNotValidException.class)
    public Response&lt;Map&lt;String, String&gt;&gt; methodArgumentNotValidException(MethodArgumentNotValidException e, HttpServletRequest req) {
        String errMsg = e.getBindingResult().getFieldErrors()
                .stream()
                .map(FieldError::getDefaultMessage)
                .collect(Collectors.joining(&quot;, &quot;));

        log.error(&quot;request method:{} uri:{}&quot;, req.getMethod(), req.getRequestURI(), e);
        return Response.fail(Code.System.ERROR_PARAMS, null, errMsg);
    }

}


</code></pre>
<p>可以从图2 中下面一个方法handleBusinessException唯一的不同是在ExceptionHandler注解的参数上。这里使用的参数类型是BusinessException.class，是我们自己定义的业务异常类。</p>
<p>如图3所示，spring/exception目录下面建立BusinessException类，其继承与RuntimeException类。</p>
<p>这个BusinessException可以在全局异常处理类中被处理，如果在业务代码中也接抛出这个异常，也是可以被GlobalExceptionHandler类中的handleBusinessException方法捕获到并处理的。</p>
<pre><code class="language-java">

/**
 * @desc 公共自定义异常
 */
@Slf4j
public class MrbException extends RuntimeException {

    private static final long serialVersionUID = -3361373660274001948L;

    private StatusCode errorCode;

    public MrbException() {
        super();
    }

    public MrbException(String errorMsg) {
        super(errorMsg);
        this.errorCode = Code.System.FAIL;
    }

    public MrbException(StatusCode errorCode) {
        super(errorCode.getLabel());
        this.errorCode = errorCode;
    }

    public MrbException(StatusCode errorCode, String msg) {
        super(msg);
        this.errorCode = errorCode;
    }

    public static void throwException(StatusCode errorCode) {
        throw new MrbException(errorCode);
    }

    public static void throwException(String errorMsg) {
        throw new MrbException(errorMsg);
    }

    public static void throwException(boolean expression, String errorMsg) {
        if (expression) {
            throw new MrbException(errorMsg);
        }
    }


    public static MrbException convert(Integer code, String msg) {
        StatusCode statusCode = Code.enumValueOf(code);
        if (statusCode != null) {
            return new MrbException(statusCode, msg);
        }
        return null;
    }

    public static MrbException convert(Integer code) {
        StatusCode statusCode = Code.enumValueOf(code);
        if (statusCode != null) {
            return new MrbException(statusCode);
        }
        return null;
    }

    public StatusCode getErrorCode() {
        return errorCode;
    }

    public void setErrorCode(StatusCode errorCode) {
        this.errorCode = errorCode;
    }
}

</code></pre>
<p>4、总结<br>
本节课在介绍了利用Spring AOP 实现全局异常处理的思路，从ControllerAdvice与ExceptionHandler注解入手，<br>
我们创建了自己的全局异常处理类GlobalExceptionHandler。到Controller中抛出异常的时候GlobalExceptionHandler中有对应的方法就可以捕获到。<br>
针对不同的异常类，可以编写不同的异常处理方法。</p>
<h3 id="基于spring-aop机制拦截处理登陆信息">基于Spring AOP机制拦截处理登陆信息</h3>
<p>上节课验证了处理异常的代码，通过GlobalExceptionHandler类处理Controller中抛出的异常信息，从而解决页面报错的问题。本节课通过使用Spring AOP的方式拦截登陆信息，并且判断今天的内容：</p>
<p>添加LoginInterceptor类用来拦截登陆信息</p>
<p>配置spring-web.xml中的拦截器</p>
<h4 id="添加logininterceptor类拦截登陆信息">添加LoginInterceptor类拦截登陆信息</h4>
<p>既然是拦截登陆信息，就需要先建立一个拦截类，用来处理登陆请求之前的判断工作。如图1所示，在/spring/interceptor 目录下面建立LoginInterceptor类，这个类是专门用来拦截登陆请求的。</p>
<p>该类实现了HandlerInterceptor，并且Override了preHandle方法，也就是在处理登陆请求之前会执行该方法。下面来看下方法中执行的内容。</p>
<pre><code class="language-java">

@Slf4j
public class RequestHeaderInterceptor extends HandlerInterceptorAdapter {


    final static Response&lt;Void&gt; ERROR_PARAMS = Response.fail(Code.System.ERROR_PARAMS);

    @Override
    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) {
        String appSystem = request.getHeader(&quot;appSystem&quot;);
        if (StringUtils.isBlank(appSystem)) {
            ERROR_PARAMS.setMsg(&quot;appSystem 不能为空&quot;);
            responseJson(ERROR_PARAMS, response);
            return false;
        }
        String clientChannel = request.getHeader(&quot;clientChannel&quot;);
        if (StringUtils.isBlank(clientChannel)) {
            ERROR_PARAMS.setMsg(&quot;clientChannel 不能为空&quot;);
            responseJson(ERROR_PARAMS, response);
            return false;
        }
        return true;
    }

    @Override
    public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) {

    }

    @Override
    public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) {

    }

    @Override
    public void afterConcurrentHandlingStarted(HttpServletRequest request, HttpServletResponse response, Object handler) {

    }

    private void responseJson(Object result, HttpServletResponse res) {
        try {
            if (result != null) {
                res.setHeader(&quot;Pragma&quot;, &quot;No-cache&quot;);
                res.setHeader(&quot;Cache-Control&quot;, &quot;no-cache&quot;);
                res.setDateHeader(&quot;Expires&quot;, 0);
                res.setContentType(&quot;text/json&quot;);
                res.setCharacterEncoding(&quot;UTF-8&quot;);
                PrintWriter out = res.getWriter();
                String json = &quot;&quot;;
                if (result instanceof String) {
                    json = (String) result;
                } else {
                    json = JsonUtils.obj2json(result);
                }
                out.write(json);
                out.flush();
            }
        } catch (Exception e) {
            log.error(&quot;&quot;, e);
        }
    }

}
</code></pre>
<p>图1 的preHandle方法中传入了request参数，其类型是HttpServletRequest类型，另外还有response其类型是HttpServletResponse，最后是Object类型的 handler。</p>
<p>首先，从request中也就是请求中通过getParameter获取参数，这里的参数是phoneNumber也就是手机号。</p>
<p>然后判断手机号是否为空，再通过getHeader方法获取phoneNumber，看看在请求头中是否存在手机号。如果也不存在，那么打印日志说明请求接口不包含手机号。</p>
<p>接下来，通过response的getOutputStream获取相应的response 流。通过setHeader方法设置返回的类型content-type和application/json。另外通过指定JSON字符串中的编码格式UTF8，告知相应的编码模式。</p>
<p>最后通过，outputStream的write方法返回输出的字节流，如果在登陆的时候不存在手机号信息就会返回失败，否则可以登陆继续后面的操作。</p>
<h4 id="配置spring-webxml中的拦截器">配置spring-web.xml中的拦截器</h4>
<p>看完了登陆拦截器的代码部分，再回到spring-web.xml配置文件中，来看看配置部分。这里会在interceptors加入一个interceptors（拦截器）。如图2所示，这个拦截器包括以下几个部分：</p>
<p>Mapping：这里主要指定需要拦截的路径，包含在该配置下的路径会被拦截到，否则拦截器是不会处理的。这里设置的是“/**”，意思是全部路径都会拦截。</p>
<p>Exclude-mapping：这里配置除了那些路径是不做拦截的。也是与Mapping配置相呼应的一个配置，也就是说拦截器也有例外，在这里配置的路径不做拦截处理。这里配置了三个节点，包括“/consumer/login”、”/demo/**”、”/”。</p>
<p>Bean：这里是对拦截器进行指定，也就是说上面的规则通过哪个拦截器实现。这里我们配置了“com.ruyuan.little.project.spring.interceptor.LoginInterceptor&quot;，也就是我们刚刚编写的拦截器。</p>
<h3 id="精选面试题spring-aop的设计模式有哪些">精选面试题：Spring AOP的设计模式有哪些？</h3>
<h4 id="策略模式在aop中的使用">策略模式在AOP中的使用</h4>
<p>策略模式，将各种算法封装到具体的类中，作为一个抽象策略类的子类，使得他们可以互换。客户端可以决定使用那种哪种算法</p>
<p>策略模式角色划分为如下三个部分：<br>
Strategy策略接口或者抽象策略类，定义策略执行接口<br>
ConcreteStrategy 具体策略类<br>
Context 上下文类，支持具有策略类的实例，并且负责调用相关的算法</p>
<p>策略模式提供了对&quot;&quot;开闭原则&quot;的完美支持，用户可以在不修改原有系统的基础上选择算法（策略），并且可以灵活的增加新的算法（策略）</p>
<p>策略模式通过Context类提供了管理具体策略类（算法族）的办法。<br>
结合简单工厂模式和Annotation，策略模式可以方便的在不修改客户端代码的前提下切换算法（策略）。<br>
在SpringAop中策略模式也起到了重要的作用，还记得SpringAOP的两种动态代理的机制吗？JDK动态代理和CGLIB动态代理这两种代理就是两种AOP的代理的策略。<br>
AOP中定义了DefaultAOPProxyFactory代理类，这个类分别关联了两种代理：JdkDynamicAopProxy和CglibProxy<br>
Factory。</p>
<h4 id="策略模式在aop中的使用-2">策略模式在AOP中的使用</h4>
<p>策略模式（Strategy Pattern），将各种算法封装到具体的类中，作为一个抽象策略类的子类，使得它们可以互换。客户端可以自行决定使用哪种算法。</p>
<p>如图1所示，策略模式角色划分为如下三个部分：</p>
<ul>
<li>
<p>Strategy 策略接口或者（抽象策略类），定义策略执行接口</p>
</li>
<li>
<p>ConcreteStrategy 具体策略类</p>
</li>
<li>
<p>Context 上下文类，持有具体策略类的实例，并负责调用相关的算法</p>
</li>
</ul>
<p>策略模式提供了对“开闭原则”的完美支持，用户可以在不修改原有系统的基础上选择算法（策略），并且可以灵活地增加新的算法（策略）。</p>
<p>策略模式通过Context类提供了管理具体策略类（算法族）的办法。</p>
<p>结合简单工厂模式和Annotation，策略模式可以方便的在不修改客户端代码的前提下切换算法（策略）。</p>
<p>在Spring AOP 中策略模式也起到了重要的作用，还接的Spring AOP 的两种动态代理机制吗？JDK动态代理和CGLIB动态代理，这里两种代理就是两种AOP 代理的策略。</p>
<p>如图2琐事，AOP中定义了DefaultAopProxyFactory的代理类，这个类分别关联了两种代理：JdkDynamicAopProxy（JDK动态代理）和CglibProxyFactory（CGLIB动态代理）。</p>
<h4 id="模板模式在aop中的使用">模板模式在AOP中的使用</h4>
<p>模板模式（Template Pattern）中，一个抽象类公开定义了执行它的方法的方式/模板。<br>
它的子类可以按需要重写方法实现，但调用将以抽象类中定义的方式进行。这种类型的设计模式属于行为型模式。</p>
<p>如图3所示，模版模式包括两部分内容：</p>
<ul>
<li>
<p>抽象类（AbstractClass）：实现了模板方法，定义了算法的骨架。</p>
</li>
<li>
<p>具体类（ConcreteClass)：实现抽象类中的抽象方法，已完成完整的算法。</p>
</li>
</ul>
<p>模板方法模板通过把不变的行为搬移到超类，去除了子类中重复的代码。子类实现算法的某些细节，有助于算法的扩展。通过<br>
一个父类调用子类实现的操作，通过子类扩展增加新的原则。符合“开放-关闭原则”。</p>
<p>回到Spring AOP中的代理创建的时候用到了AbstractAutoProxyCreator，如图4所示，它位于图的上部定义了一些通用的行为，<br>
例如：postProcessBeforeInstantiation、getAdvicesAndAdvisorsForBean、CreateProxy等，这些行为是所有代理创建者都需要使用的，因此在父类中进行统一定义。</p>
<p>AbstractAdvisorAutoProxyCreator，是根据增强器自动代理的创建者，它加入针对增强器特有的方法。<br>
再往下有三个类都继承了AbstractAdvisorAutoProxyCreator，分别是DefaultAdvisorAutoProxyCreator、AspectJAwareAdvisorAutoProxyCreator和InfrastructureAdvisorAutoProxyCreator，它们又根据自身处理场景扩展了父类的模版方法。</p>
<h4 id="适配器模式在aop中的使用">适配器模式在AOP中的使用</h4>
<p>适配器模式（Adapter Pattern），将一个类的接口转换成客户希望的另外一个接口。适配器模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。</p>
<p>如图5所示，适配器模式包括如下部分：</p>
<ul>
<li>
<p>目标接口， Target</p>
</li>
<li>
<p>具体目标实现，如ConcreteTarget</p>
</li>
<li>
<p>适配器，Adapter</p>
</li>
<li>
<p>待适配类，Adaptee</p>
</li>
</ul>
<p>回到Spring AOP 如图6琐事，其中Advisor使用了AdvisorAdapter适配器，<br>
该适配器关联另外两个适配器：ThrowsAdviceAdapter和AfterReturningAdviceAdapter，这两个适配会分别依赖ThrowsAdviceInterceptor和AfterRuturningAdviceInterceptor。<br>
也就是说Advisor增强器通过适配器模式使用了不同接口的Interceptor使用的拦截器中的功能，通过适配器模式做到了代码解耦和复用。</p>
<h3 id="先来看看原生jdbc如何进行数据库操作">先来看看原生JDBC如何进行数据库操作</h3>
<p>介绍了日志和异常处理的功能，它们都属系统级别的功能。数据库访问作为另外一个系统级别的功能会在后面的开发过程中用到，因此这周会给大家介绍这方面的内容。整体脉络会根据JDBC、连接池、JDBCTemplate以及MyBatis几部分内容展开，本节课主要介绍JDBC访问数据库的功能，今天的内容：</p>
<ul>
<li>
<p>JDBC 简介</p>
</li>
<li>
<p>上传服务</p>
</li>
<li>
<p>启动服务</p>
</li>
</ul>
<h4 id="jdbc简介">JDBC简介</h4>
<p>JDBC是Java Database Connectivity的简称，可以理解为Java数据库连接，为Java程序员开发者使用和操作数据库提供了统一编程接口API，有一组<br>
使用Java语言编写的类和接口组成。</p>
<p>JDBC整个工作包括如下具体流程：</p>
<ul>
<li>
<p>加载和操作数据库驱动程序。</p>
</li>
<li>
<p>连接数据库，建立连接</p>
</li>
<li>
<p>操作数据库：创建数据库对象(用于执行SQL语句)。这里会涉及到Statement对象或者PrepareStatement对象的创建。以及基于两者进行的执行SQL语句工作，<br>
从而获得并处理结构，这个结果通常放在ResultSet对象并且返回。</p>
</li>
<li>
<p>释放资源：释放数据库连接资源</p>
</li>
</ul>
<h4 id="配置jdbc访问数据">配置JDBC访问数据</h4>
<p>对JDBC进行简介以后，接下来就尝试如何配置JDBC的组件和环境。<br>
由于JDBC的组件包包含在JDK中因此不用额外引用，那么先来看看它的配置文件。<br>
如图1所示，在jdbc.properties文件中配置JDBC的基本信息，这里包括MySQL的驱动、数据库连接串（url）以及访问数据库的用户名和密码。</p>
<pre><code class="language-java">jdbc.driverClassName=com.mysql.cj.jdbc.Driver
jdbc.url =jdbc:mysql://
jdbc.username =root
jdbc.password =password
</code></pre>
<p>然后创建一个JDBC的访问类，如图2所示，在/spring/dao 目录下面创建JdbcBaseDao的类，<br>
该类需要对数据库驱动、url地址、用户名和密码进行初始化，这个初始化的工作后续会通过Spring IOC 完成。</p>
<p>在这个JdbcBaseDao类文件中定义了queryList、queryOne以及update等访问数据库的方法。这里我们以queryList方法为例给大家介绍。</p>
<p>如图3所示，首先通过DriverManager中的getConnection方法传入url、用户名和密码等信息创建数据库的连接。然后调用数据库连接的prepareStatement方法传入SQL语句用来处理查询操作。queryList方法通过params接受了一个参数的数组，在代码中会通过prepareStatement的setObject方法放入其中。</p>
<p>prepareStatement将SQL语句以及对应的参数做了分离，从而实现动态SQL提高了运行效率。最后通过prepareStatement的executeQuery方法从数据库中获取查询结果。这个结果返回给了resultSet变量，后面通过while循环读出resultSet变量中的信息，放入到List<T> result中并且返回给调用者。</p>
<p>上面描述了JDBC访问数据库的整个过程，这里需要注意的是PreparedStatement，它会经常在JDBC的代码中遇到，这里总结一下它的作用：</p>
<p>1）PreparedStatement 实例包含已编译的 SQL 语句。这就是使语句“准备好”。包含于 PreparedStatement 对象中的 SQL 语句可具有一个或多个 IN 参数。IN参数的值在 SQL 语句创建时未被指定。相反的，该语句为每个 IN 参数保留一个问号（“？”）作为占位符。每个问号的值必须在该语句执行之前，通过适当的setXXX 方法来提供。</p>
<p>2）由于 PreparedStatement 对象已预编译过，所以其执行速度要快于 Statement 对象。因此，多次执行的 SQL 语句经常创建为 PreparedStatement 对象，以提高效率。</p>
<p>有了JdbcBaseDao以后，就有了通过JDBC访问数据库的基本类，为了测试访问数据库的功能还需要创建一个Controller类。如图4所示，在/spring/controller目录下面创建JdbcDemoController进行测试，加入GetAll方法编写对应的SQL语句从t_teacher表中获取教师的信息，并且通过JdbcBaseDao中的queryList方法返回结果。</p>
<h4 id="jdbc的弊端">JDBC的弊端</h4>
<p>数据库连接，使用时就创建，不使用立即释放，对数据库进行频繁连接开启和关闭，造成数据库资源浪费，影响数据库性能。<br>
如图2所示，回到JdbcBaseDao类中的queryList方法，在finally代码段中须要定义对resultSet的关闭动作，同时还要对prepareStatement和conn（数据库连接）进行关闭操作，确实是一系列费事且影响数据库性能的操作。<br>
后续我们可以通过数据库连接池功能提升这部分的使用体验。</p>
<p>除了数据库连接的问题，JDBC中需要将SQL语句硬编码到java代码中，如果SQL语句修改，需要重新编译java代码。同时向PreparedStatement中设置参数，对占位符号位置和设置参数值，硬编码在java代码中，不利于系统维护。如果能够将SQL语句配置在xml配置文件中，即使SQL变化，不需要对java代码进行重新编译。后面的Mybatis优化会提到这点。</p>
<p>另外，从resultSet中遍历结果集数据时，将获取表的字段进行硬编码，使用如此冗余的操作增加了代码维护的难度。也可以通过JDBCTemplate和Mybatis的优化解决这个问题。</p>
<h4 id="基于druid连接池进行数据操作提升系统性能">基于Druid连接池进行数据操作，提升系统性能</h4>
<p>在介绍JDBC访问数据库时，提到了需要在访问数据库时建立与数据库的连接，在当使用完数据库连接的时候需要做释放连接的操作。<br>
由于，建立连接和释放连接需要耗费大量的系统资源，对于访问数据库这个经常使用的功能而言是需要进行优化的。</p>
<p>因此这里需要引入数据库连接池的概念，数据库连接池的基本思想就是为数据库建立一个缓冲池，预先在缓冲池中放入一定数量的连接，<br>
当需要建立数据库连接时，只需要从缓冲池中取出一个，使用完毕之后在放回去。我们可以通过设定连接池连接数来防止系统无尽的与数据库连接。<br>
更为重要的是我们可以通过连接池的管理机制监视数据库的连接的数量﹑使用情况，为系统开发﹑测试及性能调整提供依据。</p>
<p>Druid是Java语言中最好的数据库连接池，在功能、性能、扩展性方面，都超过其他数据库连接池，包括DBCP、C3P0、Proxool、JBoss DataSource。Druid已经在阿里巴巴部署了超过600个应用，经过生产环境大规模部署的严苛考验。</p>
<p>Druid连接池为监控而生，内置强大的监控功能，监控特性不影响整体性能。<br>
功能强大，能防SQL注入，内置Loging能诊断Hack应用行为。同时Druid支持所有JDBC兼容的数据库，包括Oracle、MySql、Derby、Postgresql、SQL Server、H2等等。<br>
Druid针对Oracle和MySql做了特别优化，比如Oracle的PS Cache内存占用优化，MySql的ping检测优化。</p>
<p>介绍完了Druid连接池，回到项目的代码中，如图1所示要使用Druid首先需要在pom.xml中配置druid的依赖信息。</p>
<pre><code class="language-java">
&lt;dependency&gt;
    &lt;groupId&gt;com.alibaba&lt;/groupId&gt;
    &lt;artifactId&gt;druid&lt;/artifactId&gt;
    &lt;version&gt;1.1.12&lt;/version&gt;
    &lt;type&gt;pom&lt;/type&gt;
&lt;/dependency&gt;

</code></pre>
<h4 id="druid连接池还存在哪些问题">Druid连接池还存在哪些问题</h4>
<p>Druid连接池的代码，除了通过数据库连接池的方式解决连接数据库断开数据的资源使用问题。</p>
<p>通过数据库连接池的方式，尽量让数据的访问先从连接池中获连接，如果连接池中没有连接的时候，再创建新的连接。</p>
<p>但是，针对连接数据库、关闭数据库、使用PrepareStatement预编译SQL语句，以及ResultSet承载需要返回的数据库信息都没有进行优化。</p>
<p>换句话说，作为开发者还是需要重复使用这些代码去访问数据库。</p>
<p>下一步，我们需要减少上述这些冗余代码的编写，尽量用更加简化的方式访问数据库，减少数据库查询/编辑缓解的代码引入，以及数据集返回时使用的额外对象。</p>
<p>也就是我们下节课要介绍的JDBCTemplate。</p>
<h4 id="基于jdbctemplate-模板方式操作数据库解决代码重复问题">基于JDBCTemplate 模板方式操作数据库，解决代码重复问题</h4>
<p>JDBCTemplate简介<br>
JDBCTemplate是Spring JDBC抽象框架提供的JDBC模板类，其中JDBCTemplate是core包的核心类，其他模板类都是基于它封装完成的。</p>
<p>JDBCTemplate类通过模板设计模式帮助消除了冗长的代码，</p>
<p>只做需要的事：SQL语句的编写，并且帮我们做固定的事：数据库连接的创建及关闭</p>
<p>如图所示，在pom.xml文件中加入依赖就可以在项目中使用JDBCTemplate了.</p>
<pre><code class="language-java">
&lt;dependency&gt;
    &lt;groupId&gt;.org.springframework&lt;/groupId&gt;
    &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt;
    &lt;version&gt;4.3.8.RELEASE&lt;/version&gt;
    &lt;type&gt;pom&lt;/type&gt;
&lt;/dependency

</code></pre>
<p>在实际工作中，JDBCTemplate主要提供以下五类方法访问数据库：</p>
<ul>
<li>
<p>execute方法：可以用于执行任何SQL语句，一般用于执行DDL语句；</p>
</li>
<li>
<p>update方法：用于执行新增、修改、删除等语句；</p>
</li>
<li>
<p>batchUpdate：方法用于执行批处理的update语句；</p>
</li>
<li>
<p>query方法：用于执行查询相关语句；</p>
</li>
<li>
<p>call方法：用于执行存储过程、函数相关语句。</p>
</li>
</ul>
<p>spring下面通过修改项目代码引入JDBCTemplate，看看我们是如何使用它的</p>
<h4 id="思考一下jdbctemplate操作数据库存在的问题">思考一下JDBCTemplate操作数据库存在的问题</h4>
<p>通过JDBC、连接池、JDBCTemplate几个阶段的改造以后，我们逐渐解决了数据库连接效率问题（连接池），手动打开关闭数据库连接（JDBCTemplate）。</p>
<p>但是作为JDBCTemplate还是需要将SQL语句硬编码到java代码中，如果SQL语句修改，需要重新编译java代码。</p>
<p>而且依旧需要输入SQL语句以及对应的Params作为参数。</p>
<p>如果能够将数据库对应的实体类与查询语句对应。</p>
<p>同时针对使用最频繁的查询语句而言，如果针对不同的条件匹配不同的参数就更好了。</p>
<p>因此我们需要一个更加强大的数据库持久层，同时具备更加灵活的SQL查询方式，不仅能够根据不同的数据库平台调整SQL的语法，也可以根据业务的调整手动调整SQL内容，而不用重新编译程序。</p>
<p>为了避免JDBCTemplate的这些问题，我们引出Mybatis的数据库访问方式。</p>
<p>MyBatis 是支持 SQL查询，存储过程和高级映射的优秀持久层框架。</p>
<p>MyBatis 消除了几乎所有的JDBC代码和参数的手工设置以及结果集的检索。</p>
<p>MyBatis 使用简单的 XML或注解用于配置和原始映射，将接口和 Java 的POJOs（Plain Ordinary Java Objects，普通的 Java对象）映射成数据库中的记录。</p>
<h4 id="mybatis运行原理介绍">Mybatis运行原理介绍</h4>
<figure data-type="image" tabindex="11"><img src="images/mybaits%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86.png" alt="Mybatis运行原理介绍" loading="lazy"></figure>
<h3 id="springevent事件通知机制底层原理">SpringEvent事件通知机制底层原理</h3>
<h4 id="springevent简单使用">SpringEvent简单使用</h4>
<p>如图1 所示，首先注入ApplicationEventPublisher对象，<br>
sendEvent方法中可以通过ApplicationEventPublisher的publishEvent方法将创建的EventDTO发送出去，<br>
通常而言在这之前可以编写一些业务逻辑，然后把业务逻辑的相关信息封装到EventDTO发送出去，从而达到通知其他组件或者模块的目的。<br>
接着，EventListener会监听发送的消息，通过注册listenEvent方法获取以后会处理接收到的EventDTO对象，并且对其进行处理。</p>
<pre><code class="language-java">
@Resource
ApplicationEventPublisher applicationEventPublisher;

//发送事件

public void sendEvent(){

    EventDto eventDto = new EventDto();
    
    applicationEventPublisher.pulishEvent(eventDto);
}



//监听事件
@EventListener
public void listenEvent(EventDto eventDto){
    System.out.println();
}

</code></pre>
<h4 id="spring-event-流程分析">Spring Event 流程分析</h4>
<p>看完Spring Event简单使用之后，<br>
可以得知Spring Event 由消息发送者ApplicationEventPublisher，<br>
消息发送体EventDTO以及消息接收者EventListener组成的.</p>
<p>将Spring Event的执行流程进行分析，该图从上往下看。</p>
<ul>
<li>首先是ApplicationEventPublisher发布event。</li>
<li>发布消息的动作会调用Multicaster中的multicastEvent方法。</li>
<li>然后检查是否有监听器监听该消息，如果没有监听整个流程结束，否则进入到下一步。</li>
<li>接着判断Multicaster是否有线程池，如果没有主线程直接调用。</li>
<li>如果Multicaster有线程池，那么多线程调用。</li>
<li>最后执行对应的方法，完成Spring Event的调用。</li>
</ul>
<figure data-type="image" tabindex="12"><img src="images/springevent%E6%B5%81%E7%A8%8B.png" alt="springevent流程" loading="lazy"></figure>
<h4 id="springevent源码分析">SpringEvent源码分析</h4>
<p>Multicaster广播器主要负责发送事件到各个订阅的Listener方法中，整个流程的核心节点。<br>
在分析整个Event之前需要对在Spring启动时，如何注入广播器等等工作做一个简单分析。<br>
Spring的启动注入流程如图3 代码所示如下，我们将特别注意的函数用红色框体标注处理，在后面进行讲解。</p>
<pre><code class="language-java">	@Override
	public void refresh() throws BeansException, IllegalStateException {
		synchronized (this.startupShutdownMonitor) {
			// Prepare this context for refreshing.
			prepareRefresh();

			// Tell the subclass to refresh the internal bean factory.
			ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory();

			// Prepare the bean factory for use in this context.
			prepareBeanFactory(beanFactory);

			try {
				// Allows post-processing of the bean factory in context subclasses.
				postProcessBeanFactory(beanFactory);

				// Invoke factory processors registered as beans in the context.
				invokeBeanFactoryPostProcessors(beanFactory);

				// Register bean processors that intercept bean creation.
				registerBeanPostProcessors(beanFactory);

				// Initialize message source for this context.
				initMessageSource();

				// Initialize event multicaster for this context.
				initApplicationEventMulticaster();

				// Initialize other special beans in specific context subclasses.
				onRefresh();

				// Check for listener beans and register them.
				registerListeners();

				// Instantiate all remaining (non-lazy-init) singletons.
				finishBeanFactoryInitialization(beanFactory);

				// Last step: publish corresponding event.
				finishRefresh();
			}

			catch (BeansException ex) {
				if (logger.isWarnEnabled()) {
					logger.warn(&quot;Exception encountered during context initialization - &quot; +
							&quot;cancelling refresh attempt: &quot; + ex);
				}

				// Destroy already created singletons to avoid dangling resources.
				destroyBeans();

				// Reset 'active' flag.
				cancelRefresh(ex);

				// Propagate exception to caller.
				throw ex;
			}

			finally {
				// Reset common introspection caches in Spring's core, since we
				// might not ever need metadata for singleton beans anymore...
				resetCommonCaches();
			}
		}
	}
</code></pre>
<ul>
<li>initApplicationEventMulticaster方法：主要对上下文中的对注册事件的广播器进行初始化。</li>
<li>registerListeners()方法：检查消息监听器并且注册这些监听器。</li>
<li>finishBeanFactoryInitialization(beanFactory)：初始化含有@EventListener注解的Bean包装到ApplicationListener中，然后将它们注入到Spring IOC容器。这里初始化的Bean指的是所有的非懒加载的Bean</li>
</ul>
<p>下面就对这三个方法的源代码进行解读。</p>
<p>initApplicationEventMulticaster 方法中重要代码<br>
如图4所示，创建一个简单广播器，传入beanFactory主要用于后期获取BeanFactory中的Listener，然后将广播器单例bean注入到Spring IOC容器中</p>
<pre><code class="language-java">
//创建一个简单广播,传入beanFactory主要用于后期获取BeanFactory中的listener
	this.applicationEventMulticaster = new SimpleApplicationEventMulticaster(beanFactory);
			beanFactory.registerSingleton(APPLICATION_EVENT_MULTICASTER_BEAN_NAME, this.applicationEventMulticaster);
</code></pre>
<p>registerListeners 是用来检查和注册监听器的，<br>
如图5所示，此方法主要将容器中所有声明的ApplicationListener的Bean对象加载到广播器中，并且发送在广播器和Bean准备期间所需要发送的事件</p>
<pre><code class="language-java">
protected void registerListeners() {
		// Register statically specified listeners first.
		for (ApplicationListener&lt;?&gt; listener : getApplicationListeners()) {
			getApplicationEventMulticaster().addApplicationListener(listener);
		}

		// Do not initialize FactoryBeans here: We need to leave all regular beans
		// uninitialized to let post-processors apply to them!
		String[] listenerBeanNames = getBeanNamesForType(ApplicationListener.class, true, false);
		for (String listenerBeanName : listenerBeanNames) {
			getApplicationEventMulticaster().addApplicationListenerBean(listenerBeanName);
		}

		// Publish early application events now that we finally have a multicaster...
		Set&lt;ApplicationEvent&gt; earlyEventsToProcess = this.earlyApplicationEvents;
		this.earlyApplicationEvents = null;
		if (earlyEventsToProcess != null) {
			for (ApplicationEvent earlyEvent : earlyEventsToProcess) {
				getApplicationEventMulticaster().multicastEvent(earlyEvent);
			}
		}
	}
</code></pre>
<p>如图6所示，将容器中初始化好的Spring Bean中包含@EventListener注解的方法和Bean包裹到一个新通用的ApplicationListener子类（即ApplicationListenerMethodAdapter中）</p>
<p>ApplicationListenerMethodAdapter类，其继承自ApplicationListener实现onApplicationEvent方法，传入参数：String beanName, Class&lt;?&gt; targetClass, Method method（@EventListener的目标方法）。<br>
如图7所示，它会实现processEvent方法，方法体中的handleResult会将目标方法执行的结果继续当做Event发送出去。</p>
<pre><code class="language-java">public void processEvent(ApplicationEvent event){
    Pbject[] args = resolveArguments(event);
    if(shouldHandler(event,args)){
        Object result =  doInvoke(args);
        if(result != null){
            handleResult(result);
        }
        else{
            logger.trace(&quot;&quot;);
        }
    }
}
</code></pre>
<h4 id="spring同步和异步事件的使用场景">Spring同步和异步事件的使用场景</h4>
<h4 id="spring消息处理机制">Spring消息处理机制</h4>
<p>Spring事件的监听机制可以理解为是一种观察者模式，有数据发布者（事件源）和数据接受者（监听器）；</p>
<p>在Java中，事件对象都是继承java.util.EventObject对象，事件监听器都是java.util.EventListener实例.</p>
<p>其中，java.util.EventObject是事件状态对象的基类，它封装了事件源对象以及和事件相关的信息。所有java的事件类都需要继承该类。</p>
<p>而java.util.EventListener是一个接口，所有事件监听器都需要实现该接口。</p>
<p>事件监听器注册在事件源上，当事件源的属性或状态改变的时候，调用相应监听器内的回调方法。</p>
<p>Source作为事件源不需要实现或继承任何接口或类，它是事件最初发生的地方。因为事件源需要注册事件监听器，所以事件源内需要存放事件监听器的容器。</p>
<h4 id="spring消息的同步处理">Spring消息的同步处理</h4>
<p>假设上面的例子中两个方法insertUsers和sendInsertUser需要同步执行，也就是insertUsers方法会先执行，完成数据库的提交以后再执行sendInsertUser方法发送消息。如果insertUsers数据库提交一直没有返回成功的结果，sendInsertUser就需要一直等待。也就是说insertUsers 会阻塞sendInsertUser方法，这就是我们常说的同步执行。为了做到同步执行就需要将监听器加入到主线程的事务中，让这两个方法顺序执行。</p>
<p>如图5所示，首先在insertUsers方法上面打上@Transcational的注释，标注它是一个事物操作</p>
<p>然后修改insertUserListener方法，在上面加上@TranscationalEventListener的注释，并且通过phase属性标注TransactionPhase.AFTER_COMMIT，意思是在事物提交以后在执行方法内容。也就是在insert user 数据库事物提交以后，再处理监听到的UserEvent消息，从而保证事务的一致性。</p>
<pre><code class="language-java">@Transaction(rollbackFor= Exceptions.class)
public void insertUser(){
    
}

@TransactionEventListener(phase = TransactionPhase.AFTER_COMMIT,fallbackException = true)
public void insertUserListener(){

}


</code></pre>
<h4 id="spring消息的异步处理">Spring消息的异步处理</h4>
<p>上面聊了同步处理的业务场景，还是这个例子我们接着聊。假设insertUsers和sendInsertUser这两个操作的业务关联性并不大，那么如何处理。例如：insertUsers之后只是发一个通知给用户，说数据入库了。就算是入库不成功，消息发送也不用撤回，毕竟没有然后的业务损失。</p>
<p>这种场景业务就可以用异步处理来做，也就是说sendInsertUser不用等待insertUsers方法数据库条件完成就可以处理发送的消息了。换句话说这两个方法不是串行执行的而是并行执行的，两个操作的执行是互相不干扰的。</p>
<p>如图7 所示，异步处理只需要在原方法上加上@Async的注释，同时需要注意的是，需要使用@EnableAsync开启Spring 异步模式</p>
<pre><code class="language-java">@Transaction(rollbackFor= Exceptions.class)
public void insertUser(){
    
}

@Async
@TransactionEventListener(phase = TransactionPhase.AFTER_COMMIT,fallbackException = true)
public void insertUserListener(){

}


</code></pre>
<h3 id="玩转spring-cache中cacheable注解的底层原理">玩转Spring Cache中@Cacheable注解的底层原理</h3>
<h4 id="cacheable注解的类结构">@Cacheable注解的类结构</h4>
<p>为了通过@Cacheable进行缓存需要在容器中注入3个bean：CacheOperationSource、BeanFactoryCacheOperationSourceAdvisor、CacheInterceptor。</p>
<p>如图1 所示，带颜色的部分就是我们需要关注的部分，其中BeanFactoryCacheOperationSourceAdvisor作为bean工厂是用来产生操作缓存源，并且对缓存@Cacheable注解进行增强的，这一点从类的名字上可以看出来。</p>
<p>CacheInterceptor是缓存的拦截器，是整个过程中处理缓存信息的类，其主要实现的方法是execute，这个方法是从CacheAspectSupoort类中继承过来的。</p>
<p>最后就是CacheOperationSource，它是处理缓存操作的源头，CacheInterceptor也就是围绕这个缓存源头展开工作。</p>
<p>CacheOperationSource有一个子类为AnnotationCacheOperationSource，这个类是作为缓存操作的源头，它会关联CacheAnnotationParser接口，使用SpringCacheAnnotationParser对缓存信息进行解析。在图的最左边是缓存操作的几个类，所有的缓存操作CacheOperation是实现了BasicOperation接口，针对@Cacheable缓存注释是继承与CacheOperation类的。</p>
<h4 id="cacheoperationsource-介绍">CacheOperationSource 介绍</h4>
<p>介绍完了@Cacheable的类/接口结构再来对其中涉及到的类文件进行表述。<br>
如图2 所示，@Cacheable作为接口定义，可以被用在方法和类型上面，在接口中还对其属性进行了描述。</p>
<pre><code class="language-java">public @interface Cacheable{
    //缓存名称，可以写多个
    @AliasFor(&quot;cacheNames&quot;)
    String[] cacheNames() default{};
    @AliasFor(&quot;value&quot;)
    String[] values() default{};
    // 支持Spel，切可以使用#root
    String key() default &quot;&quot;;
    //Mutually exclusive ：它和key属性互相排斥，请只使用一个
    String keyGenerator() defaule &quot;&quot;;
    String cacheManager() defaule &quot;&quot;;
    String cacheResolver() defaule &quot;&quot;;
    
    // Spel ,切可以使用#Root ，只有true时，才会作用在这个方法上
    String condition() default &quot;&quot;;
    // Spel ,切可以使用#Root ，并且可以使用#result拿到方法返回值
    String unless() default &quot;&quot;;
    boolean sync() default false;
    
    
}


</code></pre>
<table>
<thead>
<tr>
<th>属性名</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>value</td>
<td>缓存的名称，可以定义多个（至少需要定义一个）</td>
</tr>
<tr>
<td>cacheNames</td>
<td>同value属性</td>
</tr>
<tr>
<td>keyGenerator</td>
<td>key生成器，字符串为：beanName</td>
</tr>
<tr>
<td>key</td>
<td>缓存的key，可使用SpEL，优先级大于keyGenerator</td>
</tr>
<tr>
<td>cacheManager</td>
<td>缓存管理器填写beanName</td>
</tr>
<tr>
<td>cacheResolver</td>
<td>缓存处理器填写beanName</td>
</tr>
<tr>
<td>condition</td>
<td>缓存条件，若填写了，返回true才会执行此缓存，可使用SpEL</td>
</tr>
<tr>
<td>unless</td>
<td>否定缓存，false为失效，可以写SpEL</td>
</tr>
<tr>
<td>sync</td>
<td>true：所有相同的key的同线程顺序执行。默认是false</td>
</tr>
<tr>
<td>allEntries</td>
<td>是否清空所有缓存内容，缺省为false，如果指定为true，则方法调用后立即清空所有缓存</td>
</tr>
<tr>
<td>beforeInvocation</td>
<td>是否在方法执行前就清空，缺省为false</td>
</tr>
</tbody>
</table>
<p>既然有了@Cacheable可以在类型和方法上面标注缓存，就需要有一个对缓存进行操作的类。</p>
<p>如图4所示，CacheableOperation继承与CacheOperation类，在CacheableOperation通过build方法将缓存操作进行生成。</p>
<p>有了CacheableOperation之后需要提供一个接口给拦截器使用，也就是说拦截器通过注释获取要缓存的对象以后需要调用缓存操作，此时就有了接口CacheOperationSource，我们称之为缓存属性源。</p>
<p>如图5 所示，CacheOperationSource通过getCacheOperations方法返回所有缓存操作的CacheOperation集合。</p>
<p>那么CacheOperationSource 如何与CacheOperation产生关系的呢？</p>
<p>如图6所示，AbstractFallbackCacheOperationSource会实现CacheOperationSource接口，同时AnnotationCacheOperationSource继承与AbstractFallbackCacheOperationSource，在AnnotationCacheOperationSource中会依赖CacheOperation，调用其中对缓存操作的方法。</p>
<h4 id="beanfactorycacheoperationsourceadvisor-介绍">BeanFactoryCacheOperationSourceAdvisor 介绍</h4>
<p>说完了缓存操作源，在来看看它是如何被使用的。BeanFactoryCacheOperationSourceAdvisor 就是来整合缓存和增强器的，根据AOP的思想有了目标，定义切面和连接点以后就需要通过增强器对方法进行增强操作。</p>
<p>如图7所示，cacheAdvisor方法就是将缓存源和拦截器作为BeanFactoryCacheOperationSourceAdvisor的属性进行绑定。而CacheInterceptor也是处理缓存的主力军后面会给大家讲到。</p>
<h4 id="cacheinterceptor-介绍">CacheInterceptor 介绍</h4>
<p>说完缓存操作来源和生成缓存来源、增强器的工厂类以后，再来看看处理缓存的CacheInterceptor，从字面意思可以看出它主要是做缓存拦截器的。<br>
如图9 所示，它继承与CacheAspectSupport类，并且实现了invoke方法，在这个方法中有执行了execute方法，这个方法来自父类。</p>
<p>接着看父类的excute方法，如图10 所示，我们将CacheAspectSupport 中的execute方法截取给大家讲解。</p>
<p>其中红框的部分，首先获取CacheOperationSource也就是缓存操作源，然后通过getCacheOperations方法获取这个操作源中的所有对缓存的操作。保存到operations变量中，最后又调用了一个execute函数传入了CacheOperationContexts这个是缓存操作的上下文，所有的操作都会基于这个上下文进行，而且是多个上下文。</p>
<h3 id="精选面试题什么情况导致spring事务失效">精选面试题：什么情况导致Spring事务失效？</h3>
<ul>
<li>
<p>没有被Spring管理</p>
<pre><code>  没有被Spring管理的Bean，如果其中出现了方法需要进行事务处理的情况，此时的事务不会执行。如图1 所示，在OrderServiceImpl类中如果将@Service注释掉，那么此类对应的bean也就不会被Spring IoC容器管理。即便updateOrder上面注释了@Transactional，这个方法也不会执行事务。
</code></pre>
</li>
<li>
<p>方法不是public</p>
<p>需要注意的是，@Transactional 只能用于 public 的方法上，否则事务不会失效，如果要用在非 public 方法上，可以开启 AspectJ 代理模式。如图2 所示，saveAll方法上面标注了@Transactional的注释，由于saveAll方法没有标注public，因此saveAll中的内容是不会执行事务的。</p>
</li>
<li>
<p>数据源没有配置事务管理器</p>
<pre><code>  数据源若没有配置事务管理器，那么下面的方法transactionManager是无法实现事务的。
</code></pre>
</li>
<li>
<p>事务传播级别选择</p>
<pre><code>  如果在方法嵌套调用时存在事务传播的场景，在定义事务级别的时候使用了NOT_SUPPORTED 那么对应的方法就不会执行事务。
</code></pre>
</li>
<li>
<p>事务中出现异常</p>
<pre><code>  如果在执行的方法中执行的事务，由于异常退出的情况，那么这个事务是无法完成的。如图7 所示，在updateOrder方法中使用了try catch，一旦在方法体中出现异常，此时事务会中断无法执行
</code></pre>
</li>
<li>
<p>错误定义异常类型</p>
<pre><code>  在事务中出现异常的时候，需要对异常进行定义才能正确执行事务。如果出现错误的定义，当出现事务中的异常时，还可以继续执行事务。如图8所示，在方法updateOrder中对Exception进行了捕捉，但是在方法上面的Transactional注释中rollbackFor中定义的是SQLException.class ，正确的定义应该是Exception.class。
</code></pre>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elasticsearch]]></title>
        <id>https://zuolinlin.github.io/zuo.github.io/post/elasticsearch/</id>
        <link href="https://zuolinlin.github.io/zuo.github.io/post/elasticsearch/">
        </link>
        <updated>2022-04-08T13:23:03.000Z</updated>
        <content type="html"><![CDATA[<h1 id="elasticsearch">Elasticsearch</h1>
<h2 id="目录">目录</h2>
<ul>
<li><a href="#Elasticsearch%E6%A0%B8%E5%BF%83%E7%9F%A5%E8%AF%86%E5%85%A5%E9%97%A8%E7%AF%87">Elasticsearch核心知识入门篇</a>
<ul>
<li><a href="#Elasticsearch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8">Elasticsearch快速入门</a>
<ul>
<li><a href="#Elasticsearch%E5%8A%9F%E8%83%BD%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF%E7%89%B9%E7%82%B9">Elasticsearch功能适用场景特点</a></li>
<li><a href="#Elasticsearch%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5">Elasticsearch核心概念</a></li>
<li><a href="#Elasticsearch%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2">Elasticsearch安装部署</a></li>
<li><a href="#Elasticsearch%E6%96%87%E6%A1%A3%E7%9A%84CRUD">Elasticsearch文档的CRUD</a></li>
<li><a href="#Elasticsearch%E5%A4%9A%E7%A7%8D%E6%90%9C%E7%B4%A2%E6%96%B9%E5%BC%8F">Elasticsearch多种搜索方式</a></li>
<li><a href="#Elasticsearch%E8%81%9A%E5%90%88%E6%90%9C%E7%B4%A2">Elasticsearch聚合搜索</a></li>
</ul>
</li>
<li><a href="#Elasticsearch%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84">Elasticsearch分布式架构</a>
<ul>
<li><a href="#Elasticsearch%E5%9F%BA%E7%A1%80%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84">Elasticsearch基础分布式架构</a>
<ul>
<li><a href="#Elasticsearch%E5%AF%B9%E5%A4%8D%E6%9D%82%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%88%B6%E7%9A%84%E9%80%8F%E6%98%8E%E9%9A%90%E8%97%8F%E7%89%B9%E6%80%A7">Elasticsearch对复杂分布式机制的透明隐藏特性</a></li>
<li><a href="#Elasticsearch%E7%9A%84%E5%9E%82%E7%9B%B4%E6%89%A9%E5%AE%B9%E4%B8%8E%E6%B0%B4%E5%B9%B3%E6%89%A9%E5%AE%B9">Elasticsearch的垂直扩容与水平扩容</a></li>
<li><a href="#Elasticsearch%E5%A2%9E%E5%87%8F%E8%8A%82%E7%82%B9%E6%97%B6rebalance">Elasticsearch增减节点时rebalance</a></li>
<li><a href="#Elasticsearch%E7%9A%84master%E8%8A%82%E7%82%B9">Elasticsearch的master节点</a></li>
<li><a href="#Elasticsearch%E8%8A%82%E7%82%B9%E5%B9%B3%E7%AD%89%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84">Elasticsearch节点平等的分布式架构</a></li>
</ul>
</li>
<li><a href="#%E5%88%86%E7%89%87%E5%8E%9F%E7%90%86">index分片原理</a>
<ul>
<li><a href="#Shard&amp;replica%E6%9C%BA%E5%88%B6%E6%A2%B3%E7%90%86">shard&amp;replica机制梳理</a></li>
</ul>
</li>
<li><a href="#Elasticsearch%E6%A8%AA%E5%90%91%E6%89%A9%E5%AE%B9%E5%8E%9F%E7%90%86">Elasticsearch横向扩容原理</a>
<ul>
<li><a href="#Elasticsearch%E5%88%86%E5%B8%83%E5%BC%8F%E5%8E%9F%E7%90%86_%E6%A8%AA%E5%90%91%E6%89%A9%E5%AE%B9%EF%BC%8C%E5%A6%82%E4%BD%95%E8%B6%85%E5%87%BA%E6%89%A9%E5%AE%B9%E6%9E%81%E9%99%90%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E6%8F%90%E5%8D%87%E5%AE%B9%E9%94%99%E6%80%A7">Elasticsearch分布式原理_横向扩容，如何超出扩容极限以及如何提升容错性</a></li>
</ul>
</li>
<li><a href="#Elasticsearch%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6%EF%BC%9Amaster%E9%80%89%E4%B8%BE%EF%BC%8Creplace%E5%AE%B9%E9%94%99%EF%BC%8C%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D">Elasticsearch容错机制：master选举，replace容错，数据恢复</a></li>
</ul>
</li>
<li><a href="#Elasticsearch%E5%88%86%E5%B8%83%E5%BC%8Fdocument">Elasticsearch分布式document</a>
<ul>
<li><a href="#Index%E5%85%83%E6%95%B0%E6%8D%AE">_index元数据</a></li>
<li><a href="#Type%E5%85%83%E6%95%B0%E6%8D%AE">_type元数据</a></li>
<li><a href="#Id%E5%85%83%E6%95%B0%E6%8D%AE">_id元数据</a></li>
<li><a href="#Source%E5%85%83%E6%95%B0%E6%8D%AE">_source元数据</a></li>
<li><a href="#Document%E7%9A%84%E5%85%A8%E9%87%8F%E6%9B%BF%E6%8D%A2">document的全量替换</a></li>
<li><a href="#%E5%9F%BA%E4%BA%8Eexterna1lVersion%E8%BF%9B%E8%A1%8C%E4%B9%90%E8%A7%82%E9%94%81%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6">_基于externa1lVersion进行乐观锁并发控制</a></li>
<li><a href="#PartialUpdate">partial update</a></li>
<li><a href="#%E6%89%B9%E9%87%8F%E6%93%8D%E4%BD%9C">批量操作</a></li>
</ul>
</li>
<li><a href="#Elasticsearch%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F">Elasticsearch分布式系统</a>
<ul>
<li><a href="#Document%E6%95%B0%E6%8D%AE%E8%B7%AF%E7%94%B1%E5%8E%9F%E7%90%86">document数据路由原理</a></li>
<li><a href="#Document%E5%A2%9E%E5%88%A0%E6%94%B9%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86">Document增删改内部原理</a></li>
<li><a href="#%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8Aquorum%E6%9C%BA%E5%88%B6%E7%9A%84%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90">写一致性原理以及quorum机制的深入解析</a></li>
<li><a href="#Document%E5%86%85%E9%83%A8%E6%9F%A5%E8%AF%A2%E5%8E%9F%E7%90%86">Document内部查询原理</a></li>
<li><a href="#BuilApi%E7%9A%84%E5%A5%87%E7%89%B9json%E6%A0%BC%E5%BC%8F%E4%B8%8E%E5%BA%95%E5%B1%82%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%85%B3%E7%B3%BB">BuilApi的奇特json格式与底层性能优化关系</a></li>
</ul>
</li>
<li><a href="#Elasticsearch%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E">Elasticsearch搜索引擎</a>
<ul>
<li><a href="#Search%E7%BB%93%E6%9E%9C%E7%9A%84%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90">search结果的深入解析</a></li>
<li><a href="#Multi-index&amp;multi-type%E6%90%9C%E7%B4%A2%E6%A8%A1%E5%BC%8F%E8%A7%A3%E6%9E%90%E4%BB%A5%E5%8F%8A%E6%90%9C%E7%B4%A2%E5%8E%9F%E7%90%86">Multi-index&amp;multi-type搜索模式解析以及搜索原理</a></li>
<li>[分页搜索以及deep paging性能问题深度图解](#分页搜索以及deep paging性能问题深度图解)</li>
<li>[快速掌握query stringserach语法以及_all metadata原理揭秘](#快速掌握query stringserach语法以及_all metadata原理揭秘)</li>
<li><a href="#mapping">mapping</a></li>
<li><a href="#%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%E7%9A%84%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86%E5%BF%AB%E9%80%9F%E8%A7%A3%E5%AF%86">倒排索引的核心原理快速解密</a></li>
<li><a href="#%E5%88%86%E8%AF%8D%E5%99%A8">分词器</a></li>
<li><a href="#QueryDSL">queryDSL</a></li>
<li><a href="#%E5%AF%B9StringField%E6%8E%92%E5%BA%8F">对stringfield排序</a></li>
<li><a href="#%E7%9B%B8%E5%85%B3%E5%BA%A6%E8%AF%84%E5%88%86TF&amp;IDF%E7%AE%97%E6%B3%95">相关度评分TF&amp;IDF算法</a></li>
<li><a href="#DocValues%E6%AD%A3%E6%8E%92%E7%B4%A2%E5%BC%95">doc Values</a></li>
<li><a href="#QueryPhase">query phase</a></li>
<li><a href="#FetchPhase">fetch phase</a></li>
<li><a href="#%E6%90%9C%E7%B4%A2%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0%E6%A2%B3%E7%90%86%E4%BB%A5%E5%8F%8Abouncingresults%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">搜索相关参数梳理以及bouncing results问题的解决方案</a></li>
<li><a href="#Scoll%E6%8A%80%E6%9C%AF%E6%BB%9A%E5%8A%A8%E6%90%9C%E7%B4%A2%E5%A4%A7%E9%87%8F%E6%95%B0%E6%8D%AE">scoll技术滚动搜索大量数据</a></li>
</ul>
</li>
<li><a href="#Elasticsearch%E7%B4%A2%E5%BC%95%E7%AE%A1%E7%90%86">Elasticsearch索引管理</a>
<ul>
<li><a href="#%E7%B4%A2%E5%BC%95%E7%9A%84%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5">索引的增删改查</a></li>
<li><a href="#%E4%BF%AE%E6%94%B9%E5%88%86%E8%AF%8D%E8%B5%B7%E4%BB%A5%E5%8F%8A%E5%AE%9A%E5%88%B6%E5%88%86%E8%AF%8D%E5%99%A8">修改分词起以及定制分词器</a></li>
<li><a href="#%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%B4%A2type%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84">深入探索type底层数据结构</a></li>
<li><a href="#MappingRootObject%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90">mapping root object深入解析</a></li>
<li><a href="#%E5%AE%9A%E5%88%B6%E5%8C%96%E8%87%AA%E5%B7%B1%E7%9A%84dynamicMapping%E7%AD%96%E7%95%A5">定制化自己的dynamic mapping策略</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#Elasticsearch%E9%AB%98%E6%89%8B%E8%BF%9B%E9%98%B6%E7%AF%87">Elasticsearch高手进阶篇</a>
<ul>
<li><a href="#%E6%B7%B1%E5%BA%A6%E6%8F%AD%E7%A7%98%E6%90%9C%E7%B4%A2%E6%8A%80%E6%9C%AF">深度揭秘搜索技术</a></li>
<li><a href="#IK%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%99%A8">IK中文分词器</a></li>
<li><a href="#%E6%B7%B1%E5%85%A5%E8%81%9A%E5%90%88%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90">深入聚合数据分析</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E5%AE%9E%E6%88%98">数据建模实战</a></li>
<li><a href="#%E5%AE%8C%E6%88%90%E5%BB%BA%E8%AE%AE">完成建议</a></li>
<li><a href="#%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E9%9B%86%E7%BE%A4">生产实践-集群</a></li>
<li><a href="#Elasticsearch%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98">Elasticsearch性能调优</a></li>
</ul>
</li>
</ul>
<h1 id="elasticsearch-2">Elasticsearch</h1>
<h2 id="elasticsearch核心知识入门篇">Elasticsearch核心知识入门篇</h2>
<h3 id="elasticsearch快速入门">Elasticsearch快速入门</h3>
<h4 id="elasticsearch功能适用场景特点">Elasticsearch功能适用场景特点</h4>
<p>1.Elasticsearch的功能、适用场景、以及特点介绍</p>
<pre><code>1.分布式搜索引擎和数据分析引擎
2.全文检索、结构化检索、数据分析
3.对海量数据进行近实时处理
</code></pre>
<p>2.Elasticsearch适用场景</p>
<pre><code>1.维基百科、全文检索、高亮、搜索推荐
2.用户日志、社交网络数据、分析新闻文章公众反馈
3.日志数据分析、logstash采集日志、复杂的数据分析
4.分布式搜索引擎和数据分析引擎
5.全文检索、结构化检索、数据分析
6.对海量数据进行近实时处理
</code></pre>
<p>3.Elasticsearch特点介绍</p>
<pre><code>1.可以作为大型分布式集群技术，处理PB级数据服务大公司，也可以在单机上服务小公司
2.全文检索、数据分析、分布式技术结合在一起。
3.开箱即用、非常简单
4.Elasticsearch提供了全文检索、同义词处理、相关度排序、复杂数据分析、海量数据近实时的功能
</code></pre>
<h4 id="elasticsearch核心概念">Elasticsearch核心概念</h4>
<pre><code>1.Near Realtime(NRT)：近实时,意思是：从查询数据库到数据可以被es搜索到有一个延迟(大概1S);基于es执行搜索和分析可以达到秒级
2.cluster:集群,包含多个节点,每个节点属于哪个 集群是通过一个配置(集群名称,默认是elasticsearch)来决定的。
3.node：节点，几圈中的一个节点，节点也有名称（默认是随机分配的）,节点名称很重要(在运维管理进行操作的时候),默认节点会去加入一个名称为&quot;elasticsearch&quot;的集群中，如果直接启动一堆节点,那么他们会自动组成elasticsearch集群，当然一个节点可以组成elasticsearch集群
4.document:文档,es中最小的数据单元,一个document可以是一条订单数据或者是一个商品数据，通常用JSON数据结构表示,每个index下type中都可以存储多个document,一个document里面有多个field，每个field就是一个数据字段
5.index:索引，包含一堆相似的文档数据,比如订单索引，索引有一个名称。一个index包含多个document，一个index就代表一类类似的document.比如说建立一个product index，商品索引,里面可能就存放了所有的商品数据，所有的商品document.
6.type:类型,每个索引都可以有一个活多个type，type是index中的一个逻辑数据分类,一个type下的document都有相同的field
7.shard：单台机器无法存储大量数据,es可以将一个索引中的数据切分为多个shard，分布在多台服务器上存储.有了shard就可以横向扩展，存储更多数据,让搜索和分析等操作分布到多台机器上，执行速度快,提升吞吐量和性能.
8.replica:粉盒一个服务器随机可能出现故障或者宕机.因此可以为每个shard创建多个replica副本,replica可以在shard故障时候提供备用，保证数据不丢失。多个replica哈可以提升搜索作用的吞吐量和性能。
primary shard(建立索引时候一次设置,不能修改,默认5个),replica shard（随时修改数量,默认1个）,默认每个索引10个shard.5个primary shard。5个replica shard。最小高可用配置，是两台server.
</code></pre>
<h4 id="elasticsearch安装部署">Elasticsearch安装部署</h4>
<p>Elasticsearch安装</p>
<pre><code>docker pull elasticsearch:7.6.2

vim /etc/sysctl.conf #文件最后添加一行 vm.max_map_count=262144

mkdir -p /data/elasticsearch/config
mkdir -p /data/elasticsearch/data

进入config目录下 创建 elasticsearch.yml文件 粘贴下面配置

http.host: 0.0.0.0
http.port : 9200
transport.tcp.port : 9300
http.cors.enabled : true
http.cors.allow-origin : &quot;*&quot;
network.bind_host: 0.0.0.0
xpack.security.enabled: true
xpack.security.audit.enabled: true

#启动

docker run -d --restart=always -p 9200:9200 -p 9300:9300 --name elasticsearch -e &quot;discovery.type=single-node&quot; -e &quot;cluster.name=elasticsearch&quot; -v /data/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml -v /data/elasticsearch/data:/usr/share/elasticsearch/data -v /data/elasticsearch/plugins:/usr/share/elasticsearch/plugins elasticsearch:7.6.2

#启动完毕后设置密码 参考文档: https://www.cnblogs.com/woshimrf/p/docker-es7.html
#进入容器
docker exec -it elasticsearch /bin/bash
执行: ./bin/elasticsearch-setup-passwords auto
输出以下信息:
Changed password for user apm_system
PASSWORD apm_system = l5CWYr67Q6CJUzpKyvZb

Changed password for user kibana
PASSWORD kibana = HOauyvrBjHKxwQ1R2Idt

Changed password for user logstash_system
PASSWORD logstash_system = sHvJEh4kxu0inCAlk8Uc

Changed password for user beats_system
PASSWORD beats_system = 8YmZ4TAAlaSzuVMgBSDi

Changed password for user remote_monitoring_user
PASSWORD remote_monitoring_user = X48M4DRRnWBTgG8y9dWb

Changed password for user elastic
PASSWORD elastic = e4R0G5bbwWTT7IuTdR63

审核服务器:
Changed password for user apm_system
PASSWORD apm_system = E1OeyBsIoY1f4Hk8p3gM

Changed password for user kibana
PASSWORD kibana = 2qlVVaovTSguYNhw4YRf

Changed password for user logstash_system
PASSWORD logstash_system = m0Z9JdoGcPLLuLtjqrNv

Changed password for user beats_system
PASSWORD beats_system = LgNP0AhqfkEBKyg7E006

Changed password for user remote_monitoring_user
PASSWORD remote_monitoring_user = MpiGnxDhy36DtSviy7Bj

Changed password for user elastic
PASSWORD elastic = YWUVdA9MQPhUcFAt9JdH

生产密码:
Changed password for user apm_system
PASSWORD apm_system = rx7WKzaqc1jZMLISiodA

Changed password for user kibana
PASSWORD kibana = lXGbcUu5wFH27XLeOUfL

Changed password for user logstash_system
PASSWORD logstash_system = 9S2Mg1vkUjeqQUdDmCik

Changed password for user beats_system
PASSWORD beats_system = k0pxVqpDQtJnK6z6sjok

Changed password for user remote_monitoring_user
PASSWORD remote_monitoring_user = gN9dIqHHHRMyVmzifIVU

Changed password for user elastic
PASSWORD elastic = n4rI5IOzxqC0Db1HJKzc
</code></pre>
<p>查看Elasticsearch是否启动成功</p>
<p>http://ip:port/?pretty</p>
<pre><code>{
&quot;name&quot; : &quot;66405ae2daed&quot;,    //Node名称
&quot;cluster_name&quot; : &quot;docker-cluster&quot;,  //集群名称  在Elasticsearch.yml 文件里修改
&quot;cluster_uuid&quot; : &quot;5_7wD3BtSKSuIYrBas5w0A&quot;,
&quot;version&quot; : {
&quot;number&quot; : &quot;7.13.1&quot;,         //版本号
&quot;build_flavor&quot; : &quot;default&quot;,
&quot;build_type&quot; : &quot;docker&quot;,
&quot;build_hash&quot; : &quot;9a7758028e4ea59bcab41c12004603c5a7dd84a9&quot;,
&quot;build_date&quot; : &quot;2021-05-28T17:40:59.346932922Z&quot;,
&quot;build_snapshot&quot; : false,
&quot;lucene_version&quot; : &quot;8.8.2&quot;,
&quot;minimum_wire_compatibility_version&quot; : &quot;6.8.0&quot;,
&quot;minimum_index_compatibility_version&quot; : &quot;6.0.0-beta1&quot;
},
&quot;tagline&quot; : &quot;You Know, for Search&quot;
}
</code></pre>
<p>查看Elasticsearch集群状态<br>
GET ip:port/_cat/health?v</p>
<p>green：每个索引的primary shard和replica shard 都是activity<br>
yellow：每个索引的primary shard 都是activity ，部分replica shard 不是activity状态，是不可用的状态<br>
red：不是所有的primary shard都是activity，部分索引有数据丢失</p>
<p>kibana安装</p>
<pre><code>mkdir -p /data/kibana/config
docker pull kibana:7.6.2

vim /data/kibana/config/kibana.yml # 填入下面配置

i18n.locale: 'zh-CN'
server.host: '0.0.0.0'
elasticsearch.hosts: ['http://172.18.14.12:9200','http://172.18.14.15:9200','http://172.18.14.10:9200']
elasticsearch.username: 'elastic'
elasticsearch.password: 'n4rI5IOzxqC0Db1HJKzc'
xpack:
  apm.ui.enabled: false
  graph.enabled: false
  ml.enabled: false
  monitoring.enabled: false
  reporting.enabled: false
  security.enabled: false
  grokdebugger.enabled: false
  searchprofiler.enabled: false


# 运行
docker run -d -it --restart=always --privileged=true --name=kibana -p 15601:5601 -v /data/kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml kibana:7.6.2
</code></pre>
<h4 id="elasticsearch文档的crud">Elasticsearch文档的CRUD</h4>
<p>快速查看集群中有哪些索引</p>
<pre><code>    GET /_cat/indecs?v
</code></pre>
<p>创建索引</p>
<pre><code>   PUT /test?pretty
</code></pre>
<p>删除索引</p>
<pre><code>DELETE /test?pretty
</code></pre>
<p><strong>document CRUD</strong></p>
<pre><code>新增document文档
    put /index/type/id
    {
        &quot;key1&quot;:&quot;value1&quot;,
        &quot;key2&quot;:&quot;value2&quot;,
    }
更新document文档
POST /index/type/id/_update
   &quot;doc&quot; {
        &quot;key1&quot;:&quot;value1&quot;,
        &quot;key2&quot;:&quot;value2&quot;,
}

删除document文档
DELETE /index/type/id?pretty
</code></pre>
<h4 id="elasticsearch多种搜索方式">Elasticsearch多种搜索方式</h4>
<p>（1）、query string search（因为search都是http请求query string来附带的）</p>
<pre><code>语法：GET /index/type/_search

例：GET /index/type/_search?q=&quot;search&quot;&amp;sort=filed:desc

结果
    {
    &quot;took&quot; : 0, h耗费了几秒
    &quot;timed_out&quot; : false, 是否超时
    &quot;_shards&quot; : { // 请求了几个shard
    &quot;total&quot; : 3,
    &quot;successful&quot; : 3,
    &quot;skipped&quot; : 0,
    &quot;failed&quot; : 0
    },
    &quot;hits&quot; : {
    &quot;total&quot; : {  查询的数量
    &quot;value&quot; : 174,
    &quot;relation&quot; : &quot;eq&quot;
    },
    &quot;max_score&quot; : 1.0, 相关度匹配分数，越相关就越匹配，分数越高
    &quot;hits&quot; : [  匹配的document的相关数据
    ]
</code></pre>
<p>在生产环境很少用</p>
<p>（2）、query DSL（Domain Specified Language 特定领域的语言） 基于Http request body请求体，可以用json格式构建语法，可以构建各种复杂的语法</p>
<pre><code>    例如：  
    {
        &quot;query&quot;:{ //查询
        “match_all”:{ //
        },
        &quot;filter&quot; :{ // 过滤
            &quot;range&quot;{
                &quot;price&quot; :{&quot;gt&quot;,&quot;&quot;}
            }
        }
    }
        &quot;sort&quot;:[ 排序
        {
        &quot;price&quot;:&quot;desc&quot;
        }
        ]
        &quot;from&quot;:1, 查询游标
        &quot;size&quot;:2 查询数量
        }
        &quot;_source&quot;:[&quot;&quot;,&quot;&quot;] :指定要查询出来的field
     }
</code></pre>
<p>match_all:全部查询<br>
match:全文检索，将搜素词拆分为一个个词之后去倒排索引中进行匹配<br>
match_phrase(短语搜索) :要求输入的搜索串，必须在指定字段文本中，完全一模一样的，才能算匹配<br>
sort:排序<br>
highlight:高亮<br>
from:查询游标<br>
size：查询数量<br>
_source:指定要查询出来的field</p>
<h4 id="elasticsearch聚合搜索">Elasticsearch聚合搜索</h4>
<pre><code>GET /index/type/_search
{
    &quot;size&quot;:0,         // 去掉返回值里的hits 具体的document
    &quot;query&quot;：&quot;&quot;  // 查询条件之后在分组
    &quot;aggs&quot;:{
        &quot;group_by_tags&quot;:{       //聚合的别名
            //分组下的操作
            &quot;terms&quot;:{
                &quot;field&quot;:&quot;vaue&quot; //根据field的分组
                &quot;order&quot;:{&quot;group_by_tag1&quot;:desc/asc}  //按照内层聚合的结果降序排序
            },
            // 组内分组计算
            &quot;aggs&quot;:{
                &quot;group_by_tag1&quot;:{
                  }
                }
        }
    }
}
</code></pre>
<p>为需要聚合的filed添加正排索引</p>
<pre><code>PUT /index/_mapping/type
{
    &quot;properties&quot;:{
        &quot;field&quot;:{ //根据filed的设置
            &quot;type&quot;:&quot;string&quot;,
            &quot;fielddata&quot;:&quot;true&quot;,
        }
    }
    
}
</code></pre>
<h3 id="elasticsearch分布式架构">Elasticsearch分布式架构</h3>
<h4 id="elasticsearch基础分布式架构">Elasticsearch基础分布式架构</h4>
<h5 id="elasticsearch对复杂分布式机制的透明隐藏特性">Elasticsearch对复杂分布式机制的透明隐藏特性</h5>
<pre><code>分片机制:我们将document插入到es集群中去，不用关心数据是怎么进行分片的，数据到哪个shard中
cluster discovery：新加入的node自动发现集群，并且加入进去还接受了部分数据
Shared 负载均衡:es会自动进行负载均衡（让每个node上具备差不多的shard数量），以保持每个节点均衡读写负载请求
share副本:rep1ica shard是primary. shard的副本
集群扩容:水平扩容
请求路由:节点对等
share重分配:集群rebalance
</code></pre>
<h5 id="elasticsearch的垂直扩容与水平扩容">Elasticsearch的垂直扩容与水平扩容</h5>
<pre><code>(1).垂直扩容：采购更强大的服务器。成本高，有瓶颈
(2).水平扩容：增加服务器的数量
</code></pre>
<h5 id="elasticsearch增减节点时rebalance">Elasticsearch增减节点时rebalance</h5>
<pre><code>总有一些服务器负载重一些，承载的数据和请求会大一些，当增加或者减少节点时，数据分片会重新rebalance，实现shard的负载均衡（让每个节点的数据量差不多）
</code></pre>
<h5 id="elasticsearch的master节点">Elasticsearch的master节点</h5>
<p>（主要管理es的元数据）</p>
<pre><code>(1)、创建或者删除索引
(2)、增加或者删除节点
</code></pre>
<h5 id="elasticsearch节点平等的分布式架构">Elasticsearch节点平等的分布式架构</h5>
<pre><code>(1)、节点对等，每个节点都能接受所有请求
(2)、自动请求路由
(3)、响应收集
</code></pre>
<h4 id="分片原理">分片原理</h4>
<h5 id="shardreplica机制梳理">Shard&amp;replica机制梳理</h5>
<pre><code>(1) index包含 多个shard，将多个shard分配到各个节点上去，每个shard存储一部分数据
(2)每个shard都是一个最小工作单元， 承载部分数据，1ucene实例， 完整的建立索引和处理请求的能力
(3)增减节点时，shard会自动在nodes中负载均衡
(4) primary shard和replica shard; ，每个document肯定只存在于某一个primary      shard以及其对应的rep1ica shard中， 不可能存在于多个primary shard
(5) rep1ica shard是primary. shard的副本， 负责容错，以及承担读请求负载
(6).primary shard的数量在创建索引的时候就固定了，replica shard的数量可以随时修改
(7).primary shard的默认数量是5，rep1ica默认是1:默认直10个shard; 5 primary shard, 5个replica, shard
(8) primary shard不能和自己的replica shard放在同一个节点上(否则节点宕机，primary shard和副本都丢失，起不到容错的作用)，但是可以和其他primary shard的 ep1ica shard放在同一一个节点上
</code></pre>
<h4 id="elasticsearch横向扩容原理">Elasticsearch横向扩容原理</h4>
<h6 id="elasticsearch分布式原理_横向扩容如何超出扩容极限以及如何提升容错性">Elasticsearch分布式原理_横向扩容，如何超出扩容极限以及如何提升容错性</h6>
<pre><code>1、图解横向扩容过程，如何超出扩容极限，以及如何提升容错性
(1) primary&amp;rep1ica 自动负载均衡，6个shard， 3 primary，3 replica
(2)每个node有更少的shard, I0/CPU/Memory资 源给每个shard分配更多，每个shard性能更好
(3) 扩容的极限， 6个shard (3 primary, 3 replica) ，最多扩 容到6台机案，每个shard可以占用单 台服务器的所有资源，性能最好
(4)超出扩容极限，动态修改rep1ica数量，9个shard (3primary， 6 rep1ica)，扩容到9台机器，比3台机器时，拥有3倍的读吞吐量
(5) 3台机器下，9个shard (3 primary, 6 replica) ，资源更少，但是容错性更好，最多容纳2台机器宕机，6个shard只能容纳1台机器宕
</code></pre>
<h4 id="elasticsearch容错机制master选举replace容错数据恢复">Elasticsearch容错机制：master选举，replace容错，数据恢复</h4>
<pre><code>(1).容错第-步: master选举，自动选举另外一个node成为新的master ,承担起master的责任来
(2).容错第二步:新master ,将丢失掉的primary shard的某个replica shard提升为primary shard.此时cluster status会变为yellow ,因为primaryshard全都变成active了.但是,少了一个replica shard ,所以不是所有的replica shard都是active了.
(3).容错第三步:重启故障的node ,new master ,会将缺失的副本都是copy-份到该node上去。而且该node会使用之前已有的shard数据，只是同步- -下宕机之后发生过的修改。cluster status变为green,因为primary shard和replica shard都齐全了
</code></pre>
<h3 id="elasticsearch分布式document">Elasticsearch分布式document</h3>
<h4 id="index元数据">Index元数据</h4>
<pre><code>(1)代表一个document存放在哪个index中
(2)类似的数据放在一一个素引，非类似的数据放不同索引
(3) index中包含了很多类似的document
(4)索引名称必须是小写的，不能用下划线开头，不能包含逗号
</code></pre>
<h4 id="type元数据">Type元数据</h4>
<pre><code>(1)代表document属于index中的哪个类别(type)
(2)一个索引通常会划分为多个type,谭辑上对index中有些许不同的几类数据进行分类
(3) type名称可以是大写或者小写，但是同时不能用下划线开头，不能包含逗号
</code></pre>
<h4 id="id元数据">Id元数据</h4>
<pre><code>(1)代表document的唯-标识， 与index和type一起， 可以唯-标识和定位一-个document
(2)我们可以手动指定document的id,也可以不指定，由es自动为我们创建一-个id
</code></pre>
<p>根据应用情况来说，是否满足手动指定document id的前提:</p>
<pre><code>-般来说，是从某些其他的系统中，导入一些数据到es时，会采取这种方式，就是使用系统中已有数据的唯一标识，作为es中document的id。 举个例子，比如说，我们现在在开发
- 一个电商网站，做搜索功能:或者是0A系统，做员工检索功能。这个时候，数据首先会在网站系统或者IT系统内部的数据库中，会先有一 份，此时就肯定会有- 一个数据库的primary
  key (自增长，UID,或者是业务编号)。如果将数据导入到es中，此时就比较适合采用数据在数据库中已有的primary key。
  如果说，我们是在做一个系统，这个系统主要的数据存储就是es- -种，也就是说，数据产生出来以后，可能就没有id, 直接就放es- -个存储，那么这个时候，可能就不太适合说手
  动指定document id的形式了，因为你也不知道i d应该是什么，此时可以采取下面要讲解的让es自动生成i d的方式。
  (2) put /index/ type/id
  (3.2)、自动生成document id
  (1) post ./index/ type.
  (2)自动生成的id,长度为20个字符，URL安全，base64编码， GUID, 分布式系统并行生成时不可能会发生冲突
</code></pre>
<h4 id="source元数据">Source元数据</h4>
<pre><code>_source元数据:就是说，我们在创建一个document的时候， 使用的那个放在request body中的json串， 默认情况下，在get的时候，会原封不动的给我们返回回来。定制返回的结果，指定_source中，返回哪些field
</code></pre>
<h4 id="document的全量替换">Document的全量替换</h4>
<p>(1)语法与创建文档是一样的。如果document id不存在，那么就是创建:如果document. ig已经存在，那么就是舍量替换操作,替换document的json串内容<br>
(2) document是不可变的，如果要修改document的内容，第-种方式就是全量替换，直接对document重新建立索引，替换里面所有的内容。<br>
PUT /index/type/id?<br>
(3) es会将老的document标记为deleted,然后新增我们给定的一个document, 当我们创建越来越多的document的时候，es会 在适当的时机在后台自动删除标记为deleted的<br>
document<br>
2、document的强制创建<br>
(1)创建文档与全量替换的语法是一样的， 有时我们只是想新建文档，不想替换文档，如果强制进行创建呢?<br>
(2) PUT /index/ type/id?op__type=create,<br>
PUT /index/type/id/_create<br>
3、document的删除<br>
(1) DELETE  index/type/id<br>
(2)不会理解物理删除，只会将其标记为deleted，当数据越来越多的时候，在后台自动删除</p>
<p>1.Elasticsearch内部如何基于_version如何进行乐观锁并发控制</p>
<p>1.第一次创建一个document的时候，它的_version内部版本号就是1;<br>
2.以后，每次对这个document执行修改或者删除操作，都会对这个_version版本号自动加1;哪怕是删除，也会对这条数据的版本号加1<br>
3.多线程并发更新数据时,先获取document数据和最新版本号， 只有当你提供的version与es中的，version-模一样的时候，才可以进行修改，只要不一样，就报错或执行retry策略（retry_ on_ conf1ict）;<br>
3.别的线程更新失败后，执行retry策略<br>
retry策略<br>
1、再次获取document数据和最新版本号<br>
2、基于最新版本号再次去更新，如果成功那么就ok了<br>
3、如果失败,重复1和2两个步骤,最多,重复几次呢?可以通过retry那个参数的值指定,比如5次</p>
<h4 id="基于externa1lversion进行乐观锁并发控制">基于externa1lVersion进行乐观锁并发控制</h4>
<pre><code>es提供了一个feature, 就是说，你可以不用它提供的内部_version版本号来进行并发控制，可以基于你自己维护的一一个版本号来进行并发控制。
举个列子，加入你的数据在mysq1
里也有一份，然后你的应用系统本身就维护了-一个版本号，无论是什么自己生成的，程序控制的。
这个时候，你进行乐观锁并发控制的时候，可能并不是想要用es内部的_version来进行控制，而是用你自己维护的那个version来进行控制。
?version=1
?version=1&amp;version_type=externa1

version_type=externa1,唯一-的区别在于， version, 只有当你提供的version与es中的，version-模一样的时候，才可以进行修改，只要不一样，就报错;

当version_type=externa1的时候， 只有当你提供的versi on比es中的_versi on大的时候，才能完成修改
es，_version=1?version=1， 才能更新成功
es，_version=1?version&gt; 1&amp;version_type=externa1, 才能成功，比如说?versi on=2&amp;version_type=externa1
</code></pre>
<h4 id="partialupdate">PartialUpdate</h4>
<p>全量替换语法：</p>
<pre><code>PUT /index/type/id,创建文档&amp;替换文档，就是-样的语法
</code></pre>
<p>partial update语法：</p>
<pre><code>post /index/ type/id/_ update
{
'要修改的少数几个fie1d即可，不需要全量的数据”：&quot;&quot;，
&quot;retry_ on_ conf1ict&quot;:&quot;5&quot;  //retry策略
}
</code></pre>
<p>般对应到应用程序中，每次的执行流程基本是这样的（和全量替换的原理一样）:</p>
<pre><code>(1)应用程序发起一个get请求，获取到document， 展示到前台界面，供用户查看和修改
(2)用户在前台界面修改数据，发送到后台
(3)后台代码，会将用户修改的数据在内存中进行执行，然后封装好修改后的全量数据
(4)然后发送PUT请求，到es中， 进行全量替换
(5) es将老的document标记为de1eted，然后重新创建一个 新的document
</code></pre>
<p>看起来，好像就比较方便了，每次就传递少数几个发生修改的fie1d即可，不需要将全量的document数据发送过去</p>
<p>2、图解partial update实现原理以及其优点</p>
<pre><code>partial update, 看起来很方便的操作，实际内部的原理是什么样子的，然后它的优点是什么|

其实es内部对partial update的实际执行,跟传统的全量替换方式，是几乎-样的

1、内部先获取document
2、将传过来的field更新到document的json中
3、将老的document标记为deleted
4、将修改后的新的document创建出来
</code></pre>
<p>partial update相较于全量替换的优点</p>
<pre><code>1、所有的查询、修改和写回操作,都发生在es中的一 个shard内部,避免了所有的网络数据传输的开销(减少2次网络请求) , 大大提升了性能
2、减少了查询和修改中的时间间隔,可以有效减少并发冲突的情况
</code></pre>
<p>基于groovy脚本，如何执行partial update</p>
<pre><code>es， 其实是有个内置的脚本支持的， 可以基于groovy脚本实现各种各样的复杂操作
基于groovy脚本，如何执行partia1 update.
es scripting module, 我们会在高手进阶篇去讲解，这里就只是初步讲解一-下
</code></pre>
<p>partial update乐观锁并发控制原理</p>
<p>1.第一次创建一个document的时候，它的_version内部版本号就是1;<br>
2.以后，每次对这个document执行修改或者删除操作，都会对这个_version版本号自动加1;哪怕是删除，也会对这条数据的版本号加1<br>
3.多线程并发更新数据时,先获取document数据和最新版本号， 只有当你提供的version与es中的，version-模一样的时候，才可以进行修改，只要不一样，就报错或执行retry策略（retry_ on_ conf1ict）;<br>
3.别的线程更新失败后，执行retry策略<br>
retry策略<br>
1、再次获取document数据和最新版本号<br>
2、基于最新版本号再次去更新，如果成功那么就ok了<br>
3、如果失败,重复1和2两个步骤,最多,重复几次呢?可以通过retry那个参数的值指定,比如5次</p>
<h4 id="批量操作">批量操作</h4>
<p>mget批量查询API(很重要，性能优化的一种方式)</p>
<pre><code>就是一条一条的查询，比如说要查询100条数据，那么就要发送100次网络请求，这个开销还是很大的
如果进行批量查询的话，查询100条数据，就只要发送1次网络请求，网络请求的性能开销缩减100倍
</code></pre>
<p>语法一：查询不同一个index不同type的Document</p>
<pre><code>GET /index/_mget
    {
    &quot;docs&quot;:[
        {
        &quot;_index&quot;：&quot;index&quot;，
        &quot;_type&quot;: &quot;type&quot;,
        &quot;_id&quot;:&quot;id&quot;
        },
        {
        &quot;_index&quot;：&quot;index&quot;，
        &quot;_type&quot;: &quot;type&quot;,
        &quot;_id&quot;:&quot;id&quot;
        }
}
</code></pre>
<p>语法二：查询同一个index不同type的Document</p>
<pre><code>GET /index/_mget
{
    &quot;docs&quot;:[
        {
            &quot;_type&quot;: &quot;type&quot;,
            &quot;_id&quot;:&quot;id&quot;
        },
        {
        &quot;_type&quot;: &quot;type&quot;,
        &quot;_id&quot;:&quot;id&quot;
        }
}
</code></pre>
<p>mget的重要性</p>
<p>可以说mget是很重要的，一 般来说，在进行查询的时候，如果一 次性要查询多条数据的话，那么一定要用batch批量操作的api<br>
尽可能减少网络开销次数，可能可以将性能提升数倍，其至数十倍，非常非常之重要</p>
<p>bulk批量增删改</p>
<p>1、bulk语法</p>
<pre><code>POST /_bulk 或则POST index/type/_bulk
delete&quot;: {” index&quot;:&quot; test_ index,，” type&quot; :“ test_ type'“?”J
create&quot; :index&quot; :” test_ index，”_ type' :” test_ type&quot;，”_ id&quot; :“ 12”} }test_ fie1d&quot;: &quot; test12”}
index&quot;: {_index&quot;:&quot; test_ index' '，”_type&quot; : &quot;test_ _type” }}，test_ field&quot; :auto-generate id test
index&quot; :_index&quot; :” test_ index”_type”: &quot;test_ type&quot;， ”id&quot;:“2”}}'test_ fie1d”:&quot; replaced test2”
update&quot; : {”_ index :“ test_ index'，“type”: &quot;test_ _type”，”id&quot;: &quot;1”， ”retry_ on_ conf1ict&quot; :3} }
{“doc&quot; : {&quot;test_ fie1d2” : &quot;bulk test1&quot;} }
</code></pre>
<p>每一-个操作要两个json串， 语法如下:</p>
<pre><code>'action&quot;: { 'metadata&quot;}}&quot;data&quot;}
举例，比如你现在要创建一 个文档，放bulk里面，看起来会是这样子的:

{&quot; index&quot;: {&quot;_ _index&quot;: &quot;test_ index&quot;，”type&quot;，&quot;test_ type&quot;， &quot; id&quot;:“1&quot;}}
{&quot; test_ fie1d1&quot;:“ test1&quot;，&quot;test_ fie1d2&quot;: &quot;test2&quot;}
</code></pre>
<p>一个操作的语法不能换行，不同操作的语法要换行</p>
<p>有哪些类型的操作可以执行呢?</p>
<pre><code>(1) delete: 删除一个文档
(2) create: PUT /index/type/id/ create， 强制创建
(3) index: 普通的put操作，可以是创建文档，也可以是全量替换文档
( 4) update: 执行的partial update操作
</code></pre>
<p>bulk size最佳大小</p>
<pre><code>bulk, reguest会加载到内存里， 如果太大的话，性能反而会下降，因此需要反复尝试一 个最佳的bulk size。 - -般从1000 5000条数据开始，尝试逐渐增加。另外，如果看大小的话
最好是在5^ 15MB之间。
</code></pre>
<p>document 总结</p>
<pre><code>到目前为止，你觉得你在学什么东西，给大家个真观的感觉，好像已经知道了es是分布式的， 包括一些基本的原理，然后化了不少时间在学习document本身相关的操作，增删改
查。一句话点出来，给大家归纳总结一 下，其实我们应该思考一 下，es的一个最最核心的功能，已经被我们相对完整的讲完了。
Blastigsearch件电智
来以
其实起到的第一个最核心的功能。就是、个分布式的文档数据存储系统。ES是 分布式的。文档数据存储系统。文档据，存储系统。
文档数据: es可以存储和操作json文档类型的数据， 而且这也是es的核心数据结构。
存储系统: es可以对json文档类型的数据进行存储，查询，创建，更新，删除，等等操作。其实已经起到了一个什么样的效果呢?其实ES满足了这些功能，就可以说已经是一个
NoSQL的存储系统了。
围绕着document在操作，其实就是把es当成了一个NoSQL存储引擎，一个 可以存储文档类型数据的存储系统，在操作里面的document。
s可以作为一个分布式的文档存储系统，所以说，我们的应用系统，是不是就可以基于这个概念，去进行相关的应用程序的开发了。
什么类型的应用程序呢?
I (①)数据量较大: es的分布式本质，可以帮助你快速进行护
有的
承载大量数据
(2)
教据精构家适备著随时可能会奔化计雪且教据替格之间的美系常基森出 如果聚们用传统数摄居奇那号不是很玩，因为要面临太量的表
(3)
对数据的
作较为简单，比
就是一些间
改查，用我们之前讲解的那些document操作就可以搞定
(4) NoSQL数据库，适用的也是类似于，上面的这种场景
举个例子，比如说像一些网站系统，或者是普通的电商系统，博客系统，面向对象概念比较复杂，但是作为终端网站来说，没什么太复杂的功能，就是一些简单的CRUD操作， 而且
数据量可能还比较大。这个时候选用ES这种NoSQL型的数据存储，比传统的复杂的功能务必强大的支持SOL的关系型数据库，更加合适一些。无论是性能，还是
春吐量，可能都会更|
好。
</code></pre>
<h3 id="elasticsearch分布式系统">Elasticsearch分布式系统</h3>
<h4 id="document数据路由原理">Document数据路由原理</h4>
<p>document路由到shard是什么意思？</p>
<p>我们知道一个index数据会被分为多片，每片都在一个shard中<br>
所以一个document，只能存放在一个shard中（primary shard）</p>
<p>后面primary shard 会同步到replica shard上</p>
<p>当客户端创建document的时候，此时需要决定，这个document<br>
要放在这个index哪个shard上。<br>
<img src="images/document_route.png" alt="document_route.png" loading="lazy"></p>
<p>(2)路由算法: shard = hash(routing) % number_of_primary_shards举个例子，一个index有3个primary shard, PO, P1, P2</p>
<pre><code>每次增册改查一 个document的时候， 都会带过来一 个routing number,
默认就是这个document的_id (可能是手动指定，也可能是自动生成)routing =. id, 假设. id=1

会将这个routing值， 传入一个hash函数中，产出- -个routing值的hash值， hash(routing) = 21

然后将hash函数产出的值对这个index的primary shard的数量求余数，21%3= 0就决定了，这个document就放在P0上。
</code></pre>
<p>决定一个document在哪个shard上， 最重要的一一个值就是routing值，默认是id,也可以手动指定，相同的routing值，</p>
<p>每次过来，从hash函数中， 产出的hash值-定是相同的无论hash值是几，无论是什么数字，对number_ _of. primary. shards求余数，结果-定是在0 *number_of_primary_shards-1之间这个范围内的。0,1, 2。</p>
<p>(3)_ id or custom routing value默认的routing就是id.</p>
<pre><code>也可以在发送请求的时候，手动指定一 个routing value, 比如说put /index/type/id?routing =useid

手动指定routing value是很有用的， 可以保证说，某- -类document- -定被路由到一-个shard上去， 那么在后续进行应用级别的负载均衡，以及提升批量读取的性能的时候，是很有
</code></pre>
<p>(4) primary shard数量不可变的谜底（路由算法只和primary. shards的数量有关）</p>
<pre><code>因为shard = hash(routing) % number_of_primary_shards决定了document 在哪个shard上，
如果改变，导致数据不在之前的shard上，导致查询的时候，无法找到，就会间接导致数据丢失。
private shard 一旦index建立，是不允许修改的，replica shard 是可以改变的
</code></pre>
<h4 id="document增删改内部原理">Document增删改内部原理</h4>
<figure data-type="image" tabindex="1"><img src="images/document_rud.png" alt="document_rud.png" loading="lazy"></figure>
<pre><code>(1)客户端任意选择-个node发送请求过去，，这个node就会变成coordinating. node (协调节点)
(2) coordinating, node,对document进行路由，,将请求转发给对应的node (有primary shard,因为是增删改操作，请求只能路由到primary shard上去)
(3)实际的node.上的pri mary shard处理请求，然后将数据同步到replica node
(4) coordinating node, 如果发现primary node和所有replica node都搞定之后，就返回响应结果给客户端
</code></pre>
<h4 id="写一致性原理以及quorum机制的深入解析">写一致性原理以及quorum机制的深入解析</h4>
<p>(1)我们在发送任何一一个增删改操作的时候，比如说put /index/type/id?consistency=one, 都可以带上一个consistency参数，指明我们想要的写-致性是什么 ?put /index/ type/id?consistency-quorum</p>
<p>consistency= one (primary shard)或者a11 (a11 shard)或者quorum (default)</p>
<p>one:要求我们这个写操作，只要有一个primary shard量active活跃可用的、 就可以执行<br>
all: 要求我们作，必须所有的primary shard和replica shard都是活跃的，才可以执行这个写操作，<br>
quorum: 默认的值，要求所有的shard中， 必须是大部分的shard都是活跃的，可用的，才可以执行这个写操作</p>
<p>(2) quorum机制， 写之前必须确保大多数shard都可用，</p>
<p>quorum = int( (primary+number_of_replicas) / 2 ) + 1， 当number_of_replicas&gt;1 quorum机制才生效<br>
quorum= int( (primary_number_of_replicas) / 2 )1、<br>
举个例子，3个primary, shard, number_of_replicas=1, 总共有3 + 3 * 1 = 6个shard<br>
quorum= int( (3+1)/2)<br>
所以，要求6个shard中至少有3个shard是active状态的， 才可以执行这个写操作</p>
<p>(3)如果节点数少于quorum数量，可能导致quorum不齐全， 进而导致无法执行任何写操作<br>
3个primary, shard, rep1ica=1, 要求至少3个shard号active;<br>
3个shard按照之前学习的shard&amp;replica机制，必须在不同的节点上，如果说只有1台机器的话，是不是有可能出现说3个shard都没法分配齐全，<br>
此时就可能会出现写操作无法执行的情况</p>
<p>1个primary_ shard, replica=3, quorum=((1+3) / 2) + 1=3，要求1个primary shard + 3个rep1ica shard = 4个shard,<br>
其中必须有3个shard是要处于active状态的。如果这个时候只有2台机器的话，会出现什么情祝呢?<br>
es提供了一 种特殊的处理场景，就是说当number_of_replicas&gt; 1时才生效，因为假如说，你就一 个primary shard, replica=1, 此时就2个shard.<br>
(1+1/2)+1=2，要求必须有2个shard是活跃的，但是可能就1个node, 此时就1个shard是活跃的，如果你不特殊处理的话，导致我们的单节点集群就无法工作</p>
<p>(4) quorum不齐全时，wait， 默认1分钟，timeout, 100， 30s<br>
等待期间，期望活跃的shard数量可以增加，最后实在不行，就会timeout<br>
我们其实可以在写操作的时候，加一个timeout参数，<br>
比如说put /index/type/id?timeout-30， 这个就是说自己去设定quorum不齐全的时候， es的timeout时长， 可以缩短，也可以增长</p>
<h4 id="document内部查询原理">Document内部查询原理</h4>
<figure data-type="image" tabindex="2"><img src="images/document_query.png" alt="img.png" loading="lazy"></figure>
<pre><code>1、客户端发送请求到任意一个node, 成为coordinate. node, coordinate node对document进行路由 ，将请 求转发到对应的node,
2、此时会使用round-robin随机轮询算法，在primary shard以及其所有rep1ica中随机选择一个，让读请求负载
3、接收请求的node返回document给coordinate node
4、coordinate node返回
</code></pre>
<p>和写不一样的是，写是找primary shard，读的时候primary shard和replica shard</p>
<p>5、特殊情况: document如果还在建立索引过程中，可能只有primary shard有, 任何一个repl1ica shard都没有， 此时可能会导致无法读取到document, 但是document完成引建立z后，primary shard和replica shard就都有了</p>
<h4 id="builapi的奇特json格式与底层性能优化关系">BuilApi的奇特json格式与底层性能优化关系</h4>
<p>1、bulk中的每个操作都可能要转发到不同的node的shard去执行<br>
2、如果采用比较良好的json数组格式<br>
允许任意的换行，整个可读性非常棒，读起来很爽，es拿到那种标准格式的ison串以后，要按照下述流程去进行处理<br>
(1)将json数组解析为ISONArray对象，这个时候，整个数据，就会在内存中出现一份-模一样的拷贝，-份数据是jison文本，一份数据是JSONArray对象2)解析json落组里的每个对每个请求中的document进行路由<br>
为路由到同一个shardE的多个请求，创建一个请求数组(4)将这个<br>
(5)将序列化后的请求数组发送到对应的节点上去</p>
<p>3、耗费更多内存，更多的jvm gc开销<br>
我们之前提到过bulk size最佳大小的那个问题，--般建议说在几千条那样，然后大小在10MB左右，所以说，可怕的事情来了。假设说现在100个bulk请求发送到了一个节点上去，然后每个请求是10MB，100个请求，就是1000MB = 1GB， 然后每个请求的json都copy 份为jsonarray对象， 此时内存中的占用就会翻倍，就会占用2GB的内存， 甚至还不止。因为弄成jsonarrayZ后，还可能会多搞一些其他的数据结构，2GB+的内存占用。</p>
<p>占用更多的内存可能就会积压其他请求的内存使用量，比如说最重要的搜索请求，分析请求，等等，此时就可能会导致其他请求的性能急速下降，<br>
另外的话，占用内存更多，就会导致java虚拟机的垃圾回收次数更多，跟频繁，每次要回收的垃圾对象更多，耗费的时间更多，导致es的java虚拟机停止工作线程的时间更</p>
<p>现在的奇持格式<br>
(action:{meta&quot;]}\n&quot;data&quot;}}n<br>
&quot;action&quot;:(meta&quot;}n'data&quot;'}\r</p>
<p>(1)不用将其转换为json对象，不会出现内存中的相同数据的拷见，直接按照换行往ison<br>
(2)对每两个组的json，读取meta,进行document路由<br>
(3)直接将对应的json发送到node上去</p>
<p>5、最大的优势在于，不需要将ison数组解析为一个TSONArray对象， 形成一份大数据的拷贝，很费内存空间，尽可能地保证性的</p>
<h3 id="elasticsearch搜索引擎">Elasticsearch搜索引擎</h3>
<h4 id="search结果的深入解析">Search结果的深入解析</h4>
<pre><code>took:整个搜索请求花费了多少毫秒
hits. total:本次搜索返回了几条结果
hits. max_score:本次搜索的所有结果中，最大的相关度分数是多少，每一条document对于search的相关度，越相关，score分数越大， 排位越靠前
hits. hits:默认查询前10条数据，完整数据，score降序排序
shards; shards. fa1的条件(pri mary和replica全部挂掉)，不影响其他shard。默认情况下来说，一个搜索请求，会打到一个index的所有primary shard上去，当然了，每个primary shard都可能会有一个或多 个rep1ic shard, 所以请求也可以到primary shard的其中一个rep1ica shard上去。
timeout:默认无timeout, 1atency平衡icompleteness, 手动指定tineout，
timeout 查询执行机制:指定每个shard就在指定的timed out时间范围内，将搜索到的部分数据直接返回给client，而不是等到所有的数据都搜索出来了在返回
确保说一次搜索，可以在用户指定的time out时间内完成，为时间敏感的搜索应用提供良好的支持
</code></pre>
<h4 id="multi-indexmulti-type搜索模式解析以及搜索原理">Multi-index&amp;multi-type搜索模式解析以及搜索原理</h4>
<figure data-type="image" tabindex="3"><img src="images/Multi-index.png" alt="img.png" loading="lazy"></figure>
<h4 id="分页搜索以及deep-paging性能问题深度图解">分页搜索以及deep paging性能问题深度图解</h4>
<figure data-type="image" tabindex="4"><img src="images/deep_paging.png" alt="img.png" loading="lazy"></figure>
<p>deep paging:搜索特别深，总共6w数据，每个shard分了2w，每页10条数据，<br>
这个时候你要搜索到1000页，每个shard都要返回10010条数据，那么会返回30030条数据，排序，<br>
汇总之后，取第1000页的数据，会出现性能问题</p>
<h4 id="速掌握query-string-search语法以及_all-metadata原理揭秘">速掌握query string search语法以及_all metadata原理揭秘</h4>
<p>基础语法：<br>
GET /index/type/_search?q=filed:value<br>
_all metadata原理：<br>
es中的_all元数据，在建立索引的时候，我们插入一条document，它里面包含了多个field,此时es会将多个field值串联起来<br>
作为_all field的值，同时建立索引<br>
后面如果在搜索的时候，没有对某个filed指定搜索，就默认搜_all field的,其中是包含了所有field的值</p>
<h4 id="mapping">mapping</h4>
<p>mapping:自动或者手动对index建立数据结构和相关配置。mapping里包含了每个field对应的数据类型以及如何分词和搜索的行为。<br>
dynamic mapping:自动为我们建立index，type，以及对应的mapping。mapping里包含了每个field对应的数据类型以及如何分词。</p>
<pre><code>(1)往es里面直接插入数据:会自动建立索引。同时建立type以及对应的mapping
(2) mapping中就自动定义每个filed的数据类型
(3)不同的数据类型(正如说text和date)，可能有的是exact value, full text
(4)exact value在建立倒排索引的时候，分词的时候，是将整个值-起作-一个关键词建立到倒排索引中的: full text, 会经历各种各样的处理，分词，normaliztion (时态转换，同义词转换，天小弓转换)，才会建立到倒排索引中
(5)同时呢，exact value和fu11 text类型的filed就决定了，在一个搜索过来的时候，对exact value fie1d或者是full text. filed进行搜索的行为也是不一样的，会跟建立倒排索引的行为保持一致;比如说exact value搜索的时候，  直接按照整索引行为，包括分词器，等等
(6)可以用dynamic mapping让其自动建立mapping,包括自动设置数据类型，也可以手动index和type的mapping, 自己对各filed进存设置，包括数据类型，包括数据类型，包括分词等等
</code></pre>
<p>总结：mapping决定了field数据类型，倒排索引的行为，还有搜索的行为。</p>
<p>mapping数据类型：</p>
<pre><code>text，byte，short,integer,long float ,double ,boolean ,date
</code></pre>
<p>查询 mapping</p>
<pre><code>GET index/_mapping/type
</code></pre>
<p>只能创建index时手动指定mapping或者添加field mapping，不能修改field mapping</p>
<p>string默认是分词的，也可以手动指定分词行为<br>
analyzed：分词<br>
no_analyzed:不分词<br>
no：不分词不被搜索</p>
<p>创建索引指定filed mapping</p>
<figure data-type="image" tabindex="5"><img src="images/mapping.png" alt="img.png" loading="lazy"></figure>
<p>添加field mapping</p>
<figure data-type="image" tabindex="6"><img src="images/add_field_mapping.png" alt="img.png" loading="lazy"></figure>
<p>查看分词效果</p>
<figure data-type="image" tabindex="7"><img src="images/sehngchanzhexierushuju.png" alt="img.png" loading="lazy"></figure>
<p>es支持两种模式的搜索：<br>
full_text:全文检索<br>
exact_value:精确搜索<br>
不同的filed 有点可能是full_text 有的可能是exact_value<br>
query string search 会用跟倒排索引一样的分词器去进行分词</p>
<h4 id="倒排索引的核心原理快速解密">倒排索引的核心原理快速解密</h4>
<p>倒排索引最简单的建立过程</p>
<figure data-type="image" tabindex="8"><img src="images/daopaisuoyin.png" alt="img.png" loading="lazy"></figure>
<p>normalization：在建立倒排索引的时候，会执行一个操作，也就是说对拆分出来的各个单词进行处理<br>
以提升后面搜索的时候能够搜到相关联文档的概率</p>
<p>我们在搜索的时候，会把词进行拆分，把每一个词去倒排索引中去匹配。</p>
<h4 id="分词器">分词器</h4>
<p>切分词语，<br>
normalization (提升recal1召回率)</p>
<h4 id="querydsl">QueryDSL</h4>
<p>（2）、query DSL（Domain Specified Language 特定领域的语言） 基于Http request body请求体，可以用json格式构建语法，可以构建各种复杂的语法</p>
<pre><code>    例如：  
    {
        &quot;query&quot;:{ //查询
        “match_all”:{ //
        },
        &quot;filter&quot; :{ // 过滤
            &quot;range&quot;{
                &quot;price&quot; :{&quot;gt&quot;,&quot;&quot;}
            }
        }
    }
        &quot;sort&quot;:[ 排序
        {
        &quot;price&quot;:&quot;desc&quot;
        }
        ]
        &quot;from&quot;:1, 查询游标
        &quot;size&quot;:2 查询数量
        }
        &quot;_source&quot;:[&quot;&quot;,&quot;&quot;] :指定要查询出来的field
     }
</code></pre>
<p>bool:组合查询，其他的查询放在bool下<br>
must:必须匹配<br>
should：可以匹配也可以不匹配<br>
must_not:不要匹配<br>
match_all:全部查询<br>
match:全文检索，将搜素词拆分为一个个词之后去倒排索引中进行匹配<br>
match_phrase(短语搜索) :要求输入的搜索串，必须在指定字段文本中，完全一模一样的，才能算匹配<br>
term:不分词去倒排索引中匹配（比较少用，建立mapping的时候，可以指定那个field不分词）<br>
terms:不分词去倒排索引中匹配（比较少用，建立mapping的时候，可以指定那个field不分词）<br>
exists：搜索词不能为空<br>
sort:排序<br>
highlight:高亮<br>
from:查询游标<br>
size：查询数量<br>
_source:指定要查询出来的field</p>
<p>filter,仅仅只是按照搜索条件过滤出需要的数据而已，不计算任何相关度分数，对相关度没有任何影响<br>
query,会去计算每个document相对于搜索条件的相关度，并按照相关度进行排序<br>
-般来说，如果你是在进行搜索，需要将最匹配搜索条件的数据先返回，那么用query; 如果你只是要根据一些条件筛选出一部分数据，不关注其排序。那么用filter<br>
除非是你的这些搜索案件，你希望越符合这些搜索条件的document起排在前面返回，那么这些搜索条件放在query中;如果你不希望-一些搜索条 件来影响你的document排序，那么filter中即可<br>
3、fi1ter与 query性能<br>
fi1ter,不需要计算相关度分数:不需要按照相关度分数进行排序，同时还有内置的自动cache最常使用filter的功能<br>
query,相反，要计算相关度分数，按照分数进行排序，而且无法cache结果</p>
<p>不要bool，只要filter的话，</p>
<pre><code>{
&quot;query&quot;:{ //查询
  “constant_score”:{
    &quot;filter&quot; :{ // 过滤
    &quot;range&quot;{
    &quot;price&quot; :{&quot;gt&quot;,&quot;&quot;}
    }
   },

}
</code></pre>
<p>定位DSL语法不合法的原因</p>
<pre><code>GET /index/type/_validate/query?explain
{
    DSL语句
}
</code></pre>
<h4 id="对stringfield排序">对StringField排序</h4>
<p>如果对string file 排序，结果往往是不准确的，因为分词后是多个单词，再排序就不是我们想要的结果<br>
通常解决办法是，将一个string filed索引两次，一个分词用来搜索，另一个不分词，用来排序</p>
<p>方式一：</p>
<pre><code>PUT /index/type/
{
   &quot;mapping&quot;:{
      type:text, /第一次设置分词
      fields:{    //第二次设置不分词
        &quot;raw&quot;:{
            type:string,
            index:&quot;not_analyzed&quot;
        },
      &quot;fielddata&quot;:true  //设置正排索引，为了排序
     }
   }
}
</code></pre>
<h4 id="相关度评分tfidf算法">相关度评分TF&amp;IDF算法</h4>
<p>relevance score算法， 简单来说，就是计算出，-个索引中的文本，与搜索文本，他们之间的关联匹配程度</p>
<p>Elasticsearch 使用的是 term frequency/ inverse document frequency算法， 简称为TF /IDF算法</p>
<p>Term frequency: 搜索文本中的各个词条在fie1d文本中出现了多少次，出现次数越多，就越相关<br>
Inverse document frequency: 搜索文本中的各个词条在整个索引的所有文档中出现了多少次，出现的次数越多，就越不相关<br>
Field-1ength norm: fie1d长度， fie1d越长， 相关度越弱</p>
<p>查看_score分数<br>
GET /test_index/test_type/_search?explain<br>
{<br>
query”: {<br>
'term”: {<br>
&quot;test_filed&quot;:&quot;&quot;<br>
}<br>
}<br>
}</p>
<h4 id="docvalues正排索引">DocValues正排索引</h4>
<p>搜索的吋候，要依靠倒排索引;排序的吋候，需要依靠正排索引，看到毎个document的毎个field, 然后迸行排序，<br>
所渭的正排索引,其是就是doc values<br>
在建:立索引的吋候，-一方面会建立倒排素引，以供搜索用; 一方面会建立正排索引，也就是doc values, 以供排序，聚合,辻濾等操作使用</p>
<p>doc values是被保存在磁盘上的。</p>
<p>如果内存足够，os会自劫将其缓存在内存中，性能逐是会很高;如果内存不足够，os会将其写入磁盈上</p>
<h4 id="queryphase">QueryPhase</h4>
<figure data-type="image" tabindex="9"><img src="images/queryPhase.png" alt="img_1.png" loading="lazy"></figure>
<p>(1)搜索请求发送到某一个coordinate node, 构构建一个priority queue, 长度以paging操作from和size为准，默认为10<br>
(2) coordinate_node将请求转发到所有shard,每个shard本地搜索，并构建一个本地的priority queue<br>
(3)各个shard将自己的priority queue返回给coordinate node， 并构建一个全局的priority queue</p>
<h4 id="fetchphase">FetchPhase</h4>
<figure data-type="image" tabindex="10"><img src="images/FetchPhase.png" alt="img_1.png" loading="lazy"></figure>
<p>（1）coordinate node构建完priority queue之后，就发送mget请求去所有shard上获取对应的document<br>
（2）各个shard将document返回给coordinate node<br>
（3）coordinate node将合并后的document结果返回给client客户端</p>
<h4 id="搜索相关参数梳理以及bouncingresults问题的解决方案">搜索相关参数梳理以及bouncingresults问题的解决方案</h4>
<p>1、preference<br>
决定了哪些shard会被用来执行搜索操作<br>
<em>primary,</em> primary_ first,_ 1oca1，_ on1y_ <em>node:xyz，</em> prefer_ node:xyz，_ shards:2,3<br>
bguncing. results问题，两个document排序， fie1d值相同; 不同的shard上，可能排序不同;每次请求轮询打到不同的replica shard上; 每次页面上看到的搜索结果的排序都不一<br>
样。这就是bouncing resu1t， 也就是跳跃的结果。<br>
搜索的时候，是轮询将搜索请求发送到每一-个replica shard (primary shard)，但是在不同的shard上，可能document的排序不同<br>
解决方案就是将preference设置为一个字符串，比如说user_ id, 让每个user每次搜索的时候，都使用同- -个replica shard去执行， 就不会看到bouncing results了<br>
2、timeout, 已经讲解过原理了，主要就是限定在一定时间内，将部分获取到的数据直接返回，避免查询耗时过张<br>
3、 routing, document文档路由， id路由，routing=user id, 这样的话可以让同一个user对应的数据到一个shard上去<br>
4、search_ type<br>
default: query_ <em>then</em> <em>fetch,<br>
dfs</em> query_ <em>then</em> fetch,可以提升revel ance sort精准度</p>
<h4 id="scoll技术滚动搜索大量数据">Scoll技术滚动搜索大量数据</h4>
<p>如果一次性要查出来比如10万条数据，那么性能会很差，此时一般会采取用sco11滚动查询，一批一批的查，直到所有数据都查询完处理完<br>
使用sco11滚动搜索，可以先搜索一批数据，然后下次再搜索一批数据，以此类推，直到搜索出全部的数据来，</p>
<p>scoll搜索会在第一次搜索的时候，保存一一个当时的视图快照，之后只会基于该旧的视图快照提供数据搜索，如果这个期间数据变更,是不会让用户看到的<br>
采用基于_doc进行排序的方式，性能较高<br>
每次发送scroll请求，我们还需要指定一个sco11参数， 指定一个时间窗口， 每次搜索请求只要在这个时间窗口内能完成就可以了<br>
GET /index/_search?scroll=1m<br>
&quot;query&quot;: {<br>
&quot;metch_a11&quot;: {},<br>
&quot;sort&quot;:[&quot;_doc&quot;],<br>
&quot;size&quot;: 1000<br>
」<br>
获得的结果会有一个scrollid, 下一次再发送scroll请求的时候，必须带上这个scrollid .<br>
GET /_search/scro11<br>
&quot;scroll&quot; :<br>
” scroll<br>
“cXV1 cn1UaGVuRmV0Y2g7NTsxMDk5NDpkUmpiR2F j0F NhNn1CM 1ZDMWpWYnRR0zEw0Tk 10mRSamJHYWM 4U2E2eUI zVkl{xa1Zi dFE7MTA50TM6ZF JqYkdhYzhTYTZ5Q jNWQzF qVmJOUTsxMTE5MDpBVUtwN21 x<br>
c1FLZV8yRGVjW1I2QUVB0zEw0Tk20mRSamJHYWM4U2E2eUI zVkMxa1Zi dFE7MDs='<br>
size会发送给每个shard, 因此每次最多会返回size * primary shard条数据</p>
<p>scoll，看起来挺像分页的，但是其实使用场景不-样。<br>
分页主要是用来一页一页搜索,给用户看的;<br>
sco11主要是用来一批一批检索数据，让系统进行处理的</p>
<h3 id="elasticsearch索引管理">Elasticsearch索引管理</h3>
<h4 id="索引的增删改">索引的增删改</h4>
<h4 id="修改分词起以及定制分词器">修改分词起以及定制分词器</h4>
<h4 id="深入探索type底层数据结构">深入探索type底层数据结构</h4>
<h4 id="mappingrootobject深入解析">MappingRootObject深入解析</h4>
<h4 id="定制化自己的dynamicmapping策略">定制化自己的dynamicMapping策略</h4>
<h2 id="elasticsearch高手进阶篇">Elasticsearch高手进阶篇</h2>
<h3 id="深度揭秘搜索技术">深度揭秘搜索技术</h3>
<p>TermFilter</p>
<p>GET /index/_search<br>
{<br>
&quot;query&quot; : {<br>
constant_score:{<br>
filter:{bool:{<br>
}<br>
}<br>
}<br>
}<br>
}</p>
<pre><code>es新版本内置的建立对于text类型的filed，会建立两次索引，一个是分词的，另一个是不分词的，不分词的是基于field.keyword,最多保留256个字符直接一个字符串放入到倒排索引中。
teamquery：根据exact value进行搜索，数字 boolean 日期有天然支持
text类型的filed需要建立索引是指定not_analyzed,才能用terms
新版本中设置field为keyword和not_analyzed一样就是不分词
term 匹配一个值
terms 匹配多个值
</code></pre>
<p>全文检索多字段搜索</p>
<pre><code>GET /index/_search
{
    bool:{
        &quot;should&quot;:{
            term:{field:value},
            term:{field:value},
        }
    }
}

and match 转term+must

GET /index/_search
{
  bool:{
    &quot;must&quot;:{
       query:'value',
        operator:and
    }
   }
}
等价于
GET /index/_search
{
    bool:{
        &quot;must&quot;:{
            term:{field:value},
            term:{field:value},
        }
    }
}
</code></pre>
<figure data-type="image" tabindex="11"><img src="images/duoziduansousuo.png" alt="img.png" loading="lazy"></figure>
<p>手动控制全文检索的精准度</p>
<figure data-type="image" tabindex="12"><img src="images/convert.png" alt="img.png" loading="lazy"></figure>
<p>boost的细粒度搜索条件控制</p>
<figure data-type="image" tabindex="13"><img src="images/boost.png" alt="img.png" loading="lazy"></figure>
<p>多shard场景下 relevance score 不准确问题大揭秘</p>
<pre><code>如何解决该问题？    
生产环境下，数据量大尽可能的实现均匀分配
测试环境下，将所有的primary设置为1
测试环境下搜索附带search_type = dfs_query_then_query参数，会将local IDF取出来计算global IDF
</code></pre>
<p>基于dis_max实现best_field策略进行多字段搜索</p>
<pre><code>dix_max取某一个query最大的分数
GET /index/_search
{
    &quot;query&quot;:{
    &quot;dis_max&quot;:{
        &quot;queries&quot;[
        {match:&quot;title&quot;:&quot;java solution&quot;},
        {match:&quot;content&quot;:&quot;java solution&quot;}
     ]
    }  ,
    tie_ breaker:0.3
}

}
</code></pre>
<figure data-type="image" tabindex="14"><img src="images/best_field.png" alt="img.png" loading="lazy"></figure>
<p>基于tie_ breaker参数优化dis_ max搜索效果</p>
<pre><code>使用tie_ breaker参数的意义在于将其他query分数乘以tie_ breaker 综合与
最高分数的那个query，综合一起计算，除了最高分外将其他query的分数也考虑进去

GET /index/_search
{
    &quot;query&quot;:{
    &quot;dis_max&quot;:{
      &quot;queries&quot;[
        {match:&quot;title&quot;:&quot;java solution&quot;},
        {match:&quot;content&quot;:&quot;java solution&quot;}
     ]
    }  ,
    tie_ breaker:0.3
}

}
</code></pre>
<p>实战基于multi match语法实现dis_max+tie_breaker<br>
best_fields 策略</p>
<pre><code>GET / index/_search
    {
    &quot;query&quot;:{
      &quot;multi_match&quot;:{
         &quot;query&quot;:&quot;aaa&quot;,
         &quot;fileds&quot;:[&quot;field1&quot;,&quot;field2&quot;],
         type:&quot;best_fields&quot;,
         &quot;tie_breaker&quot;:0.3,
        &quot;minimum_should_match&quot;:50%
     }
}
</code></pre>
<p>等价于</p>
<pre><code>GET /index/_search
{
    &quot;query&quot;:{
    &quot;dis_max&quot;:{
        &quot;queries&quot;[
        {match:&quot;title&quot;:&quot;java solution&quot;
        &quot;minimum_should_match&quot;:50%
        &quot;boost&quot;:2},
        {match:&quot;content&quot;:&quot;java solution&quot;}
    ]  ,
    }  ,
    tie_ breaker:0.3
}

}
minimum_should_match:去长尾，控制搜索的精准度，只要匹配到一定数量的关键数据才能返回
</code></pre>
<p>基于multi_ match+most field策略进行multi-field搜索</p>
<pre><code>GET / index/_search
    {
    &quot;query&quot;:{
      &quot;multi_match&quot;:{
        &quot;query&quot;:&quot;&quot;,
          filed:[&quot;field1&quot;,&quot;field2&quot;],
         type:&quot;most_field&quot;,
         &quot;tie_breaker&quot;:0.3,
        &quot;minimum_should_match&quot;:50%
     }
}
</code></pre>
<p>与best_fields的区别</p>
<pre><code>1.best_fields是对多个field进行搜索，搜索挑选某个field匹配度最高的那个分数，同时在多个query最高分相同的情况下，在一定
程度上考虑其他query的分数。简单来说，你对多个field进行搜索，就想搜索到某一个field尽可能的包含更多关键字的数据
优点：通过best_fields策略，以及综合考虑其他field，还有minimum_should_match，可以尽可能精准地将匹配结果推送到最前面
缺点：除了那些精准匹配的结果，其他差不多大的结果，排序结果不太均匀
实际的例子，百度之类的搜索引擎，最匹配的在前面，但是其他的就没有什么区分度了
2.most_fields,综合多个field一起进行搜索，尽可能多的让所有field的query参与到总分计算中来，此时就会是个大杂烩结果不一定精准，
某一个document的一个field包含了多个关键字，但是因为有其他document有更多的field匹配到了，所以排在了前面；所以需要建立sub_title.std这样的field，
尽可能的让某一个field匹配到query string，贡献更高的分数，将更精准的结果拍到前面
优点：尽可能的匹配更多的field的结果推送到前面整个结果是比较均匀分布的
缺点：可能那些精准匹配的结果无法推送到到前面
实际的例子wiki，明显就是most_fields策略，搜索的结果比较均匀，但是翻好几页才能找到最匹配的结果
</code></pre>
<p>使用most_ fields策 略进行cross-fields search弊端大揭秘</p>
<p>使用copy_ to 定制组合field解决cross- -fields搜索弊端</p>
<p>使用原生cross-field技术解决搜索弊端</p>
<p>phrase matching搜索技术</p>
<pre><code>match query 做全文检索时，只能搜索包含这些次的document，
如果需要这些词里得很近的document，那就要给他一个更高的relevance score 这里
就涉及到proximity match 近似匹配
 
GET /index/_search
{
    &quot;query&quot;:{
    match_phrase:{
        &quot;title&quot;:{
        &quot;query&quot;:&quot;java spark&quot;,
        &quot;slop&quot;:1
    }
  }
 }
}
slop:query string搜索文本中的几个term，要经过多少次移动才能与一个document匹配 
其实加了phrase match 就是 proximity match 近似匹配
</code></pre>
<p>混合使用match和近似匹配实现召回率与精准度的平衡</p>
<pre><code>召回率（recall）： 比如说你搜索一个java spark 总共有100个doc，能返回多少个结果作为doc，就是召回率。
精准度（precision）:比如你搜索一个java spark ,能不能尽可能的让包含java spark，或者java和spark离得很近的排在前面
近似匹配的时候，召回率比较低，精准度太高了
但是有时候可能我们希望的是匹配到几个term中的部分，就可以作为结果出来，这样可以提高召回率。同时我们也希望用上match_phrase根据距离提高分数的功能，
让几个term距离越近的分数越高，优先返回
就是优先返回召回率同时兼顾精准度
   GET /index/_search
{
    &quot;query&quot;:{
      bool:{
         must:{
            &quot;match&quot;:{
              &quot;field&quot;:{
                &quot;query&quot;:&quot; vaule&quot;,
                &quot;minimum_should_match&quot; :&quot;50%&quot;
          },
          should:{
            &quot;match_phrase&quot;:{
                &quot;field&quot;:{
                   &quot;query&quot;:&quot;value&quot;
                    &quot;slop&quot;:&quot;50&quot;
                }
            }
        }
      }
    }
  }
 }
}
</code></pre>
<p>使用rescore机制优化近似匹配搜索的性能</p>
<pre><code>match 和 match_phrase区别
    match：只要简单的匹配到了一个term，就可以将doc作为结果返回
    match_phrase：首先扫描到所有的term的doc list；找到包含所有的
    term 的doc list；然后对每个doc都计算每个term的position，是否符合指定范围
    slop，需要进行复杂的运算，来判断是否通过slop，
match query的性能要比match_phrase和 proximity match(有slop) 近似匹配要高很多，
因为后两者豆芽计算position 的距离。match query 比match_phrase性能搞10被，比proximity match 高20倍
但是别担心，因为es的性能都是毫秒级别的，match query一般就在几毫秒或者几十毫秒，所以是可以接受的

优化proximity match的性能一般就是减少要进行proximity match搜索的documeng 的数量
主要思路就是match query 先过滤出需要的数据，然后再用proximity match来根据term距离来提高doc分数
rescore：重打分
</code></pre>
<pre><code class="language-java">   GET/index/_search
        {
        &quot;query&quot;:{
                    &quot;match&quot;:{
                     &quot;field&quot;:&quot; vaule&quot;,
                     
               }
          },
        rescore:{
            &quot;window_size&quot;:50,
             rescore_query:{
                &quot;match_phrase&quot;:{
                &quot;field&quot;:{
                &quot;query&quot;:&quot;value&quot;
                &quot;slop&quot;:&quot;50&quot;
                }
            }
           }
        }
</code></pre>
<figure data-type="image" tabindex="15"><img src="images/rescoring.png" alt="img.png" loading="lazy"></figure>
<figure data-type="image" tabindex="16"><img src="images/chongdafen.png" alt="img.png" loading="lazy"></figure>
<p>实战前缀搜索、通配符搜索、正则搜索等技术</p>
<pre><code>前缀搜索

    GET /index/_search/
    {
        &quot;query&quot;:{
         &quot;prefix&quot;:{
            &quot;field&quot;:&quot;value&quot;
        }
      }
    }
prefix query 不计算relevance score 与prefix filter 唯一区别就是
filter 会cache bitset
前缀越短，要处理的doc越多，性能越差，尽可能的用长前缀搜索
 
通配符搜索：
跟前缀搜索类似，功能更加强大

GET /index/_search/
{
&quot;query&quot;:{
  &quot;wildcard&quot;:{
      &quot;field&quot;:&quot;value&quot;
      }
   }
}

 正则搜索
   GET /index/_search/
 {
    &quot;query&quot;:{
      &quot;regexp&quot;:{
          &quot;field&quot;:&quot;value&quot;
          }
       }
  }

wildcard 和regexp和prefix原理一致，都会扫描整个索引，性能很差
</code></pre>
<p>实战match_phrase_prefix实现search-time搜索推荐（少用）</p>
<pre><code>match_phrase_prefix原理跟match_phrase类似，唯一的区别就是把最后一个term超过这个数量的就不需要匹配了，限定性能
也可以指定slop，但是最后一个term会最为前缀
 max_expansions:指定prefix最多匹配多个term，超过这个数量就不再匹配了，限定性能
   GET /index/_search/
 {
    &quot;query&quot;:{
      &quot;match_phrase_prefix&quot;:{
          &quot;field&quot;:&quot;{
                &quot;query&quot;:&quot;value1 value2&quot;,
                &quot;slop&quot;:&quot;10&quot;,
                &quot;max_expansions&quot;:50,
             }
          }
       }
  }
默认情况下，前缀要扫描所有的倒排索引中的term，去找这个词打头的，但是这样性能太差，可以用max_expansions限定，最对匹配多少个。就停止搜索了。
</code></pre>
<p>实战通过ngram分词机制实现index-time搜索推荐</p>
<figure data-type="image" tabindex="17"><img src="images/ngram.png" alt="img.png" loading="lazy"></figure>
<p>深入揭秘TF&amp;IDF算法以及向量空间模型算法</p>
<p>深入揭秘lucene的相关度分数算法</p>
<p>实战掌握四种常见的相关度分数优化方法</p>
<p>实战用function_ score自定 义相关度分数算法</p>
<p>实战掌握误拼写时的fuzzy模糊搜索技术<br>
<img src="images/fuzzy.png" alt="img.png" loading="lazy"></p>
<h3 id="ik中文分词器">IK中文分词器</h3>
<h3 id="深入聚合数据分析">深入聚合数据分析</h3>
<p>bucket与metric核心概念</p>
<pre><code>bucket:对数据进行分组，每一组就是一个bucket
metric:对一个数据组进行统计，就是对一个bucket执行某种聚合分析操作，比如说求最大值，求最小值
select count(*) from table group by id 
bucket:group by id --&gt; 那些id相同的数据，就会被划分到一个bucket中
metric:count(*),对每个id 对应饿bucket中的所有数据，计算一个数量
</code></pre>
<p>聚合分组最基本语法</p>
<pre><code>GET /index/_search/
{
   size:0,
   &quot;aggs&quot;:{
      &quot;group_name&quot;: {
          &quot;terms&quot;:{
                &quot;field&quot;:&quot;value&quot;
            }
        }
    }
}
size：只获取聚合结果，而不要执行聚合的原始数据
aggs: 固定语法，要对一份数据执行分组聚合操作
group_name：就是对每个aggs，都要起一个名字，这个名字是自定义的，你取什么都OK 
terms:根据字段的值进行分组
filed:根据指定的字段的值进行分组
</code></pre>
<p>聚合查询结果分析：</p>
<pre><code>hits.hits:我们指定了size是0，所以hits.hits就是空的，否则会把执行聚合的那些原始数据给你返回回来
aggregations:聚合结果
group_name：我们执行的聚合的名称
buckets:我们执行的field划分出的buckets
doc_count:这个bucket分组内有多少数据
默认的排序规则:按照doc_count倒叙排序
</code></pre>
<p>聚合分组后，执行每组的metric聚合操作：</p>
<pre><code>GET /index/_search/
{
    size:0,
   &quot;aggs&quot;:{
     &quot;group_name&quot;: {
     &quot;terms&quot;:{
     &quot;field&quot;:&quot;value&quot;
      },
     &quot;aggs&quot;:{
        &quot;group_name_1&quot;:
            &quot;ave&quot;:{
                &quot;field&quot;:&quot;value&quot;
            }
        }
    }
  }
}
doc_count: 其实知识es的bucket操作默认执行的一个内置的metric
对每组bucket执行metric聚合统计操作
在一个aggs执行的bucket操作（terms），平级的json结构下，再加一个aggs，
这个aggs内部同样去个名字，执行metric操作avg（max，min），对之前的每个bucket中的数据的执行field，求一个平均值

select avg(field) from tabel group by filed
</code></pre>
<p>bucket嵌套实现多层下钻分析：</p>
<pre><code>下钻的意思是：已经分了一个组，比如说颜色的分组，然后还要对这个分组内的数据在分组，比如说一个颜色内有多个不同品牌的组
最后对每个最小粒度的分组进行聚合分析操作，这就叫做下钻分析。 

es实现下钻分析，就是对bucket进行多层嵌套，多次分组
     GET /index/_search/
{
    size:0,
   &quot;aggs&quot;:{
     &quot;group_name&quot;: {
         &quot;terms&quot;:{
         &quot;field&quot;:&quot;value&quot;
          },
         &quot;aggs&quot;:{
            &quot;group_name_1&quot;:
                &quot;avg&quot;:{
                    &quot;field&quot;:&quot;value&quot;
                },
                   &quot;max&quot;:{
                    &quot;field&quot;:&quot;value&quot;
                },
                   &quot;min&quot;:{
                    &quot;field&quot;:&quot;value&quot;
                },
            },
         &quot;aggs&quot;:{
            &quot;group_name_1&quot;:
                &quot;terms&quot;:{
                    &quot;field&quot;:&quot;value&quot;
                },
                 &quot;aggs&quot;:{
                   &quot;group_name_1&quot;:
                    &quot;ave&quot;:{
                       &quot;field&quot;:&quot;value&quot;
                }
            },
            },
    }
  }
}
avg:计算组内平均值
max:计算组内最大值
min:计算组内最小值
sum:计算组内平均值
</code></pre>
<p>histogram区间统计</p>
<pre><code>histogram，类似terms，也是进行bucket分组操作，按照这个field的值的各个范围区间，进行bucket分组操作
按照数字区间：
&quot;histogram&quot;:{
         &quot;field&quot;:&quot;value&quot;
         &quot;interval&quot;:2000
         },
interval:2000 划分范围，比如0-2000，2000-4000 bucket
 去根据field的值看落在哪个区间，就会将这条数据放在哪个bucket中
按照日期区间；
    &quot;histogram&quot;:{
         &quot;field&quot;:&quot;value&quot;
         &quot;interval&quot;:&quot;month&quot;
         &quot;format&quot;:&quot;yyyy-MM-dd&quot;
        &quot;min_doc_count&quot;:0
        &quot;extended_bounds&quot;:{
            &quot;min&quot;:&quot;2017-01-10&quot;,
            &quot;max&quot;:&quot;2017-11-10&quot;,
        }
     },
    min_doc_count:即使某个日期interval一条数据也没有，那么这个区间也是要返回的，不然默认会过滤掉这个区间
    extended_bounds：划分bucket会限定起止日期
    extended_bounds.min：开始日期
    extended_bounds.max：截止日期
</code></pre>
<p>搜索+聚合</p>
<pre><code>es aggregation scope 任何的聚合都必须在搜索的结果中执行，搜索结果就是分析的scope

        GET /index/_search/
{
    query：{},
    size:0,
   &quot;aggs&quot;:{
     &quot;group_name&quot;: {
         &quot;terms&quot;:{
         &quot;field&quot;:&quot;value&quot;
          },
         &quot;aggs&quot;:{
            &quot;group_name_1&quot;:
                &quot;avg&quot;:{
                    &quot;field&quot;:&quot;value&quot;
                },
                   &quot;max&quot;:{
                    &quot;field&quot;:&quot;value&quot;
                },
                   &quot;min&quot;:{
                    &quot;field&quot;:&quot;value&quot;
                },
            },
         &quot;aggs&quot;:{
            &quot;group_name_1&quot;:
                &quot;terms&quot;:{
                    &quot;field&quot;:&quot;value&quot;
                },
                 &quot;aggs&quot;:{
                   &quot;group_name_1&quot;:
                    &quot;ave&quot;:{
                       &quot;field&quot;:&quot;value&quot;
                }
            },
            },
    }
  }
}
</code></pre>
<p>排序</p>
<pre><code>es聚合排序默认是按照每组的doc_count降序来排的
如果按照指定字段来排序
GET /index/_search/
{
    &quot;size&quot;:&quot;0&quot;,
    &quot;aggs&quot;:{
        &quot;group_name&quot;:{
            &quot;terms&quot;:{
              &quot;field&quot;:&quot;value&quot;,
              &quot;order&quot;:{
                &quot;group_name_1&quot;:&quot;desc&quot;
            },
           &quot;aggs&quot;:{
                &quot;group_name_1&quot;:{
                 &quot;avg&quot;:{&quot;field&quot;:&quot;value&quot;}
                }

            }
        }

    }

    }
}
</code></pre>
<p>并行聚合算法、三角选择原则、近似聚合算法</p>
<pre><code>有些聚合算法，是很容易可以并行的，比如说max
有些聚合分析的算法是不好并行的，比如说count(distinct), 并不是说在每个node上，
直接就出一些distinct value，就可以了，因为有些数据可能会很多近似估计后的结构，
不完全准确，但是速度会很快，一一般是完全精准的算法的性能的十倍

三角选择原则：精准+实时+大数据
1.精准+实时：没有大数据，数据量很小，那么一般就是单机跑，随便你怎么玩就可以
2.精准+大数据：hadoop，批处理，非实时，可以处理海量数据，保证精准，可能会跑几个小时
3.大数据+实时：es 不精准，近似估计，可能会有百分之几的错误率

近似聚合算法
如果采用近似估计的算法，延时在100ms左右，0.5%错误
如果采用100%精准的算法：延时一般在5s到几十秒 几个小时，0%错误率
</code></pre>
<p>去重</p>
<pre><code>es去重，cardinality metric，对每个bucket中指定的filed去重，取去重后的count,类似count(distinct)
 GET /index/_search/
{
    &quot;size&quot;:&quot;0&quot;,
    &quot;aggs&quot;:{
        &quot;group_name&quot;:{
            &quot;terms&quot;:{
              &quot;field&quot;:&quot;value&quot;,
              &quot;order&quot;:{
                &quot;group_name_1&quot;:&quot;desc&quot;
            },
           &quot;aggs&quot;:{
                &quot;distinct_group_name&quot;:{
                 &quot;cardinality&quot;:{&quot;field&quot;:&quot;value&quot;,&quot;precision_threshold&quot;:&quot;100&quot;}
                }

            }
        }

    }

    }
}
precision_threshold: precision_threshold* 8内存的消耗
占用内存很小而且你的unique value如果在值以内，那么可以确保100%精准
precision_threshold 设置的值越大，占用内存越大，可以确保更多unique value的100%准确
</code></pre>
<p>HyperLog++（HLL）算法性能优化(一般)</p>
<pre><code>cardinality底层是HyperLog算法
会对所有的unique value取hash值，通过hash值近似的去求distinct count，
默认情况下 ，发送一个cardinality请求的时候，会动态的对所有的filed value2
取hash值，前移到建立索引的时候
 
put /index/_mapping
 {
    &quot;mapping&quot;:{
        properties:{
         &quot;type&quot;:&quot;text&quot;,
        &quot;fileds&quot;:{
            &quot;hash&quot;:{&quot;type&quot;:&quot;murmur3&quot;}
        }
    }
    }
}

查询时
    GET /index/_search/
{
    &quot;size&quot;:&quot;0&quot;,
    &quot;aggs&quot;:{
        &quot;group_name&quot;:{
            &quot;cardinality&quot;:{
              &quot;field&quot;:&quot;type.hash&quot;,
              &quot;order&quot;:{
                &quot;group_name_1&quot;:&quot;desc&quot;
            }
        }
    }
}
</code></pre>
<p>percentiles百分比算法</p>
<pre><code>GET /index/_search/
{
    &quot;size&quot;:&quot;0&quot;,
    &quot;aggs&quot;:{
        &quot;group_name&quot;:{
            &quot;percentiles&quot;:{
              &quot;field&quot;:&quot;latency&quot;,
              &quot;percents&quot;:{
                50，95，99
            }
        }
    }，
    &quot;latency_ave&quot;:{
    &quot;avg&quot;:{field:&quot;value&quot;}
    }
}


  GET /index/_search/
{
    &quot;size&quot;:&quot;0&quot;,
    &quot;aggs&quot;:{
        &quot;group_name&quot;:{
            &quot;percentiles&quot;:{
              &quot;field&quot;:&quot;latency&quot;,
              &quot;percents&quot;:{
                50，95，99
            }
        },
    &quot;aggs&quot;:{
        &quot;group_name_1&quot;:{
            &quot;percentile_ranks&quot;:{
              &quot;field&quot;:&quot;latency&quot;,
              &quot;percents&quot;:{
                50，95，99
            }
        },

    }，
    
}
</code></pre>
<p>基于doc value正排索引聚合分析内部原理</p>
<pre><code>核心原理
    与倒排索引类似，正排索引也会写入磁盘文件中，然后呢os cache先进行缓存，以提升 doc value正排索引的性能
    如果 os cache 内存大小不足够放下整个正排索引，就会将doc value数据写入到磁盘文件中

性能问题：
    es官网建议，es大量是基于os cache来进行缓存和提升性能的，不建议用JVM内存来进行缓存
    那样会导致GC和oom的问题
    一般来说给jvm更少的内存，给os cache更大的内存
    os cache可以提升doc value和倒排索引的缓存和查询效率

column压缩：
  1.所有的值相同，直接保留单值
  2.....
  3.
disable doc value
    如果的确不需要用到doc value，比如聚合，排序等操作，那么可以禁用，减少磁盘占用
    put /index/
    {
        &quot;mapping&quot;
            &quot;properties&quot;:{
               &quot;field&quot;:&quot;keyword&quot;,
                &quot;doc_values&quot;:false
            }
    }
doc_values:false 禁用倒排索引
</code></pre>
<p>string field 和fielddata原理</p>
<pre><code>对分词的field直接执行聚合操作，会报错，
大概的意思是说，你必须要打开fielddata，然后将正排索引的数据加入到缓存中，才可以对分词的field进行聚合操作
会消耗很大的内存

对不分词的field执行聚合操作，直接可以执行，不需要设置fielddata ：true

分词field + fielddata的工作原理
    如果你的某个fie1d不分词，那么在建立索引的时候，会自动生成doc value（正排索引），针对这些不分词的fie1d 执行聚合操作，直接就可以执行
    分词的fie1d，是没有doc value的，所以必须打开是使用fielddata，那么必须将fielddata = true ，那么es在进行聚合操作的时候，会现场对
    field建立一份正排索引，完全存于内存中，结构和doc value类似，如果是ngram或者是大量的term，那么必将
    占用大量的内存，导致性能很差

fielddata加载是lazy加载的，对一个analyzed field执行聚合时，才会加载，而且是field-level
一个index的field所有的doc都会被加载，而不是少数doc
不是index-time创建，是query-time创建

fielddata内存限制
    indices.fielddata.cache.size:20%,超出限制清楚内存已有的fielddata数据
    默认设置无限制，限制内存的使用，但是会导致频繁evict和reload，大量的IO性能损耗，以及内存碎片和gc 

监控fielddata内存使用
     GET /_stats/fielddata?fileds=*
     GET /_nodes/_stats/indices/fielddata?fileds=*
     GET /_nodes/_stats/indices/fielddata?level=indices&amp;fileds=*

Circuit breaker 
        如果一次query load的fielddata超过总内存，就会oom 
indices.breaker.fielddata.limit: fielddata内存限制默认是60%
indices.breaker.request.limit: 执行聚合操作的内存限制，默认40%
indices.breaker.total.limit: 综合上面两个，限制在70%以内

fielddata filter细粒度内存加载控制
    POST /index/_mapping
    {
        &quot;field&quot;:{
            &quot;type&quot;: &quot;text&quot;,
            &quot;fielddata&quot;：{
                &quot;filter&quot;: {
                &quot;frequency&quot;:{
                    &quot;min&quot;:&quot;0.01&quot;,
                    &quot;min_segment_size&quot;:500
                    }
                }
            }
        }
    }
   min:仅仅加载至少在1%的doc中出现过的term对于的fielddata
   min_segment_size：少于500doc的segment 不加载fielddata
   这两个参数比较底层，一般不设置

fielddata预加载机制，以及序号标记预加载
    如果真的要对某个分词的field进行聚合，那么在query-time的时候现场生产fielddata并加载到内存，速度可能比较慢，此时我们可以fielddata预加载
     POST /index/_mapping
    {
       &quot;properties&quot;:{
        &quot;field&quot;:{
            &quot;type&quot;: &quot;text&quot;,
            &quot;fielddata&quot;：{
                &quot;loading&quot;：&quot;eager&quot;
            }
        }
     }
    }


  POST /index/_mapping
    {
       &quot;properties&quot;:{
        &quot;field&quot;:{
            &quot;type&quot;: &quot;text&quot;,
            &quot;fielddata&quot;：{
                &quot;loading&quot;：&quot;eager_global_ordinals&quot;
            }
        }
     }
    }
</code></pre>
<p>海量bucket优化机制：从深度优化到广度优化</p>
<figure data-type="image" tabindex="18"><img src="images/sdyx.png" alt="img.png" loading="lazy"></figure>
<h3 id="数据建模实战">数据建模实战</h3>
<h3 id="完成建议">完成建议</h3>
<pre><code>基于complete suggest实现搜索提示

比如说我们在百度搜索 '大话西游' 百度会自动给你提示 ，
'大话西游电影'，'大话西游小说'，'大话西游手游'
不用你把所有你想要的文本都输入完，搜索引擎会自动提示你可能想要搜索的那个文本
PUT index
   {
    &quot;settings&quot;: {
        &quot;number_of_shards&quot;: 3,
        &quot;number_of_replicas&quot;: 2
    },
 
    &quot;mappings&quot;: {
        &quot;properties&quot; : {
        &quot;suggest&quot; : {
            &quot;type&quot; : &quot;completion&quot;
        },
          &quot;id&quot;: {
           &quot;type&quot;: &quot;integer&quot;
        }
    }
  }
}

或者

PUT index
   {
    &quot;settings&quot;: {
        &quot;number_of_shards&quot;: 3,
        &quot;number_of_replicas&quot;: 2
    },
 
    &quot;mappings&quot;: {
        &quot;properties&quot; : {
        &quot;title&quot; : {
             &quot;type&quot; : &quot;text&quot;,
            &quot;analyzed&quot;:&quot;ik_max_word&quot;
             &quot;fields&quot;: {
                    &quot;suggest&quot;: {
                        &quot;type&quot;: &quot;completion&quot;,
                        &quot;analyzer&quot;:&quot;ik_max_word&quot;
                    }
                }
        },
          &quot;id&quot;: {
           &quot;type&quot;: &quot;integer&quot;
        }
    }
  }
}
completion,es实现的时候，是非常高性能的，会建立不是倒排索引，也不是正排索引。
就是纯基于前缀搜索的一种特殊数据结构，而且会放在内存中，所以auto completion进行
前缀搜索提示性能是非常高的。


GET /index/_search/
{
  &quot;suggest&quot;:{
    &quot;my-suggest&quot;:{
        &quot;prefix&quot;:&quot;大话西游&quot;，
        &quot;completion&quot;:{
            &quot;field&quot;:&quot;suggest&quot;
        }

        }
    }

或者

GET /index/_search/
{
  &quot;suggest&quot;:{
    &quot;my-suggest&quot;:{
        &quot;prefix&quot;:&quot;大话西游&quot;，
        &quot;completion&quot;:{
            &quot;field&quot;:&quot;title.suggest&quot;
        }
      }
    }
}
</code></pre>
<h3 id="生产实践集群">生产实践集群</h3>
<h3 id="elasticsearch性能调优">Elasticsearch性能调优</h3>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[redis]]></title>
        <id>https://zuolinlin.github.io/zuo.github.io/post/redis/</id>
        <link href="https://zuolinlin.github.io/zuo.github.io/post/redis/">
        </link>
        <updated>2022-04-08T13:20:28.000Z</updated>
        <content type="html"><![CDATA[<h1 id="redis">redis</h1>
<h2 id="目录">目录</h2>
<ul>
<li>
<p><a href="#redis%E6%9E%B6%E6%9E%84">redis架构</a></p>
<ul>
<li><a href="#Redis%E6%8C%81%E4%B9%85%E5%8C%96">redis持久化</a>
<ul>
<li><a href="#RDB%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6">RDB持久化机制</a></li>
<li><a href="#AOF%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6">AOF持久化机制</a></li>
</ul>
</li>
<li><a href="#Redis%E4%B8%BB%E4%BB%8E">redis主从</a></li>
<li><a href="#Redis%E5%93%A8%E5%85%B5">redis哨兵</a></li>
<li><a href="#Redis%E9%9B%86%E7%BE%A4">redis集群</a></li>
</ul>
</li>
<li>
<p><a href="#Redis%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7">redis双写一致性</a></p>
</li>
<li>
<p><a href="#Redis%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E3%80%81%E7%A9%BF%E9%80%8F%E3%80%81%E5%87%BB%E7%A9%BF">redis缓存雪崩、穿透、击穿</a></p>
</li>
<li>
<p><a href="#Redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86">redis分布式锁的实现原理</a></p>
</li>
<li>
<p><a href="#redis%E5%AE%9E%E6%88%98">redis实战</a><br>
-<a href="#string">string</a><br>
-[setnx](#setnx redis分布式锁实现)<br>
-<a href="#mset,mget,msetnx">mset,mget,msetnx</a><br>
-<a href="#append">append</a><br>
-<a href="#incr">incr</a><br>
-<a href="#decr">decr</a><br>
-<a href="#exists">exists</a><br>
-<a href="#exists">del</a><br>
-<a href="#type">type</a><br>
-<a href="EXPIRE">EXPIRE</a><br>
-<a href="#hash">hash</a><br>
-<a href="#hset,hget">hset,hget</a><br>
-<a href="#hincr,hdecr">hincr,hdecr</a><br>
-<a href="#list">list</a><br>
-<a href="#lpush">lpush</a><br>
-[BRPOP BLPOP](#BRPOP BLPOP)<br>
-[RPOPLPUSH BRPOPLPUSH](#RPOPLPUSH BRPOPLPUSH)<br>
-[LINDEX LSET LINSERT LTRIM LREM ](#LINDEX LSET LINSERT LTRIM LREM )<br>
-<a href="#set">set</a><br>
-<a href="#sadd">sadd</a><br>
-[BRPOP BLPOP](#BRPOP BLPOP)<br>
-[RPOPLPUSH BRPOPLPUSH](#RPOPLPUSH BRPOPLPUSH)<br>
-[LINDEX LSET LINSERT LTRIM LREM ](#LINDEX LSET LINSERT LTRIM LREM )</p>
<p>-[sorted set](#sorted set)<br>
-<a href="#HyperLoglog">HyperLoglog</a>；<br>
-<a href="#bitmap">bitmap</a><br>
-<a href="#GeoHash">GeoHash</a></p>
</li>
</ul>
<h1 id="redis-2">redis</h1>
<h2 id="目录-2">目录</h2>
<h3 id="redis架构">Redis架构</h3>
<h4 id="redis持久化">Redis持久化</h4>
<p>redis持久化机制对于故障恢复意义</p>
<pre><code>    数据备份和故障恢复，
    如果没有持久化，redis遇到灾难性故障的时候，就会丢失所有的数据，
    如果通过持久化将数据搞一份在磁盘上，然后定期同步到云服务器上，那么就可以保证数据不丢失全部，
    还是可以恢复一部分
</code></pre>
<h5 id="rdb持久化机制">RDB持久化机制</h5>
<p>RDB持久化机制原理介绍：每隔几分钟，生成redis内存数据的一份完整的快照<br>
缺点：redis故障恢复时，数据对丢失的比AOF更多<br>
主进程在fork RDB文件时，如果文件过大，可能导致服务暂停数毫秒，甚至数秒<br>
优点：1.RDB会生成多个数据文件，每个文件都代表了某一时刻中redis的数据，这种多个数据文件的方式，特别适合做冷备。<br>
2.RDB对外提供的读写服务，影响非常小，可以让redis保持高性能，因为redis主进程只需要fork一个子进程，让子进程执行磁盘IO操作来进行RDB持久化即可。<br>
3.相对于AOF来说，直接基于RDB来重启和恢复redis进程，更加快速<br>
AOF存放的是指令日志，做数据恢复的时候，其实要回放和执行所有指令日志，来恢复出来内存中的所有数据。<br>
RDB，就是一份数据文件，恢复的时候，直接加载到内存中即可</p>
<p>一般不要让redis的RDB间隔时间太长</p>
<h5 id="aof持久化机制">AOF持久化机制</h5>
<p>AOF持久化机制原理介绍：AOF机制对每条写入命令作为日志，以append-only的模式写入os cache 中，fsync 在将os cache数据输入缓存一个日志文件中，在redis重启的时候，<br>
可以通过回放AOF日志中的写入命令来重构整个数据集。<br>
如果我们想要redis仅仅作为纯内存的缓存来使用，那么可以禁止RDB和AOF所有持久化机制<br>
如果想要同时使用RDB和AOF两种持久化机制，那么在redis重启的时候，会使用AOF来重构数据，因为AOF的数据更加完整<br>
缺点：<br>
优点：1.更好的保证数据不丢失，即使redis进程挂了，最多丢失1m的数据<br>
2.AOF日志文件以append-only模式写入，写入性能非常高<br>
3.AOF日志文件即使过大，出现后台重写操作，也不会影响客户端读写，因为在rewrite log的时候，会对其中的指令进行压缩，创建出一份需要恢复数据的最小日志出来，<br>
在创建新日志文件的时候，老日志文件还是照常写入，当新的merge后日志文件ready的时候，再交换新老日志文件即可</p>
<p>RDB和AOF如何选择：<br>
redis是支持两种模式的持久化</p>
<h4 id="redis主从">Redis主从</h4>
<p>redis replication --&gt;  主从架构---&gt;读写分离---&gt;水平扩容支持高并发<br>
读多写少：</p>
<pre><code>一主多从，主负责写，并且将数据同步复制到其它slave节点，从节点负责读，所有的读请求全部走slave。
进行扩容的
</code></pre>
<p>redis replication的核心机制：</p>
<pre><code>    1.redis采用异步方式复制数据到slave节点，不过从redis 2.8开始，slave节点会周期性的确认自己每次复制数据量
    2.一个master 可以配置多个slave node 
    3.slave node也可以连接其它slave node 
    4.slave node做复制的时候是不会block master node 的正常工作的
    5.slave node在做复制的时候是不会block对自己的查询操作，它会用旧的数据集提供服务，但是复制完成时候，需要删除旧的数据集，加载新的数据集，这个时候会暂停对外的服务。
    6.slave node主要是用来做横向扩容的，增加slave node可以提高吞吐量
</code></pre>
<p>master 持久化对于主从架构安全保障的意义</p>
<pre><code>如果采用了主从架构，那么建议开启master node持久化

master 节点如果没有开启持久化机制，redis宕机之后，恢复时数据就是空的，同步到时候slave node数据也是空的，数据100%丢失。
</code></pre>
<p>redis主从复制的原理、断点续传、无磁盘化复制、过期key处理<br>
1.redis主从复制的原理</p>
<pre><code>    当启动一个slave node的时候，它会发送一个fsync命令给master node ，
    如果是slave node重新连接master node，那么master node仅仅会复制给slave node部分缺少的数据。
    否则，如果是slave node第一次连接master node，那么会触发一次full resynchronized
    
    开始full resynchronized的时候，master会启动一个后台线程，开始生成一份rdb快照，同时还会将客户端的所有写命令缓存到内存中，RDB
    生成完成之后，master 会讲这个rdb发送给slave ，slave先写入到本地磁盘，然后加载到内存，然后master会讲内存中的缓存写命令发送给slave，
    slave node也会同步这些数据
</code></pre>
<p>2.redis的断点续传</p>
<pre><code>    从redis2.0开始就支持断点续传，如果主从复制的过程中，网络连接突然断掉了，那么可以接着上次复制的地方，继续复制下去，而不是从头复制一份。
    master node会在内存中保存一个backlog，master和slave都会保存一个replica offset 还有一个master node id，
    offset就是保存在backlog中，如果master和slave node网络突然断掉了，slave node会让master node从上次replica offset开始继续复制
    但是如果没有找到对应的offset，那么会执行一次resynchronized
</code></pre>
<p>3.无磁盘化复制</p>
<pre><code>RDB在内存中直接创建rdb，然后发送给slave，不会在自己本地落磁盘了
</code></pre>
<p>4.过期key处理</p>
<pre><code>slave不会过期key，只会等待master过期key，如果msater过期了一个key或者通过lru淘汰了一个key，那么会模拟一条del 命令发送给slave。
</code></pre>
<h4 id="redis哨兵">Redis哨兵</h4>
<p>sentinal，中文名是哨兵，<br>
哨兵是redis集群架构中的一个非常重要的组建，主要功能如下<br>
(1).集群监控，负责监控redis master node和slave node进程是否正常工作<br>
(2).消息通知，如果某个redis实例有故障，那么哨兵负责发送消息作为报警通知给管理员<br>
(3).故障转移，如果master node挂掉了，那么会自动转移到slave node<br>
(4).配置中心，如果故障转移发生了，通知client客户端新的master上</p>
<p>哨兵本身也是分布式的，作为哨兵集群去运行，互相协同工作<br>
(1).故障转移时，判断一个master node是宕机了，需要大部分哨兵都同意才行，涉及到分布式选举到问题<br>
(2).即使部分哨兵节点挂掉了，哨兵集群还能正常工作，如果一个作为高可用的机制的重要组成部分故障转移系统本身是单点的，那就很坑爹了</p>
<p>目前采用的是sentinal 2版本，sentinal2 相对于sentinel 1来说，重写了很多代码，主要是让故障转移的机制和算法变得更加的健壮和简单</p>
<p>2.哨兵的核心知识</p>
<pre><code>(1).哨兵至少需要3个实例，来保证自己的健壮性
(2).哨兵+redis的主从架构，是不会保证数据0丢失的，只能保证redis集群的高可用
(3).对于哨兵+redis这种主从复杂的部署架构，尽量在测试和生产环境，都进行充足的测试和演练
</code></pre>
<p>3.为什么redis哨兵集群只有两个节点时无法正常工作</p>
<pre><code>哨兵集群必须部署2个以上的节点
如果哨兵集群仅仅部署了2个哨兵实例，quorum =1 ：表示有（1）多少个哨兵觉得master宕机了，就可以进行切换了，这个时候会尝试进行故障转移
M1       R1 
S1       S2

master宕机，s1和s2中只要有1个哨兵认为master宕机，就可以进行切换，同时s1  和s2中会选举出一个哨兵
来执行故障转移
同时这个时候，需要majority =2,也就是大多数哨兵是运行的，2个哨兵的majority就是2，2个哨兵都运行着，就可以允许故障转移

但是如果整个m1和s1运行的机器宕机了，那么哨兵只要一个了，此时就没有majority来允许执行故障转移了
虽然还有一台机器r1，但是故障转移不会执行
</code></pre>
<p>经典的3个哨兵集群</p>
<pre><code>M1       R1      R2
S1       S2      S3
quorum =2 表示有（2）多少个哨兵觉得master宕机了，就可以进行切换了，这个时候会尝试进行故障转移
majority =2 2个哨兵的majority就是2，2个哨兵都运行着，就可以允许故障转移 ， 同时s2  和s3中会选举出一个哨兵
来执行故障转移
</code></pre>
<p>哨兵主备切换的数据丢失问题：异步复制，集群脑裂</p>
<p>异步复制导致的数据丢失问题</p>
<pre><code>这个旧的master node内存里的那些数据还没来的及给是slave node 就挂掉了。
slave node就成了master node，那些内存中没来得及复制的数据不就丢失了吗
</code></pre>
<p>集群脑裂导致的数据丢失问题</p>
<pre><code>脑裂，也就是master所在的机器突然脱离了正常的网络，跟其它slave node机器不能连接，但是实际上master node运行着，
此时哨兵可能就会认为master宕机了，然后开启选举，将其它slave node 切换成master 
这个时候，集群中就有两个master，也就是所谓的脑裂。
此时虽然某个slave node被切换成了master ，但是可能client 还没来得及切换到新的master，还继续写向旧的master，
因此旧master再次恢复的时候，会作为slave挂载到存的master上去，自己的数据会清空，重新从新的master复制数据
</code></pre>
<p>解决异步复制和脑裂导致的数据丢失问题</p>
<pre><code>    min-slaves-to-write 1 
    min-slaves-max-lag 10
    要求至少有1个slave node，复制数据和同步的延迟不超过10s
    如果说一旦所有的slave，复制数据和同步数据都超过了10s，那么这个时候master将不在接受任何请求
    以上两个配置可以减少异步复制和脑裂的数据丢失问题

    (1).减少异步复制的数据丢失问题  
    min-slaves-max-lag这个配置，就可以确保说，一旦slave复制数据和ack延时太长，就认为可能master宕机后损失的数据太多了，那么就可以拒绝写请求，
    这样就可以把master宕机时由于部分数据未同步到slave node导致的数据丢失降低在可控范围内。
    (2).减少脑裂的数据丢失问题
    如果一个master出现了脑裂，跟其它slave node丢了连接，那么上面的两个配置可以确保的说，如果不能继续给指定数量的slave发送数据，而且slave node
    超过10s没有给自己ack消息，那么就直接拒绝客户端的写请求
    这样脑裂后旧的master就不会接受新的client 的新数据，也就避免了数据丢失
    以上的配置就确保了，如果跟任何一个slave node丢失了连接，在10s之内发现slave没有给自己ack，那么久拒绝新的写请求
</code></pre>
<p>sdown和odown转换机制</p>
<pre><code>sdown是主观宕机，就是一个哨兵就是自己觉得一个master宕机了，那么就是主观宕机
odown是客观宕机，如果quorum数量的哨兵都觉得一个mastar宕机了，那么就是客观宕机
sdown达成的条件很简单，如果一个哨兵ping一个master，超过了is-masterHost-down-millisecondssecond
指定的毫秒数之后，就主观认为master宕机了
sdown到odown的条件很简单，如果一个哨兵在指定时间内，收到了quorum指定数量的其它哨兵也认为那个master sdown 
那么就任务odown 
</code></pre>
<p>哨兵和slave自动发现机制</p>
<pre><code>    哨兵互相之间的发现是通过redis 的pub/sub系统实现的，每个哨兵都会放_sentinel_:hello这个
    channel里发送一个消息，这个时候其它哨兵就可以消费到这条消息，并感知其它哨兵的存在
    每隔两秒钟，每个哨兵都会往自己监控的某个master+slave对应的 _sentinel_:hello channel 里发送一个消息，
    内容是自己的host  ip 和runid，还有这个对应master的监控配置
    每个哨兵也会监听自己监控的某个master+slave对应的 _sentinel_:hello channel，然后去感知到同样在监听这个master +slave的其它哨兵的存在。
    每个哨兵还会跟其它哨兵交换对master的监控配置，互相进行监控配置的同步
</code></pre>
<p>slave --&gt; master选举算法</p>
<p>如果一个master被认为odown了，而且majority哨兵都允许了主备切换，那么某个哨兵就会执行主备切换操作，此时首先要选举出一个slave来<br>
会考虑slave的一些信息</p>
<pre><code>(1).跟master断开连接的时长
(2).slave node优先级
(3).run id
(4).复制的offset

接下来会对slave进行排序

（1).按照slave的优先级进行排序，slave node priority越低优先级越高
（2).如果slave node priority相同，那么看replica offset ，哪个slave复制了越多的数据，offset越靠后，优先级就越高。
 (3).如果 上面两个条件都相同，那么选择一个run id比较小的那个slave
</code></pre>
<p>slave配置的自动纠正</p>
<pre><code>哨兵会自定纠正slave的一些配置，比如slave node如果要成为潜在的master候选人，哨兵会确保slave node在复制现有master的数据
，如果slave连接到错误的master上，比如故障转移之后，哨兵会确保他们连接到正确的master上
</code></pre>
<p>quorum和majority</p>
<pre><code>每一次一个哨兵要做主备切换，首先要quorum数量的哨兵认为odown，然后选举出一个哨兵来做切换，
，这个哨兵还的得到majority哨兵的授权，才能正式执行切换，
但是如果quorum &gt;= majority，那么必须quorum数量的哨兵都授权，比如5个哨兵quorum是5，那么必须5个哨兵都同意授权，才能进行切换。
</code></pre>
<p>configuration epoch</p>
<pre><code>哨兵会对一套redis master +slave 进行监控，有相应监控的配置
会执行切换的那个哨兵，会从要切换到新的master（slave -&gt; master）那里会得到一个configuration epoch。这就是一个version号，每次切换的version必须是唯一的。
如果第一个选举出来的哨兵都失败了，那么其它哨兵，会等待failover-timeout时间，然后接替者继续切换此时会从新获取一个新的configuration epoch
作为新的版本号
</code></pre>
<p>configuration 传播</p>
<pre><code>    哨兵切换之后，会在自己本地更新生成最新的master配置，然后同步给其它的哨兵，所以就是通过之前所说的pub/sub消息机制
    这里之前的version号就很重要了，因为各种消息是通过channel发布和监听的，所以一个哨兵完成一次新的切换之后，
    新的master配置是跟着新的version号的，其它哨兵都是根据版本号大小更新自己的master配置
</code></pre>
<h4 id="redis集群">Redis集群</h4>
<p>主从架构的缺点：master节点的数据和slave节点的数据是一摸一样的，<br>
master最大能容纳多大的数据量，那么slave最多能容纳多大数据量</p>
<p>redis Cluster 可以支持多个master ，每个master都会挂载多个slave<br>
也支持读写分离的架构，对于每个master来说，写就写到master，然后读就从master对应的slave上读<br>
高可用，因为每个master都有slave节点，那么如果master挂掉，redis cluster 会自动将slave切换成master</p>
<p>redis cluster （多master+读写分离+高可用）</p>
<p>Redis cluster 主要针对海量数据+高并发+高可用的场景</p>
<p>Redis cluster介绍</p>
<pre><code>(1).自动将数据分片，每个master上放一部分数据
(2).提供内置的高可用支持，部分master不可用时，还可以继续工作
在redis cluster架构下，每个redis要开放两个端口号，一个是6379 另一个是加10000的端口号16379。
16379端口号是用于节点间通信的，也就是cluster bus的东西，集群总线，cluster bus的通信，用于进行故障检验，配置更新
故障转移授权
</code></pre>
<p>redis 数据分布算法<br>
1.hash算法<br>
2.一致性hash算法</p>
<h3 id="redis双写一致性">Redis双写一致性</h3>
<p>1.读的时候，先读缓存，缓存没有，在读数据库，设置缓存<br>
2.更新时候，先删除缓存，然后在更新数据库<br>
如果直接更新缓存，在更新数据库，如果在更新缓存的成功了，更新数据库失败了，下次读取的时候，直接读缓存，那么数据还是旧数据，缓存和数据库不一致</p>
<h3 id="redis缓存雪崩-穿透-击穿">Redis缓存雪崩、穿透、击穿</h3>
<h4 id="缓存雪崩">缓存雪崩，</h4>
<pre><code>是指缓存机器意外发生了全盘宕机，缓存挂了，导致请求全部落在数据库，数据库也支撑不住。
</code></pre>
<p>解决方案如下：<br>
事前：redis高可用，主从+哨兵，redis cluster避免全盘数据崩溃<br>
事中：本地缓存+hystrix限流降级，避免mysql被打死<br>
事后：redis持久化，一旦重启，自动从磁盘中加载数据到缓存，快速恢复缓存数据</p>
<h4 id="缓存穿透">缓存穿透</h4>
<pre><code>很多请求是黑客恶意发送的，缓存中查不到，每次去数据库也查不到
，这种恶意的请求就直接把数据库打死
</code></pre>
<p>解决办法；<br>
只要数据库没查到，就设置一个空值到缓存，返回设置一个过期时间，这样的话，下次有相同的key</p>
<h4 id="缓存击穿">缓存击穿</h4>
<pre><code>    某个key非常热点，访问非常频繁，处于集中式高并发访问的情况
    当这个key在失效的瞬间，大量的请求就击穿了缓存，直接请求数据库
</code></pre>
<p>解决方案如下：</p>
<pre><code>    可以将热点数据设置为永远不过期，或者基于redis or zookeeper实现互斥锁，等待第一个请求构建完缓存之后，再释放锁，进而
    其它请求也能通过该key访问数据
</code></pre>
<h4 id="redis分布式锁的实现原理">Redis分布式锁的实现原理</h4>
<p>如果说公司里落地生产环境用分布式锁的时候，一般都会使用开源类库，比如redis的分布式锁，一般就是用redisson框架就好了，非常简单易用。</p>
<pre><code class="language-java">
 /**
   * @author ChengJianSheng
   * @date 2019-07-30
   */
    @Slf4j
   @Service
  public class OrderServiceImpl implements OrderService {
     @Autowired
     private StockService stockService;
     @Autowired
     private OrderRepository orderRepository;
     @Autowired
     private RedissonClient redissonClient;
     /**
       * 乐观锁
       */
             @Override
     public String save(Integer userId, Integer productId) {
                 int stock = stockService.getByProduct(productId);
                 log.info(&quot;剩余库存：{}&quot;, stock);
                 if (stock &lt;= 0) {
                         return null;
                 }
                 //  如果不加锁，必然超卖
                 RLock lock = redissonClient.getLock(&quot;stock:&quot; + productId);
                 try {
                         lock.lock(10, TimeUnit.SECONDS);
                         //扣减库存
                        redissonClient.decr();
                         String orderNo = UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;).toUpperCase();
                         if (stockService.decrease(productId)) {
                                 OrderModel orderModel = new OrderModel();
                                 orderModel.setUserId(userId);
                                 orderModel.setProductId(productId);
                                 orderModel.setOrderNo(orderNo);
                                 Date now = new Date();
                                 orderModel.setCreateTime(now);
                                 orderModel.setUpdateTime(now);
                                 orderRepository.save(orderModel);
                                 return orderNo;
                             }
            
                     } catch (Exception ex) {
                         log.error(&quot;下单失败&quot;, ex);
                     } finally {
                         lock.unlock();
                     }
        
                 return null;
             }
         }

</code></pre>
<p>⼆、Redisson 实现 Redis 分布式锁的底层原理</p>
<p>（1）加锁机制<br>
首先一个客户端1会hash算法，选择一台机器，紧接着发送一段lua脚本到redis上<br>
<img src="img_1.png" alt="img_1.png" loading="lazy">，<br>
这一段脚本就是保证复杂业务的原子性，</p>
<p>RLock lock = redisson.getLock(&quot;myLock);<br>
先会走一个判断 exists mylock，如果你要加的那个锁的那个key不存在的话你就执行加锁</p>
<pre><code>hset mylock 客户端id：1 1
如果出现了一个这样的数据结构说明加锁成功
</code></pre>
<p>（2）锁互斥机制<br>
同样客户端过来尝试加锁，执行一段同样的lua脚本，会咋样？<br>
先会走一个判断 exists mylock，如果你要加的那个锁的那个key不存在的话你就执行加锁<br>
发现myLock这个锁key已经存在了。</p>
<pre><code>接着走第二个if判断，判断一下myLock锁key的hash数据结构中，是否包含了客户端2 的id 
但是很明显，那里只包含了客户端1的id
所以客户端2 会获取到pttl myLock返回一个数字，这个数字代表了myLock这个锁key的剩余生存是时间，比如还剩15000 毫秒的生存时间
此时客户端2 ，会进入一个while循环，不停的尝试加锁
</code></pre>
<p>（3）watch dog ⾃动延期机制</p>
<pre><code>客户端1加锁的锁key默认生存时间才30s，如果超过了30s，客户端1还想持有这把锁怎么办？
简单，客户端1一旦加锁成功，就会启动一个watch dog 看门狗，它是一个后台线程，会每隔10s检查一下
如果客户端1还持有锁key，那么就会不断延长锁key的生存时间
</code></pre>
<p>（4）可重⼊加锁机制<br>
如果客户端1已经持有了锁，再次加锁，依然执行命令，加锁次数累计加1</p>
<p>（5）锁释放机制<br>
如果执行lock.unLock()，就可以释放分布式锁，此时的业务逻辑也非常简单<br>
其实说白了，就是每次都对myLock数据结构的那个加锁次数减1。<br>
如果发现加锁次数是0了，那么说明客户端不再持有锁了，此时就会用<br>
del mylock 命令，从redis里面删除这个key</p>
<p>（6）此种⽅案 Redis 分布式锁的缺陷</p>
<pre><code>其实上⾯那种⽅案最⼤的问题，就是如果你对某个 redis master 实例，写⼊了 myLock 这种锁 key 的 value，此时会异步复制给对应的 master slave 实例。
但是这个过程中⼀旦发⽣ redis master 宕机，主备切换，redis slave 变为了 redis master。 接着就会导致，客户端 2 来尝试加锁的时候，在新的 redis master 上完成了加锁，⽽客户端 1
也以为⾃⼰成功加了锁。 此时就会导致多个客户端对⼀个分布式锁完成了加锁。 这时系统在业务语义上⼀定会出现问题，导致各种脏数据的产⽣。 
所以这个就是 redis cluster，或者是 redis master-slave 架构的主从异步复制导致的 redis 分布式 锁的最⼤缺陷：在 redis master 实例宕机的时候，可能导致多个客户端同时完成加锁。
</code></pre>
<figure data-type="image" tabindex="1"><img src="img.png" alt="img.png" loading="lazy"></figure>
<h3 id="redis实战">Redis实战</h3>
<h4 id="string">String</h4>
<h5 id="setnxredis分布式锁实现">Setnx(redis分布式锁实现)</h5>
<pre><code>分布式锁
命令：
set mykey newval nx
set mykey newval xx

java：
//加锁
jedis.set(&quot;locks_test&quot;, &quot;value_test&quot;,SetParams.setParams().nx())
//释放锁
jedis.del(&quot;locks_test&quot;);
必须是这个key此时是不存在的，才能设置成功，如果说key要是存在了，此时设置失败
</code></pre>
<h5 id="msetmgetmsetnx">mset,mget,msetnx</h5>
<pre><code>命令
mset a 10 b 20 c 30
mset a 10 b 20 c 30

java 
mset:一下子设置多个key-value对
mget:一下子获取多个key的value 
msetnx:就是多个key都不存在的情况下，一次性设置多个key的value,只要key都不存在才能成功
mset和mgt相当于batch批量设置和查询，比如说加入你要一次性要往redis里塞入
20条数据
//新增或修改
jedis.mset(&quot;key:key&quot;,&quot;value&quot;,&quot;key:key1&quot;,&quot;value2&quot;)
//新增，key不能存在
jedis.msetnx(&quot;key:key&quot;,&quot;value&quot;,&quot;key:key1&quot;,&quot;value2&quot;)
//获取
jedis.mget(&quot;key:key&quot;,&quot;key:key1&quot;)
// 获取value长度
jedis.strlen(&quot;key:key&quot;)
//截取 value的字符
jedis.getrange(&quot;key:key&quot;，0，5)
</code></pre>
<h5 id="append">append</h5>
<pre><code>日志审计
redis append api 就是不停的把数据追加到指定的key里去
jedis.append(key,value);
</code></pre>
<h5 id="incr">incr</h5>
<pre><code>生成唯一id/（点赞）
命令：
set counter 100
incr counter
incrby counter 50

INCR 命令将字符串值解析成整型，将其加一，最后将结果保存为新的字符串值，类似的命令有INCRBY, DECR 和 DECRBY。实际上他们在内部就是同一个命令，只是看上去有点儿不同。
INCR是原子操作意味着什么呢？就是说即使多个客户端对同一个key发出INCR命令，也决不会导致竞争的情况。例如如下情况永远不可能发生：『客户端1和客户端2同时读出“10”，他们俩都对其加到11，然后将新值设置为11』。最终的值一定是12，read-increment-set操作完成时，其他客户端不会在同一时间执行任何命令。
对字符串，另一个的令人感兴趣的操作是GETSET命令，行如其名：他为key设置新值并且返回原值。这有什么用处呢？例如：你的系统每当有新用户访问时就用INCR命令操作一个Redis key。你希望每小时对这个信息收集一次。你就可以GETSET这个key并给其赋值0并读取原值。

java
jedis.incr(&quot;key&quot;);
</code></pre>
<h5 id="decr">decr</h5>
<pre><code>抢购
命令：
set counter 100
自增
incr counter
</code></pre>
<h5 id="exists">exists</h5>
<pre><code>命令
exists mykey
EXISTS命令返回1或0标识给定key的值是否存在
</code></pre>
<h5 id="del">del</h5>
<pre><code>使用DEL命令可以删除key对应的值，DEL命令返回1或0标识值是被删除(值存在)或者没被删除(key对应的值不存在)。
</code></pre>
<h5 id="type">type</h5>
<pre><code>TYPE命令可以返回key对应的值的存储类型：
命令
type mykey
</code></pre>
<h5 id="expire">EXPIRE</h5>
<pre><code>EXPIRE来设置超时时间(也可以再次调用这个命令来改变超时时间，使用PERSIST命令去除超时时间 )。
命令
set key some-value
expire key 5
也可以在创建值的时候设置超时时间:
set key 100 ex 10

TTL命令用来查看key对应的值剩余存活时间。
ttl key
</code></pre>
<h4 id="hash">hash</h4>
<p>Hash 便于表示 objects，实际上，你可以放入一个 hash 的域数量实际上没有限制（除了可用内存以外）。所以，你可以在你的应用中以不同的方式使用 hash</p>
<h5 id="hsethget">hset,hget</h5>
<pre><code>值得注意的是，小的 hash 被用特殊方式编码，非常节约内存。
# 获取
hget user:1000 username
hgetall user:1000
hmget user:1000 username birthyear no-such-field
# 将value +10
hincrby user:1000 birthyear 10

HMSET 指令设置 hash 中的多个域，而 HGET 取回单个域。HMGET 和 HGET 类似，但返回一系列值：



基于token令牌的登陆会话机制：
用户平时在访问我们的系统，在处理任何一个请求之前，必须检查这个请求是否带上了一个令牌，
如果带上了这个令牌，那么此时必须在redis里检查一下，这个令牌是否在redis里合法，有效的session会话。

如果有这个session会话，那么此时可以允许这个请求被处理，说明这个人已经登陆过我们的系统，登陆之后在会在
redis里放一个有效的session会话，
如果说没有这个session会话，此时就会导致用户被迫强制登陆
如果用户登陆之后，就会返回浏览器或者客户端一块令牌，同时在redis里初始化ession会话，后续客户端
就会在指定时间范围内发送请求的时候，带上一块令牌，每次令牌和服务端的session校验通过就可以执行请求
过一段时间过后，服务端的redis里的session会话就会过期，过期之后又回导致你重新登陆。
</code></pre>
<h4 id="list">list</h4>
<p>常用案例</p>
<pre><code>秒杀活动下利用公平队列的抢购机制   待办事项  

秒杀系统有很多实现方式，其中一种技术方案，就是对所有涌入系统的秒杀抢购请求，都放入redis里的一个linhuatest
数据结构中去，进行公平队列排队，然后入队之后等待秒杀结果，专门搞一个消费者从redis 里面按顺序
获取抢购请求，按顺序进行库存扣减，扣减成功了，就让抢购成功。

如果说你要是不要公平队列的话，可能会导致你很多抢购请求进来，大家都在尝试去扣减库存
 此时可能先涌入进来的请求并没有先对redis进行抢购请求，此时可能后进入的请求先执行了抢购请求，此时就是不公平的

公平队列：基于redis里的list数据结构，搞一个队列，抢购请求先进队列，先入先出，先进来的人先抢购，此时就是公平的。

对于抢购队列，就用lpush list 就可以了，然后对出队的队列进行抢购
</code></pre>
<h5 id="lpush">lpush</h5>
<pre><code> LPUSH 命令可向list的左边（头部）添加一个新元素，而RPUSH命令可向list的右边（尾部）添加一个新元素。最后LRANGE 命令可从list中取出一定范围的元素:
//从左边(头部)添加元素
lpush mylist A
//从右边(尾部)添加元素
rpush mylist A
//从尾部开始，指定范围获取元素
lrange mylist 0 -1

注意:LRANGE 带有两个索引，一定范围的第一个和最后一个元素。这两个索引都可以为负来告知Redis从尾部开始计数，因此-1表示最后一个元素，-2表示list中的倒数第二个元素，以此类推。

rpop mylist
lpop mylist
pop,它从list中删除元素并同时返回删除的值。可以在左边或右边操作



List上的阻塞操作
可以使用Redis来实现生产者和消费者模型，如使用LPUSH和RPOP来实现该功能。但会遇到这种情景：list是空，这时候消费者就需要轮询来获取数据，这样就会增加redis的访问压力、增加消费端的cpu时间，而很多访问都是无用的。
 为此redis提供了阻塞式访问 BRPOP 和 BLPOP 命令。 消费者可以在获取数据时指定如果数据不存在阻塞的时间，如果在时限内获得数据则立即返回，如果超时还没有数据则返回null, 0表示一直阻塞。

同时redis还会为所有阻塞的消费者以先后顺序排队。

如需了解详细信息请查看 RPOPLPUSH 和 BRPOPLPUSH。
</code></pre>
<h5 id="brpop-blpop">BRPOP BLPOP</h5>
<pre><code>BRPOP 是一个阻塞的列表弹出原语。 它是 RPOP 的阻塞版本，因为这个命令会在给定list无法弹出任何元素的时候阻塞连接。 该命令会按照给出的 key 顺序查看 list，并在找到的第一个非空 list 的尾部弹出一个元素。

请在 BLPOP 文档 中查看该命令的准确语义，因为 BRPOP 和 BLPOP 基本是完全一样的，除了它们一个是从尾部弹出元素，而另一个是从头部弹出元素。

BRPOP list1 list2 0

返回值
多批量回复(multi-bulk-reply): 具体来说:

当没有元素可以被弹出时返回一个 nil 的多批量值，并且 timeout 过期。
当有元素弹出时会返回一个双元素的多批量值，其中第一个元素是弹出元素的 key，第二个元素是 value。
</code></pre>
<h5 id="rpoplpush-brpoplpush">RPOPLPUSH BRPOPLPUSH</h5>
<h5 id="lindex-lset-linsert-ltrim-lrem">LINDEX LSET LINSERT LTRIM LREM</h5>
<pre><code>LINDEX：返回指定位置的数据
LSET：设置指定位置的数据
LINSERT：往指定位置插入数据
LTRIM：然后保留指定的数据，删掉一些数据
LREM：删掉一些数据
</code></pre>
<h4 id="set">set</h4>
<pre><code>案例 抽奖  公共关注  推荐关注 微博关系 点赞 uv
无序且不重复的数据集合
</code></pre>
<h5 id="sadd">sadd</h5>
<pre><code> 添加元素
 sadd myset 1 2 3

返回所以元素
smembers myset


现在我已经把三个元素加到我的 set 中，并告诉 Redis 返回所有的元素。可以看到，它们没有被排序 —— Redis 在每次调用时可能按照任意顺序返回元素，因为对于元素的顺序并没有规定。

判断元素是否存在
sismember myset 30

返回所以元素
srem myset

取交集
sinter tag:1:news tag:2:news tag:10:news tag:27:news 

对多个集合取并集
sunionstore game:1:deck deck

取差集
sdiffstore new——set  set1 set2

获取随机元素

spop game:1:deck   随机从set里弹出几个元素

srandmember
</code></pre>
<h5 id="scard">scard</h5>
<pre><code> scared 获取key数据总数
</code></pre>
<h4 id="sorted-set">sorted set</h4>
<pre><code>案例  推荐商品  排行榜  自动补全

sorted set不能又重复数据，加入进去的每一个数据都可以带一个分数，它在里面的数据都是按照分数排序的、
有序的set 自动按照分数来排序，相当于你可以定制它的排序规则

添加
zadd hackers 1940 &quot;Alan Kay&quot;

查询   从索引0 到最后一个元素 分数排序
zrange hackers 0 -1

如果我想按相反的顺序，从最小的到最大的顺序，怎么办？使用ZREVRANGE而不是ZRANGE：
zrevrange hackers 0 -1

询问元素在有序元素集中的位置
zrank hackers &quot;Anita Borg&quot;


搜索结果返回分数
zrange hackers 0 -1 withscores
zrevrange hackers 0 -1 withscores

搜索指定分数范围内的数据
zrangebyscore hackers -inf 1950

根据指定分数搜索后删除
zremrangebyscore hackers 1940 1960

ZRANGEBYLEX，我们可以查询字典范围：
zrangebylex hackers [B [P

将分数自增
zincrby
</code></pre>
<h4 id="hyperloglog">HyperLoglog</h4>
<pre><code>案例  日活 网站垃圾数据去重或者过滤

HyperLoglog ，数据结构+概率算法 组合而成的，去重统计 近似数

 如果基于set来计数，太耗费内存，基于HyperLoglog算法来计数，是近似数，有
  0.8%误差，但是误差不会太大，可以给出一个相对准确的近似数，而且就占12KB内存

每次看到新元素时，都要使用PFADD将其添加到计数中。
pfadd hll a b c d 
    
每次要检索迄今为止使用PFADD添加的唯一元素的当前近似值时，都要使用PFCOUNT
pfcount hll


pfmerge  
</code></pre>
<h4 id="bitmap">bitmap</h4>
<pre><code>bitmap位图：二进制里的一位一位的，字符串，int ，long double 都可以用二进制表示
在二进制中都是表示多少位，一个字节是8位的二进制数
int 就是四个字节 就是32位
我可以直接在redis里操作二进制的位数据。

可以把网站里的每一种操作，每天执行过的用户放在一个位图里，
一个用户仅仅代表一位而已

案例 ：基于位图用户行为记录程序
如果说你要记录一下，在系统里执行一些特殊操作，每天执行过某个操作的用户有多少个人
操作日志，审计日志，
记录下来每个用户每天做了哪些操作

 设置位图
 setbit key offset 1
 表示把 这个offset 对应位图的位置设置位1

getbit key value
</code></pre>
<h4 id="geohash">GeoHash</h4>
<pre><code>geoadd key longitude latitude user 
geoadd key longitude latitude shop 
geodist key user shop init ='KM'_ 
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[mysql]]></title>
        <id>https://zuolinlin.github.io/zuo.github.io/post/mysql/</id>
        <link href="https://zuolinlin.github.io/zuo.github.io/post/mysql/">
        </link>
        <updated>2022-04-08T13:18:19.000Z</updated>
        <content type="html"><![CDATA[<h1 id="mysql">mysql</h1>
<h2 id="目录">目录</h2>
<ul>
<li>
<p><a href="#Mysql%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84">mysql基础架构</a></p>
<ul>
<li><a href="#Mysql%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1">mysql架构设计</a></li>
<li><a href="#InnoDB%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1">InnoDB存储引擎的架构设计</a>
<ul>
<li>[buffer pool](#buffer pool)</li>
<li>[redo log](#redo log)</li>
<li>[undo log](#undo log)</li>
</ul>
</li>
<li><a href="#%E4%BA%8B%E5%8A%A1">事务</a>
<ul>
<li><a href="#%E5%A4%9A%E4%BA%8B%E5%8A%A1%E5%B9%B6%E5%8F%91%E6%9B%B4%E6%96%B0%E6%88%96%E8%80%85%E6%9F%A5%E8%AF%A2%E7%9A%84%E6%95%B0%E6%8D%AE%E9%97%AE%E9%A2%98">多事务并发更新或者查询的数据问题</a>
<ul>
<li><a href="#%E8%84%8F%E5%86%99">脏写</a></li>
<li><a href="#%E8%84%8F%E8%AF%BB">脏读</a></li>
<li><a href="#%E4%B8%8D%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB">不可重复读</a></li>
<li><a href="#%E5%B9%BB%E8%AF%BB">幻读</a></li>
</ul>
</li>
<li><a href="#SQL%E6%A0%87%E5%87%86%E4%B8%AD%E5%AF%B9%E4%BA%8B%E5%8A%A1%E7%9A%844%E4%B8%AA%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB">SQl对事务的四种隔离级别</a>
<ul>
<li><a href="#%E8%AF%BB%E6%9C%AA%E6%8F%90%E4%BA%A4">read uncommitted</a></li>
<li><a href="#%E8%AF%BB%E5%B7%B2%E6%8F%90%E4%BA%A4">read committed</a></li>
<li><a href="#%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB">repeatable</a></li>
<li><a href="#%E4%B8%B2%E8%A1%8C%E5%8C%96">serializable</a></li>
</ul>
</li>
<li><a href="#%E9%80%8F%E5%BD%BB%E5%89%96%E6%9E%90Mysql%E7%9A%84MVCC%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E6%9C%BA%E5%88%B6">透彻剖析Mysql的MVCC事务隔离机制</a>
<ul>
<li>[undo log版本链](#Undo log版本链)</li>
<li><a href="#ReadView%E6%9C%BA%E5%88%B6">ReadView机制</a></li>
<li>[Read Committed隔离级别是如何基于ReadView机制实现的？](#Read Committed隔离级别是如何基于ReadView机制实现的？)</li>
<li><a href="#MySQL%E6%9C%80%E7%89%9B%E7%9A%84RR%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%EF%BC%8C%E6%98%AF%E5%A6%82%E4%BD%95%E5%9F%BA%E4%BA%8EReadView%E6%9C%BA%E5%88%B6%E5%AE%9E%E7%8E%B0%E7%9A%84%EF%BC%9F">MySQL最牛的RR隔离级别，是如何基于ReadView机制实现的？</a></li>
</ul>
</li>
<li><a href="#%E5%A4%9A%E4%B8%AA%E4%BA%8B%E5%8A%A1%E6%9B%B4%E6%96%B0%E5%90%8C%E4%B8%80%E8%A1%8C%E6%95%B0%E6%8D%AE%E6%97%B6%EF%BC%8C%E6%98%AF%E5%A6%82%E4%BD%95%E5%8A%A0%E9%94%81%E9%81%BF%E5%85%8D%E8%84%8F%E5%86%99%E7%9A%84%EF%BC%9F">多个事务更新同一行数据时，是如何加锁避免脏写的？</a>
<ul>
<li><a href="#%E5%85%B1%E4%BA%AB%E9%94%81">共享锁</a></li>
<li><a href="#%E7%8B%AC%E5%8D%A0%E9%94%81">独占锁</a></li>
</ul>
</li>
<li><a href="#%E7%B4%A2%E5%BC%95">索引</a>
<ul>
<li><a href="#%E7%A3%81%E7%9B%98%E4%B8%8A%E6%95%B0%E6%8D%AE%E9%A1%B5%E7%9A%84%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84">磁盘上数据页的存储结构</a></li>
<li><a href="#%E9%A1%B5%E5%88%86%E8%A3%82%E7%9A%84%E8%BF%87%E7%A8%8B">页分裂的过程</a></li>
<li><a href="#%E4%B8%BB%E9%94%AE%E7%B4%A2%E5%BC%95">主键索引</a></li>
<li><a href="#B+%E6%A0%91%E5%AE%9E%E7%8E%B0%E7%B4%A2%E5%BC%95%E7%9A%84%E7%89%A9%E7%90%86%E7%BB%93%E6%9E%84">B+树实现索引的物理结构</a></li>
<li><a href="#%E8%81%9A%E7%B0%87%E7%B4%A2%E5%BC%95">聚簇索引</a></li>
<li><a href="#%E9%92%88%E5%AF%B9%E4%B8%BB%E9%94%AE%E4%B9%8B%E5%A4%96%E7%9A%84%E5%85%B6%E4%BB%96%E5%AD%97%E6%AE%B5%E5%BB%BA%E7%AB%8B%E7%B4%A2%E5%BC%95%E7%9A%84%E5%8E%9F%E7%90%86">针对主键之外的其他字段建立索引的原理</a></li>
<li><a href="#%E7%B4%A2%E5%BC%95%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9">索引的优缺点</a></li>
<li><a href="#%E7%B4%A2%E5%BC%95%E7%9A%84%E4%BD%BF%E7%94%A8%E8%A7%84%E5%88%99">索引的使用规则</a></li>
<li><a href="#%E7%B4%A2%E5%BC%95%E7%9A%84%E8%AE%BE%E8%AE%A1%E8%A7%84%E5%88%99">索引的设计规则</a></li>
<li><a href="#%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92">执行计划</a></li>
<li><a href="#sql%E8%B0%83%E4%BC%98">sql调优</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#mysql%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B">mysql数据模型</a>
<ul>
<li><a href="#VARCHAR%E8%BF%99%E7%A7%8D%E5%8F%98%E9%95%BF%E5%AD%97%E6%AE%B5%EF%BC%8C%E5%9C%A8%E7%A3%81%E7%9B%98%E4%B8%8A%E5%88%B0%E5%BA%95%E6%98%AF%E5%A6%82%E4%BD%95%E5%AD%98%E5%82%A8%E7%9A%84">VARCHAR这种变长字段，在磁盘上到底是如何存储的</a></li>
<li><a href="#%E4%B8%80%E8%A1%8C%E6%95%B0%E6%8D%AE%E4%B8%AD%E7%9A%84%E5%A4%9A%E4%B8%AANULL%E5%AD%97%E6%AE%B5%E5%80%BC%E5%9C%A8%E7%A3%81%E7%9B%98%E4%B8%8A%E6%80%8E%E4%B9%88%E5%AD%98%E5%82%A8%EF%BC%9F">一行数据中的多个NULL字段值在磁盘上怎么存储？</a></li>
<li><a href="#%E7%A3%81%E7%9B%98%E6%96%87%E4%BB%B6%E4%B8%AD40%E4%B8%AAbit%E4%BD%8D%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A4%B4%E4%BB%A5%E5%8F%8A%E7%9C%9F%E5%AE%9E%E6%95%B0%E6%8D%AE%E6%98%AF%E5%A6%82%E4%BD%95%E5%AD%98%E5%82%A8%E7%9A%84%EF%BC%9F">磁盘文件中40个bit位的数据头以及真实数据是如何存储的？</a></li>
<li><a href="#%E8%A1%8C%E6%BA%A2%E5%87%BA">行溢出</a></li>
<li><a href="#%E8%A1%A8%E7%A9%BA%E9%97%B4">表空间</a></li>
</ul>
</li>
</ul>
</li>
<li>
<p><a href="#%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5">生产实践</a></p>
<ul>
<li><a href="#%E7%9C%9F%E5%AE%9E%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9C%BA%E5%99%A8%E9%85%8D%E7%BD%AE%E5%A6%82%E4%BD%95%E8%A7%84%E5%88%92%EF%BC%9F">真实生产环境下的数据库机器配置如何规划？</a></li>
<li><a href="#%E4%BA%92%E8%81%94%E7%BD%91%E5%85%AC%E5%8F%B8%E7%9A%84%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%95%B0%E6%8D%AE%E5%BA%93%E6%98%AF%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E7%9A%84%EF%BC%9F%EF%BC%9F">互联网公司的生产环境数据库是如何进行性能测试的？</a></li>
<li><a href="#%E5%A6%82%E4%BD%95%E5%AF%B9%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9B%E8%A1%8C360%E5%BA%A6%E6%97%A0%E6%AD%BB%E8%A7%92%E5%8E%8B%E6%B5%8B%EF%BC%9F">如何对生产环境中的数据库进行360度无死角压测？</a></li>
<li><a href="#%E5%A6%82%E4%BD%95%E4%B8%BA%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E9%83%A8%E7%BD%B2%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F%EF%BC%9F">如何为生产环境中的数据库部署监控系统？</a></li>
<li>[如何通过多个Buffer Pool来优化数据库的并发性能？](#如何通过多个Buffer Pool来优化数据库的并发性能？)</li>
<li>[如何通过chunk来支持数据库运行期间的Buffer Pool动态调整？](#如何通过chunk来支持数据库运行期间的Buffer Pool动态调整？)</li>
<li>[在生产环境中，如何基于机器配置来合理设置Buffer Pool？](#在生产环境中，如何基于机器配置来合理设置Buffer Pool)</li>
<li><a href="#Linux%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F%E8%BD%AF%E4%BB%B6%E5%B1%82%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90%E4%BB%A5%E5%8F%8AIO%E8%B0%83%E5%BA%A6%E4%BC%98%E5%8C%96%E5%8E%9F%E7%90%86">Linux操作系统的存储系统软件层原理剖析以及IO调度优化原理</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%BF%E7%94%A8%E7%9A%84RAID%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84%E5%88%9D%E6%AD%A5%E4%BB%8B%E7%BB%8D">数据库服务器使用的RAID存储架构初步介绍</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%9A%84RAID%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84%E7%9A%84%E7%94%B5%E6%B1%A0%E5%85%85%E6%94%BE%E7%94%B5%E5%8E%9F%E7%90%86">数据库服务器上的RAID存储架构的电池充放电原理</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%BA%93%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5%E6%95%85%E9%9A%9C%E7%9A%84%E5%AE%9A%E4%BD%8DToomanyconnections">数据库无法连接故障的定位，Too many connections</a></li>
<li><a href="#%E7%BA%BF%E4%B8%8A%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%9A%84%E6%80%A7%E8%83%BD%E6%8A%96%E5%8A%A8%E4%BC%98%E5%8C%96">线上数据库不确定性的性能抖动优化</a></li>
</ul>
</li>
<li>
<p><a href="#mysql%E4%B8%BB%E4%BB%8E%E6%9E%B6%E6%9E%84">mysql主从架构</a></p>
<ul>
<li><a href="#%E4%B8%BB%E4%BB%8E%E6%9E%B6%E6%9E%84%E7%9A%84%E5%8E%9F%E7%90%86">主从架构的原理</a></li>
<li><a href="#%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E6%9E%B6%E6%9E%84%E7%9A%84%E6%90%AD%E5%BB%BA">主从复制架构的搭建</a></li>
<li><a href="#%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E6%95%B0%E6%8D%AE%E5%BB%B6%E8%BF%9F%E9%97%AE%E9%A2%98">主从复制数据延迟问题</a></li>
<li><a href="#%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84">高可用架构</a></li>
<li><a href="#%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8">分库分表</a></li>
</ul>
</li>
</ul>
<h1 id="目录-2">目录</h1>
<h2 id="mysql基础架构">Mysql基础架构</h2>
<h3 id="mysql架构设计">Mysql架构设计</h3>
<p>一个不变的原则：网络连接必须让线程处理 mysql架构的整体设计原理<br>
<img src="image/mysqlshejiyuanli.png" alt="img_1.png" loading="lazy"></p>
<h3 id="innodb存储引擎的架构设计">InnoDB存储引擎的架构设计</h3>
<figure data-type="image" tabindex="1"><img src="image/InnoDByuanlli.png" alt="img.png" loading="lazy"></figure>
<p>实际上，执行器是非常核心的一个组件，负责跟存储引擎配合完成一个sql语句在磁盘与内存层面的全部数据更新操作。</p>
<p>拆分成两个阶段：</p>
<p>上图的1，2，3，4是执行更新语句的时候干的事， 5，6是从你提交事务开始的，属于提交事务阶段</p>
<pre><code>redo log 是一种偏向物理性值的重做日志，本身属于InnoDB存储引擎特有的一个东西。

redo log 主要记录下你对数据做了哪些修改，这个此时还在内存缓存区

bin log 叫做归档日志，它里面记录的时偏向逻辑性的日志，类似  对users 表中的id =10的一行数据，进行了更新操作，操作以后的值是什么   

bin log 日志不是InnoDB 存储引擎特意的日志文件，是属于 mysql server 自己的日志文件
</code></pre>
<p>提交事务时，redo log日志的刷盘策略：</p>
<pre><code>  这个策略通过innodb_flush_log_at_trx_commit 来配置
  0：提交事务的时候，不会把redo log buffer 里的数据刷入磁盘文件，此时你可能提交事务了，结果mysql宕机了，此时内存中的数据全部丢失。
  1：提交事务的时候，就必须把redo log 从内存中刷入到磁盘文件里去，只要事务提交成功，那么redo log 就必然在磁盘里。
  2：提交事务的时候，把redo日志写入磁盘文件对应的os cache缓存里去，而不是直接进入磁盘文件，可能1s之后才会把os cache里的数据写入到磁盘文件
</code></pre>
<p>对于数据库这种严格的系统而言，一般建议redo 日志刷盘策略设置为1，保证事务提交之后，数据绝对不能丢失</p>
<p>提交事务时，bin log日志的刷盘策略：</p>
<pre><code>  这个策略通过sync_binlog参数来控制binlog的刷盘策略，它的默认值是0
  0:提交事务的时候，新进入 os cache 内存缓存，后刷回到磁盘（bin log会丢失）
  1:提交事务的时候，强制把binlog直接写入磁盘文件里去（bin log不会丢失）
</code></pre>
<h4 id="buffer-pool">buffer pool</h4>
<figure data-type="image" tabindex="2"><img src="image/bufferpool.png" alt="img_1.png" loading="lazy"></figure>
<p>数据库buffer pool 里面会包含很很多个缓存页，同时每个缓存页还有一个数据描述，也可以叫做数据控制</p>
<p>初始化buffer pool</p>
<pre><code>数据库只要已启动，就会按照你设置的buffer pool 的大小稍微再加大一点去找操作系统申请一块内存区域，作为buffer pool的内存区域

然后当内存区域申请完毕之后，数据库就会按照默认的缓存页的16kb的大小以及对应800个字节左右的描述数据的大小，在buffer pool 中划分出来一个个缓存页和一个个对应的数据描述

只不过这个时候，buffer pool中一个个缓存页的都是空的，里面什么都没有，要等数据库运行起来，我们对数据进行增删改查的操作的时候，才会把数据对应的磁盘文件读取出来，放入buffer pool 的缓存页
</code></pre>
<p>哪些缓存页是空闲的？ free链表</p>
<p>从磁盘上的数据页放入到buffer pool的缓存页，必然涉及到一个问题，那就是哪些缓存页是空闲的？</p>
<pre><code>所以数据库会为buffer pool 设计一个**free链表**，它是一个双向链表的数据结构，这个free链表里，每个节点就是一个空闲的
缓存页的描述数据块的地址，也就是说，只要你的一个缓存页是空闲的，那么它的描述数据块地址就会被放入free链表中。
</code></pre>
<p>磁盘上的数据页是如何读到缓存页中？</p>
<pre><code> 其实有了free链表之后，这个问题就很简单了，首先需要重free链表中获取描述数据块，然后就可以获取这个描述数据块对应的空闲缓存页
 写缓存页，添加描述信息
</code></pre>
<p>那怎么知道一个数据是否加载到缓存页？</p>
<pre><code>数据库会维护一个**哈希表数据结构**，他会用表空间+数据页号作为key，然后缓存的地址作为value

也就是说每次你读取一个数据页缓存之后，都会在这个哈希表中写入一个key-value，下次在使用数据页只需要从哈希表中读取数据即可 
</code></pre>
<p>哪些缓存页是脏页</p>
<pre><code>内存中更新的脏页数据，都是要被刷回磁盘文件的。
但是不肯呢个所有的缓存页都刷回磁盘，因为有的缓存页可能是因为查询的时候，而被读到buffer pool 里面去的，可能根本没有修改过

所以数据库这里引入了另外一个跟free链表类似的**flush 链表**，**这个flush链表的本质也是通过缓存页的描述数据块的两个指针，让被修改过的缓存页描述数据块组成一个双向链表**
</code></pre>
<p>引入LRU算法来判断哪些缓存页是不常用的（缓存命中率）</p>
<pre><code>怎么判断哪些缓存页不是经常使用，哪些缓存页是脏页？
引入LRU链表

LRU：least recently used 最近最少使用的意思

工作原理：

假如我们从磁盘加载一个数据页到缓存页的时候，就会把这个缓存页描述数据放到LRU的头部，
那么只要有数据缓存页的时候，他就会在LRU链表里，而且最近被在加载的缓存页，都会放到
LRU的头部去。

然后假定某个缓存页的描述数据块本来是放在LRU尾部，后续你只要查询或者修改了这个缓存页的数据，也要把这个缓存页挪动到HttpServletRequest
尾部，也就是说最近被访问过的缓存页，一定在LRU的头部。
</code></pre>
<p>LRU算法带来的问题</p>
<pre><code>预读带来的巨大问题

预读会导致，一直没被访问的数据放在LRU链表的头部，在空闲缓存页全部使用完时，会将链表尾部的数据刷入磁盘，清空缓存页。但是有可能这个数据时经常被使用的
</code></pre>
<p>哪些情况会触发Mysql的预读机制</p>
<pre><code>1.innodb_read_ahead_threshold他的默认值是56，意思是就是如果顺序的访问一个区里的多个数据页，访问的数据页的数量可能超过这个阈值
此时就会触发预读机制，把下一个相邻区中所有额数据页都加载到缓存中去。

2.如果Buffer Pool里缓存了12个联系的数据页，而且这些数据都是比较频繁被访问的，此时就会出发预读机制，把这个区里的其他数据页都加载到缓存里区。
这个机制是通过参数innodb_random_read_ahead来控制的，默认时OFF，也就是这个规则是关闭的
</code></pre>
<p>另外一种可能导致频繁访问的缓存页被淘汰的场景体验一下</p>
<p>那就是<strong>全表扫描</strong></p>
<pre><code>  类似  SELECT * FROM USERS 他一下子吧这个表里的所有数据页，都加载到Buffer Pool里去
</code></pre>
<p>Mysql基于冷热数据分离方案优化LRU算法</p>
<pre><code>真正的LRU链表，会被拆分成两个部分,一个部分是热数据，一个部分是冷数据，这个冷数据比例是由

innodb_old_blocks_pct参数来控制的，它默认的是37，也就是说冷数据的占比37%。

实际上这个时候，第一次加载时，缓存页会被放到冷数据链表的头部。
</code></pre>
<p>冷数据区域的缓存页何时被加载到热数据区域</p>
<pre><code>innodb_old_blocks_time 默认设置为1000，也就是1000毫秒

也就是数据加载到冷数据区域，过了1s后，你再访问这个缓存页，他就会被放到热数据区域的链表头部
</code></pre>
<figure data-type="image" tabindex="3"><img src="image/LRU.png" alt="img.png" loading="lazy"></figure>
<p>LRU链表的热数据区域是如何进行优化的？</p>
<pre><code>经常被访问的数据时热数据，不经常被访问的数据是冷数据，所以在设计缓存的时候，经常会考虑 **热数据的缓存预加载**
也就是说，每天统计出来哪些商品被访问次数最多，然后晚上的时候，系统启动一个定时作业，把热门商品的数据，预加载到redis里。
那么第二页是不是对热门访问的商品自然就优先走redis

 LRU链表的热数据区域的访问规则被优化了一下，即你只要在热数据区域的后3/4部分缓存页被访问了，才会给你移动到链表头部

如果你是热数据区域的前面的1/4的缓存页被访问，他是不会移动到链表头部的。

这样可以尽可能减少链表中的节点移动了。
</code></pre>
<p>定时LRU尾部的部分缓存页刷入磁盘</p>
<pre><code>第一个时机：有一个后台线程，他会运行一个定时任务，这个定时任务每个一段时间，就会把LRU链表的冷数据区域的尾部一些缓存页刷入到磁盘里去，清空几个缓存页，把他们加入到free链表中。

只要缓存页被刷盘，那么这个缓存页必然会加搭配free链表中，从flush链表中一处，从LRU链表中移除。

因为LRU链表中的热数据可能是被频繁修改的，难道他们永远都不刷入到磁盘了吗？

第二个时机，这个后台线程同时也会在Mysql不怎么繁忙的时候，找个时间把flush链表中的缓存页刷入磁盘，这样被你修改过的数据迟早都会刷入磁盘。
</code></pre>
<h4 id="redo-log">redo log</h4>
<p>redo log:在事务提交成功之后，保存一条日志记录，防止机器宕机导致数据丢失。顺序写，性能高。</p>
<p>redo log长什么样？</p>
<pre><code>redo log里面记录的就是：**表空间号+数据页号+偏移量+修改了几个字节的值+具体的值**

修改了几个字节的值，redo log就划分了不同的类型，MLOG_1BYTE:就是修改了一个字节的值，以此类推
但是如果你修改了一大串的值，类型就是MLOG_WRITE_STRING,就是代表你一下子在那个数据页的某个偏移量位置插入或者修改了一大串的值

日志类型(就是类似MLOG_1BYTE)，表空间号，数据页号，数据页中的偏移量。具体修改的数据
</code></pre>
<p>redo log写磁盘的过程</p>
<pre><code>其实mysql内有另外一个数据结构，叫做 redo log block
一个 redo log block是512字节，这个redo log block字节分为三个部分
一个是12字节的header块头，一个是496字节的body块体，一个是4字节trailer块尾
在这里面，12个字节的header投又分为4个部分：
    1.包括4个字节的block no，就是块唯一编码
    2.2个字节的data length，就是block里写入了多个字节数据；
    3.2个字节的first record group ，这个是说每个事务都会有多个redo log ，一个是redo log group，另一组redo log。那么在这个block里的第一组redo log的偏移量，就是这两个字节存储的；
    4.4个字节的checkpoint on
</code></pre>
<figure data-type="image" tabindex="4"><img src="image/redolog.png" alt="img.png" loading="lazy"></figure>
<p>redo log block 与磁盘文件的关系</p>
<figure data-type="image" tabindex="5"><img src="image/redo_log_block2.png" alt="img.png" loading="lazy"></figure>
<p>平时我们执行完增删改之后，要写入磁盘的redo log，其实应该是先进入到redo log block这个数据结构里，然后再进入磁盘文件</p>
<p>redo log buffer 类似申请出一块连续的空间，然后里面划分出N多个空的redo log block</p>
<p>通过设置mysql的innodb_log_buffer_size可以指定这个redo log buffer的大小，默认也就是16MB</p>
<figure data-type="image" tabindex="6"><img src="image/redo_log_block3.png" alt="img_1.png" loading="lazy"></figure>
<p>redo log buffer中的缓冲日志，到底是什么时候写入磁盘的？</p>
<pre><code>（1）如果写入redo log buffer的日志已经占据了redo log buffer总容量的一半了，也就是超过了8MB的redo log在
缓冲里了，此时就会把他们刷入到磁盘文件里去
（2）一个事务提交的时候，必须把他的那些redo log所在的redo log block都刷入到磁盘文件里去，只有这样，当事
务提交之后，他修改的数据绝对不会丢失，因为redo log里有重做日志，随时可以恢复事务做的修改
（PS：当然，之前最早最早的时候，我们讲过，这个redo log哪怕事务提交的时候写入磁盘文件，也是先进入os cache的，进入os的
文件缓冲区里，所以是否提交事务就强行把redo log刷入物理磁盘文件中，这个需要设置对应的参数，我们之前都讲过的 ，大家回过
头去看看 ）
（3）后台线程定时刷新，有一个后台线程每隔1秒就会把redo log buffer里的redo log block刷到磁盘文件里去
（4）MySQL关闭的时候，redo log block都会刷入到磁盘里去
</code></pre>
<p>redo log占用磁盘越来越大怎么办？</p>
<p>实际上默认情况下，redo log都会写入到一个目录中文件按里，这个目录可以通过</p>
<pre><code>show variables like 'datadir'
</code></pre>
<p>可以通过修改</p>
<pre><code>innodb_log_group_home_dir
</code></pre>
<p>参数来设置redo log这个目录</p>
<p>指定每个redo log文件的大小，默认是48M</p>
<pre><code>innodb_log_file_size
</code></pre>
<p>指定日志文件的数量</p>
<pre><code>innodb_log_file_in_group
</code></pre>
<figure data-type="image" tabindex="7"><img src="image/redologsetting.png" alt="img.png" loading="lazy"></figure>
<h4 id="undo-log">undo log</h4>
<p>INSERT 语句的undo log 类型是TRX_UNDO_INSERT_REC ，这个undo log里包含了一下的东西：</p>
<pre><code>    1.这条日志的开始位置
    2.主键的各列长度和值
    3.表idx
    4.undo log 日志编号
    5.undo log 日志类型
    6.这条日志的结束位置
</code></pre>
<figure data-type="image" tabindex="8"><img src="image/undolog.png" alt="img.png" loading="lazy"></figure>
<p>现在事务要是回滚，直接从undo log 日志中拿出这个id，找到对应的数据删掉</p>
<h3 id="事务">事务</h3>
<h4 id="多事务并发更新或者查询的数据问题">多事务并发更新或者查询的数据问题</h4>
<p>多个事务要是对缓存页里的同一条数据同时进行更新或者查询，此时会产生哪些问题？</p>
<pre><code>  实际上会设计到脏读，脏写，不可重复读，幻读
</code></pre>
<h5 id="脏写">脏写</h5>
<pre><code>事务B修改了事务A修改过的值，此时事务A还没提交，所以事务A随时会回滚，导致事务B修改过的值也没了
</code></pre>
<figure data-type="image" tabindex="9"><img src="image/zangxie.png" alt="img.png" loading="lazy"></figure>
<h5 id="脏读">脏读</h5>
<pre><code>事务B查询了事务A修改过的数据，但是此时事务A还没有提交，所以事务A随时回滚，导致事务B再次查询就读不到事务A修改的数据了
</code></pre>
<figure data-type="image" tabindex="10"><img src="image/zangdu.png" alt="img.png" loading="lazy"></figure>
<p>其實一句话总结：</p>
<pre><code>无论是脏写还是脏读，都是因为一个事务去更新或者查询了另外一个还没有提交的事务更新过的数据
    
因为另外一个事务还没提交，所以他随时可能反悔回滚，那么必然导致你更新的数据没了，或者你之前查询到的数据就没了，这种就是脏读和脏写。
</code></pre>
<h5 id="不可重复读">不可重复读</h5>
<pre><code>   针对已经提交的事务修改的值，被你的事务给读到了，你的事务多次查询，多次读到的是别人已经提交事务
   修改过的值，导致每次查询的值不一样
</code></pre>
<figure data-type="image" tabindex="11"><img src="image/bukechongfudu.png" alt="img_1.png" loading="lazy"></figure>
<h5 id="幻读">幻读</h5>
<h4 id="sql标准中对事务的4个隔离级别">SQL标准中对事务的4个隔离级别</h4>
<p>SQL标准中滚定了4种事务隔离级别，并不是Mysql的事务隔离级别，mysql的事务隔离级别有点差别。</p>
<p>在SQL标准中，规定的4种事务隔离级别，就是说多个事务并发运行的同时，互相是如何隔离的，从而避免事务并发问题</p>
<p>这四种级别包括：</p>
<h5 id="read-uncommitted-读未提交是不允许脏写的">read uncommitted 读未提交：是不允许脏写的</h5>
<pre><code>也就是说，不可能两个事务在没有提交的情况下去更新同一行数据的值，
但是这种隔离级别下，可能发生脏读，不可重复度，幻读。
</code></pre>
<h5 id="read-committed-rc-读已提交不可能发生脏写和脏读">read committed  RC 读已提交：不可能发生脏写和脏读</h5>
<pre><code>也就是说人家事务没有提交修改的值，你是绝对读不到的
这种隔离级别下不会发生脏读和脏写，但是可以发生不可重复读和幻读
</code></pre>
<h5 id="repeatable-read-rr-可重复读不可能发生脏读脏写不可重复读">repeatable read RR 可重复读：不可能发生脏读脏写，不可重复读</h5>
<pre><code>你的事务多次查询一个数据的值，哪怕别的事务修改这个值还提交了，没有，你不会读到人家事务提交事务修改过的值
你的事务一旦开始，多次查询一个值，会一直读到同一个值。
</code></pre>
<h5 id="serializable-串行化">serializable 串行化</h5>
<pre><code> 这种隔离级别，根本不允许你多个事务并发执行，只能串起来执行
</code></pre>
<h4 id="spring对事务的支持">spring对事务的支持</h4>
<p>在@Transaction(isolation =isolation.DEFAULT),默认是default，表示数据库是什么就是什么 isolation.READ_UNCOMMITTED isolation.READ_COMMITTED<br>
isolation.REPEATABLE_READ isolation.SERIALIZABLE</p>
<h4 id="透彻剖析mysql的mvcc事务隔离机制">透彻剖析Mysql的MVCC事务隔离机制</h4>
<h5 id="undo-log版本链">Undo log版本链</h5>
<pre><code>我们每条数据其实都有两个隐藏字段，一个是trx_id,一个是roll_pointer ，
這個trx_id就是最近一次更新过这条数据的事务id，
roll_pointer 就是指向你了你更新这个事务之前生成的undo log。
所以不管多个事务并发执行时如何执行的，
起码先搞清楚一点，就是多个事务串行执行的时候，每个人修改了一行数据，都会更新隐藏字段trx_id和roll_pointer，
同时之前多个数据快照对应的undo log，会通过roll_pointer指针串联起来，形成一个很重要的版本链
</code></pre>
<h5 id="readview机制">ReadView机制</h5>
<p><a href="https://apppukyptrl1086.pc.xiaoe-tech.com/detail/i_5e86040203a20_GmWrGMJe/1?from=p_5e0c2a35dbbc9_MNDGDYba&amp;type=6">ReadView机制</a></p>
<p>你在在执行事务的时候，就会生成一个ReadView，里面比较重要的东西有四个</p>
<pre><code>1.一个是m_ids，这个就是说此时有哪些事务在mysql里执行还没有提交的
2.一个是min_trx_id，就是m_ids里最小的值；
3.一个是max_trx_id,这是说mysql下一个要生成的事务id，就是最大的事务id
4.一个是create_trx_id，就是你这个事务的id
</code></pre>
<p>现在两个事务并发执行过来，一个是事务A（id = 45），一个是事务B（id = 59），事务B要更新这行数据<br>
事务A是要去读取这行数据的值</p>
<p>现在事务A直接开启ReadView，这个ReadView里的m_ids就包含事务A和事务B的两个id，45和59，<br>
然后min_trx_id就是45，max_trx_id 就是60 create_trx_id 就是45 是事务A自己</p>
<p>这个时候事务A第一次查询这行数据，会走一个判断，就是判断这行数据的trx_id，是否小于 ReadView<br>
中的min_trx_id，此时发现trx_id =32是小于 ReadView中的min_trx_id 就是45的<br>
说明你在事务开启之前，修改这行数据的事务早就提交了，所以此时你就可以看到这行数据</p>
<p>接着事务B开始动手了，他把这行数据的值改为了B，然后这行数据trx_id设置为自己的id，也就是59，<br>
同时roll_pointer指向了修改之前生成的undo log，接着这个事务B就提交了。</p>
<p>接着事务A再次查询，此时查询的时候发现一个问题，就是此时数据行里的trx_id = 59 ，那么这个trx_id是大于ReadView里的<br>
min_trx_id（45），同时又小于ReadView里的max_trx_id（60）的，说明更新这条数据的事务很有可能跟自己差不多同时开启的，<br>
于是会看一下m_ids列表中有45 和 59 两个事务id，直接证实了，这条修改数据的事务是跟自己同一时段并发执行然后提交的，所以对这行数据是不能查询的。</p>
<p>既然这行数据不能查询，那查什么？<br>
简单，顺着这条数据的roll_pointer顺着undo log日志链往下找，就会找到最近一条undo log，trx_id =32，<br>
此时发现trx_id =32 是小于ReadView里的min_trx_id =45 的 说明这个undo log版本必然是在事务A开启之前就执行且提交了。<br>
好了，那么就查询最近的那个undo log里的值好了，这就是undo log多版本链条的作用，他可以保存一个快照链条，让你可以读到之前的快照值。</p>
<p>如果事务A ，更新值A,trx_id修改为45，同时保存之前事务B修改值的快照。<br>
那么此时A来查询这条数据，会发现trx_id =45 居然和自己的ReadView里的create_trx_id = 45 的值是一样的。<br>
说明这行数据就是自己修改的，自己修改的值当然可以i看到</p>
<p>如果在事务A执行的过程中，突然开启了是一个事务C，这个事务C的id =78，然后它更新了那行的值为值C，还提交了<br>
这个时候，事务A再去查询，会发现当前数据的trx_id = 78，大于了自己的ReadView中的max_trx_id（60）此时说明了什么，<br>
说明是这个事务A开启之后，然后有一个事务更新了数据，自己当然是看不到的。<br>
此时就会顺着undo log多版本链条往下找，自然先找到值A之前修改过的那个版本，因为那个trx_id = 45 跟自己的<br>
ReadView里的create_trx_id是一样的，所以此时直接读取自己修改的那个版本</p>
<figure data-type="image" tabindex="12"><img src="image/ReadViewjizhi.png" alt="img.png" loading="lazy"></figure>
<pre><code>通过undo log多版本链条，加上你开启事务时候生产的一个ReadView，然后再有一个查询的时候，根据ReadView进
行判断的机制，你就知道你应该读取哪个版本的数据。
而且他可以保证你只能读到你事务开启前，别的提交事务更新的值，还有就是你自己事务更新的值。假如说是你事务
开启之前，就有别的事务正在运行，然后你事务开启之后 ，别的事务更新了值，你是绝对读不到的！或者是你事务开
启之后，比你晚开启的事务更新了值，你也是读不到的！
</code></pre>
<h5 id="read-committed隔离级别是如何基于readview机制实现的">Read Committed隔离级别是如何基于ReadView机制实现的？</h5>
<pre><code>每次查询都生成新的ReadView，那么如果 在你这次查询之前，有事务修改了数据还提交了，你这次查询生成的ReadView里，那个m_ids列表当然不包含这个已
经提交的事务了，既然不包含已经提交的事务了，那么当然可以读到人家修改过的值了。
这就是基于ReadView实现RC隔离级别的原理，实际上，基于undo log多版本链条以及
ReadView机制实现的多事务并发执行的RC隔离级别、RR隔离级别，就是数据库的MVCC多版本并发控制机制。
</code></pre>
<h5 id="mysql最牛的rr隔离级别是如何基于readview机制实现的">MySQL最牛的RR隔离级别，是如何基于ReadView机制实现的？</h5>
<p>默认的ReadView 就是这个机制</p>
<pre><code>默认情况下，有人在更新数据的时候，你去读取这一行数据,直接默认就是开启mvcc机制的。
也就是说，此时一行数据的读和写两个操作默认是不会加锁互斥的，因为mysql的mvcc机制就是为了解决这个问题，避免频繁加锁互斥。
此时你读取数据，完全可以根据你的ReadView，去在undo log版本链条里找一个你能读取的版本，完全不用顾虑别人在不在更新。
就算你真的等他更新完毕了还提交了，基于mvcc机制，你也读取不到他更新的值啊！因为ReadView机制是不允许的，所以你默认情况下的读
，完全不需要加锁，不需要care其他食物的更新加锁问题，直接介于mvcc机制读某个快照就可以了

如果要再执行查询的时候想要加锁，mysql支持一种共享锁 就是 s锁，这种共享锁的语法
select * from table lock in mode
共享锁和独占锁互斥，独占锁之间互斥，共享锁与共享锁不互斥
    
查询的时候还能加互斥锁，也就是 X 锁（Exclude独占锁），这种独占锁的语法
select * from table for update

当有一个事务加了独占锁之后，其他事务再更新这行数据，都是要加独占锁的，但是只能生成独占锁在后面等待。

一旦你查询的时候加了独占锁，此时在你的事务提交之前，任何人都不能更新数据，只能你在本事务里更新数据，等你提交了别人在更新数据
</code></pre>
<h4 id="多个事务更新同一行数据时是如何加锁避免脏写的">多个事务更新同一行数据时，是如何加锁避免脏写的？</h4>
<p>多个事务同时更新一行数据，此时都会加锁（X 锁，也就是Exclude独占锁），然后都会等待排队，必须一个事务执行完毕了，提交了，释放了锁，才能唤醒别的事务继续执行。</p>
<p>加锁</p>
<figure data-type="image" tabindex="13"><img src="image/jiasuo.png" alt="img.png" loading="lazy"></figure>
<p>释放锁-加锁<img src="image/shifangsuojiasuo.png" alt="img.png" loading="lazy"></p>
<h5 id="共享锁">共享锁</h5>
<pre><code>如果要再执行查询的时候想要加锁，mysql支持一种共享锁 就是 s锁，这种共享锁的语法
select * from table lock in mode
共享锁和独占锁互斥，独占锁之间互斥，共享锁与共享锁不互斥


默认情况下，有人在更新数据的时候，你去读取这一行数据,直接默认就是开启mvcc机制的。
也就是说，此时一行数据的读和写两个操作默认是不会加锁互斥的，因为mysql的mvcc机制就是为了解决这个问题，避免频繁加锁互斥。

如果要再执行查询的时候想要加锁，mysql支持一种共享锁 就是 s锁，这种共享锁的语法
select * from table lock in mode
共享锁和独占锁互斥，独占锁之间互斥，共享锁与共享锁不互斥
    
查询的时候还能加互斥锁，也就是 X 锁（Exclude独占锁），这种独占锁的语法
select * from table for update

当有一个事务加了独占锁之后，其他事务再更新这行数据，都是要加独占锁的，但是只能生成独占锁在后面等待。

一旦你查询的时候加了独占锁，此时在你的事务提交之前，任何人都不能更新数据，只能你在本事务里更新数据，等你提交了别人在更新数据
</code></pre>
<h5 id="独占锁">独占锁</h5>
<pre><code>当有一个事务加了独占锁之后，其他事务再更新这行数据，都是要加独占锁的，但是只能生成独占锁在后面等待。

查询的时候还能加互斥锁，也就是 X 锁（Exclude独占锁），这种独占锁的语法

select * from table for update

一旦你查询的时候加了独占锁，此时在你的事务提交之前，任何人都不能更新数据，只能你在本事务里更新数据，等你提交了别人在更新数据
</code></pre>
<h4 id="在表级别加锁">在表级别加锁</h4>
<p>多个事务并发更新数据的时候，都要在行级别加独占锁，独占锁是互斥的，所以不可能发生脏写问题，一个事务提交了才会释放自己的独占锁，唤醒下一个事务的执行。<br>
如果你此时去读取别的事务在更新的数据，有两种可能：<br>
1.第一种可能就是基于MVCC机制进行事务隔离，读取快照版本，这个是比较常见的；<br>
2.第二种可能是查询的同时基于特殊语法去加独占锁或者共享锁。</p>
<p>一般而言，不太建议在数据卷粒度去通过行锁实现复杂的业务锁机制，而更加建议通过redis，zookeeper来用分布式锁来实现复杂业务下的锁机制</p>
<p>比较正常的情况而言，其实还是多个事务并发运行更新一行数据，默认加独占锁互斥，同时其他事物基于mvcc机制进行快照版本读实现事务隔离<br>
、表锁其实是InnoDB存储引擎的概念</p>
<h4 id="索引">索引</h4>
<h5 id="磁盘上数据页的存储结构">磁盘上数据页的存储结构</h5>
<figure data-type="image" tabindex="14"><img src="image/shujuyedecunchu.png" alt="img.png" loading="lazy"></figure>
<pre><code>大量的数据页按照顺序一页一页的存放的，然后相邻的两个数据页会采用采用双向列表的格式互相引用

然后一个数据页内部会存储一行行的数据，也就是我们平时在表里插入的一行行数据就会存储在数据页里

然后数据页里的每一行数据都会按照主键大小进行排序村粗，同时每一行数据都有一个指针指向下一行数据

的位置，组成一个单向链表。

然后每个数据页都会有一个页目录，里面根据数据行的主键存放一个目录，同时数据行是被分割存储在不同槽位里去的

如果是主键查找，在数据页的页目录里,根据主键值直接定位到槽位，遍历槽位数据行，就可以找到数据
如果是非主键查找，只能进入数据页根据单向链表查找数据
</code></pre>
<p>在没有索引的情况下查找数据</p>
<pre><code>第一个数据页遍历所有数据页查找，将数据页加载到缓存页中
如果是主键查找，在数据页的页目录里,根据主键值直接定位到槽位，遍历槽位数据行，就可以找到数据
如果非主键查找，再单向遍历查找那条数据，如果没有那条数据，再加载下一个数据页到缓存页里来
以此类推，循环往复， 其实就是一个全表扫描
</code></pre>
<h4 id="页分裂的过程">页分裂的过程</h4>
<pre><code>假如我们不停的再表里插入数据，接着数据越来越多，此时就要在搞一个数据页了
但是此时就会遇到一个问题，索引运作的一个核心基础就是要求你后一个数据页的主键值大于前面一个数据页的主键值
但是如果你的主键是自增的，还可以保证这一点，因为你新插入的后一个数据页的主键值一定都大于前一个数据页的主键值。
但是如果你的主键不是自增的，所以可能会出现你的后一个数据页的主键值里，有点小于前一个数据页的主键值。
所以此时就会出现一个过程叫做也分裂
就是万一你的主键值都是你自己设置的，那么在增加一个新的数据页的时候，实际上会把前一个数据页主键值较大，挪到新的数据页里来
然后把你新插入的主键值较小的数据挪动到上一个数据页里去。保证了新数据页里的主键值一定都比上一个数据页里的主键值大。
</code></pre>
<h4 id="主键索引">主键索引</h4>
<pre><code>我们先拿最基础的主键索引来分析，把索引原理和查询原理搞清楚

mysql针对主键设计了一个索引，针对主键的索引实际上就是主键目录这个目录呢
把每个数据页的页号，还有数据页里最小主键的值放在一起，组成一个索引目录
</code></pre>
<figure data-type="image" tabindex="15"><img src="image/zhujiansuoyin.png" alt="img.png" loading="lazy"></figure>
<pre><code>通过主键查找时，通过二分查找，对比之后，确定id到底在那个数据页，
通过页目录直接定位到数据对应的槽位，遍历槽位数据行，就可以找到数据
</code></pre>
<h4 id="b树实现索引的物理结构">B+树实现索引的物理结构</h4>
<pre><code>很多索引数据不可能一直放在索引页里，会进行分裂

在更高索引层级里，保存每个索引页里最小的主键值，如果最顶层的索引页里存放下层索引页的也好太多了怎么办？
继续分裂，加一层索引页，这就形成了B+树，
这是索引最真实的物理存储结构，采用跟数据页一样的页结构来存储，一个索引就是由很多个数据页组成的一颗B+树
</code></pre>
<h4 id="聚簇索引">聚簇索引</h4>
<p>更新数据时，自动维护的聚簇索引</p>
<pre><code>当我们要查找某个主键id 的值时，通过二分查找很容易找到对应的索引页，通过索引页就能快读定位到数据页
实际上索引页和数据页之间是有指针连接起来的
另外呢,对于同一层级的索引页互相之间都是基于指针组成双向链表的
</code></pre>
<figure data-type="image" tabindex="16"><img src="image/index1.png" alt="img.png" loading="lazy"></figure>
<pre><code>加入把索引页和数据页综合起来看,他们连接在一起,看起来就如同一颗完整的大B+树一样

在B+树里最底层的一层就是数据页,数据页也就是B+树里的叶子节点了

也就是说,上图所有的索引页+数据页组成的B+树就是**聚簇索引**

这个聚簇索引默认时按照主键来组织的,所以你在增删改的时候,一方面会更新数据页,另一方面其实会给你自动维护
B+树结构的聚簇索引,给新增和更新索引页,这个聚簇索引是默认给你建立的.
</code></pre>
<h4 id="针对主键之外的其他字段建立索引的原理">针对主键之外的其他字段建立索引的原理</h4>
<pre><code>根据主键搜索数据的原理其实很清晰了,其实就是从聚簇索引的根节点进行二分查找,一路找到对应的数据页里,
基于页目录直接定位到主键对应的数据就可以了
</code></pre>
<p>主键之外的其他字段建立索引的原理</p>
<pre><code>其实原理是一样的,简单来说,你插入数据的时候,一方面会把完整的数据插入到聚簇索引的叶子节点的数据页里面去,
同时维护好聚簇索引,另一方面会为其他字段建立索引,重新建立一颗B+树.

比如你基于name建立一个索引,那么此时你插入数据的时候,就会重新搞一个B+树(这是独立与聚簇索引的另一个索引的B+树),
B+树的叶子节点也是数据页,但是这个数据页里仅仅存放主键字段和name字段.
</code></pre>
<figure data-type="image" tabindex="17"><img src="image/qitasuoyin.png" alt="img.png" loading="lazy"></figure>
<pre><code>搜索的时候过程和主键字段一模一样,不就是从name字段的索引B+树里根节点开始一层一层往下找,一直找到
叶子节点的数据页里,定位到name字段对应的主键值.
此时还需要进行回表操作,这个回表就是根据主键值,再到聚簇索引里面从根节点开始,一路找到叶子接待你的数据页,
定位到主键对应的完整数据行.
一般吧普通字段的索引称为二级索引,一级索引就是聚簇索引
</code></pre>
<p>多个字段联合起来,建立联合索引,比如:age+name</p>
<pre><code>联合索引的运行原理也是一样的,只不过是建立了一颗独立的B+树,叶子节点里面存放了id+name+age的数据,
然后默认按照name排序,name一样就按照age排序
查询的时候和普通索引一样的原理
</code></pre>
<p>联合索引的查询原理，以及使用索引的全职匹配规则</p>
<pre><code>假如莫得sql语句的where 条件里的几个字段的名称和顺序，都是跟你的索引里的字段一样，同时你还用等号值做匹配，那就是全职匹配
通过第一个字段值，二分查找找到对应的数据页，再在数据页里面二分查找找到对应的其他值找到id之后，进行查找聚簇索引回表操作，查询出剩下的其他字段
</code></pre>
<p>这就是InnoDB存储引擎的索引的完整实现原理了.</p>
<h4 id="索引的优缺点">索引的优缺点</h4>
<p>优点：不需要全表扫描，性能提升很快<br>
缺点消耗磁盘空间，增删改的速度很比较差（因为要维护B+树结构）</p>
<h4 id="索引的使用规则">索引的使用规则</h4>
<p>设计原则：</p>
<pre><code>设计系统时，索引的设计

1.一般建立索引，尽量使用那些基数比较大的字段，就是值比较多的字段，才能发挥出B+树快速二分查找的优势来

2.尽量使用那些字段值比较小的列来设计索引

3.设计索引别太多，建议两三个联合索引就应该覆盖掉你这个表的全部查询了。

否则索引太多，必然导致你增删改的时候性能很差，因为要很多个索引树。

4.另外很关键一点，建议大家主键一定是自增的，别用UUID之类的
因为主键自增，那么你的聚簇索引不会频繁的分裂，主键都是有序的，就会自然新增一个列而已，如果你用得视UUID，那么会
导致聚簇索引频繁的页分裂
</code></pre>
<p>where 使用原则：</p>
<pre><code>1.联合索引的等值匹配规则
    where语句中的几个字段名称和联合索引的条件里的几个字段的名称和顺序，都是跟你的索引里的字段一样，同时你还用等号值做匹配，那就是全职匹配

2.最左侧列匹配
   这个意思是我们的联合索引是KEY(index_key1,index_key2,index_key3)
   那么不一定必须要在where语句里查询三个字段来查，其实只需要根据最左侧的部分字段来查也是可以的
   where index_key1 = '' and index_key2 = '' 是没有问题的
   但是 你要用 where index_key3 那就不行了，因为在联合索引B+树里，是必须先按index_key1，再按index_key2，
   不能跳过前面两个字段直接按照最后一个index_key3 来查
   另外 where index_key1 = '' and index_key3 = '' ,那么饿只要index_key1在索引里能搜到，剩下的index_key3
    没办法在索引中找到
   所以，在建立所以的过程中，你必须建立好联合索引的字段，以及平时你写sql的时候要按照哪几个字段来查

3.最左前缀匹配规则
  即如果你要用like语法来查  like '1%'，是可以用到索引的  ，但是 like '%1'，没法用索引

4.范围查找规则
    同最左侧匹配

5.等值匹配+范围匹配规则
 同最左侧匹配
</code></pre>
<p>order 在sql进行排序，如何使用索引</p>
<pre><code>通常情况下，我们建立INDEX(xxx1,xxx2,xxx3)这样的联合索引，
这个时候默认情况下索引树里本身就是依次按照xxx1,xxx2,xxx3三个字段的值进行排序的
这个时候 用order by xxx1 ,xxx2,xxx3，在联合索引的索引树里都排好了，直接取出数据，
再去聚簇索引里面回表查询所有字段
但是这里有一个限定规则，因为联合索引里的字段值在索引树里都是从小到大一次排序的，
所以order by 要么都降序，要么都升序
不能有的字段降序，有的字段升序，那是不能用索引的
</code></pre>
<p>group by 在在sql中进行分组，如何使用索引<br>
通常而言 group by 后的字段，最好也是按照联合索引最左侧的字段开始，按顺序排列开来，这样的话就可以完美的<br>
运用上索引直接提取一组一组数据，然后对每组数据进行聚合就可以了。</p>
<p>覆盖索引</p>
<pre><code>覆盖索引是不是一种索引，他就是一种基于索引的查询方式罢了。
仅仅需要联合索引里面的几个字段的值，那么其实只要扫描联合索引的索引树就可以了
这种查询的方式就是覆盖索引，这样就不需要回表
</code></pre>
<p>最好使用覆盖索引<br>
即使真的要回表到聚簇索引，那你尽可能的用limit 和where 之类的语句<br>
限定一下回表到聚簇索引的次数，这样性能也要好一些</p>
<h4 id="索引的设计规则">索引的设计规则</h4>
<p>1.一般建立索引，尽量使用那些基数比较大的字段，就是值比较多的字段，才能发挥出B+树快速二分查找的优势来</p>
<p>2.尽量使用那些字段值比较小的列来设计索引</p>
<p>3.设计索引别太多，建议两三个联合索引就应该覆盖掉你这个表的全部查询了。</p>
<p>否则索引太多，必然导致你增删改的时候性能很差，因为要很多个索引树。</p>
<p>4.另外很关键一点，建议大家主键一定是自增的，别用UUID之类的<br>
因为主键自增，那么你的聚簇索引不会频繁的分裂，主键都是有序的，就会自然新增一个列而已，如果你用得视UUID，那么会<br>
导致聚簇索引频繁的页分裂</p>
<h4 id="执行计划">执行计划</h4>
<p>每一次提交sql给mysql，他内核的查询优化器，都会针对这个sql语句的语义去生成一个执行计划，<br>
这个执行计划就代表了，如何筛选过滤如何使用函数，如何进行排序，如何进行分组。</p>
<p>执行计划里包含哪些内容：<br>
1.数据的访问方式：</p>
<p>const - ref -range - index -- all</p>
<pre><code>const:肯定是通过了主键或者唯一索引，速度超高
ref:普通索引  或者主键唯一索引 搞了is null 或者 is not null
range:利用索引来进行范围筛选，一旦索引做了范围筛选，那么这种方式就是range
index:只要遍历组合索引就可以，不需要回表到聚簇索引中去，针对这种只需要遍历二级索引就能拿到你想要的数据，而不需要回源到聚簇索引的访问方式
all:直接全表扫描
</code></pre>
<p>const ，ref ，range :本质上都是居于索引树的二分查找和多层跳转来查询的，所以性能都是很高的<br>
接着接下来就是index 速度上比上面三种压迫差一些，因为他是走遍历二级索引树的叶子节点的方式来执行的<br>
那肯定比基于索引树的二分查找要慢多了，但是还是比全表扫描好一些。</p>
<p>多表关联的sql是如何执行的？</p>
<p>select * from tabel1 t1, table2 t2 where t1.xx = xxx,t2.xxx= xxx and t1.xxx2 = t2.xxx2</p>
<p>sql关联语法的实现原理（嵌套循环关联）：</p>
<pre><code>1.首先根据t1.xxx 表里查询出来一批数据，此时可能是const， ref ,index 或者all，具体看你的索引怎么建立的，他会挑一种执行计划的访问方式。
2.筛选出t1表的数据后，比如说找到两条数据，根据每条数据xxx2的值，以及t2.xxx2这个条件去t2表里找x2字段值和xxx 都匹配的字段值
这时就把两个表的数据关联起来了，另一条数据也是如法炮制。
</code></pre>
<p>记住，他可能是先从表里查一波数据，这个表叫做”驱动表“，再根据这波数据去另一个表查一波数据进行关联，另一个表叫做”被驱动表“</p>
<p>内连接：inner join 意思是两个表的数据必须完全关联上，才能返回回来<br>
外连接：outer join 可分为左外连接（左连接），右外连接（有连接）<br>
如果你是之前的那种内连接，那么连接条件可以放在where语句里，但是外连接一般是把连接条件放在ON字句里的</p>
<p>其实一般写多表关联，主要就是内连接和外连接，连接的语义和实现过程</p>
<p>mysql是如何根据成本优化执行计划的？</p>
<p>全表扫描的成本计算：</p>
<pre><code>show like status like '表名';
rows:表里的记录数
data_length:代表表的聚簇索引的字节数大小 默认是byte

全表扫描的 IO成本：数据页的数量* 1.0 +微调值
         CPU成本：行记录数 * 0.2 + 微调值
总成本= IO成本+CPU成本
</code></pre>
<p>索引的成本计算：</p>
<p>1.基于IN查询的子查询方式的优化：</p>
<p>sql物化表：存储引擎通过内存来存放，如果结果集太大，则可能采用普通B+树聚簇索引的方式存放在磁盘里<br>
但是无论如何，这个物化表都会建立索引。</p>
<p>2.对子查询的另一种优化方式：半连接</p>
<p>在互联网公司，比较崇尚的是尽量写简单的sql，复杂的逻辑用java系统来实现就可以了<br>
sql能单表就不要多表关联，能多表关联就尽量别写子查询，多考虑用java代码在内存里实现<br>
一些数据的复杂计算逻辑，而不是都放在sql里做。</p>
<p>执行计划落实到底层无非就是先访问哪张表，用哪个索引还是全表扫描，拿到数据之后如何去聚簇索引中回表<br>
是否要基于临时磁盘文件做分组聚合或者排序</p>
<p>查看执行计划的内容</p>
<p>id：每个select 都会对应一个id<br>
select_type:说的就是这一条执行计划对应查询的是个什么查询类型<br>
table：就是表名，意思是要查询哪张表<br>
partitions:是表分区的概念<br>
type:当前这个表的访问方式，比如说 const ref range index all<br>
possible_keys:可能选择的索引<br>
key：实际上选择的索引<br>
key_len:就是索引的长度<br>
ref:就是使用某个字段的索引进行等值匹配搜索的时候，跟索引列等值匹配的那个目表值得一些信息<br>
rows:是预估通过索引或者别的方式访问这个表的时候，大概可能取多少条数据，<br>
filtered:就是通过搜索条件过滤后得剩下数据得百分比<br>
extra：一些额外得信息</p>
<p>select_type:</p>
<pre><code>一般单表或者多表连接查询，他们得select_type都是SIMPLE，就是简单得查询
如果是union 语句，第一条执行计划针对表1，select_type 就是 PRIMARY
                第二条执行计划针对表2，select_type 就是 UNION
                第三条执行计划就是针对两个查询结果依托 一个临时表去重
                第三条执行计划 select_type 就是 union_result
如果是子查询，第一条执行计划 select_type 就是 PRIMARY
            第二条执行计划 select_type 就是SUBQUERY  
            select_type 是DERIVED 针对子查询得结果集会物化一个内部临时表
</code></pre>
<h4 id="sql调优">Sql调优</h4>
<pre><code>在sql调优的时候，核心就是分析执行计划里哪些出现了全表扫描，或者扫描的数据过大，尽可能的通过合理
优化索引保证执行计划每个步骤都是基于索引执行，避免扫描过多的数据
</code></pre>
<p>如果mysql使用了错误得执行计划应该怎么办？<br>
使用 force index 语法就可以了</p>
<p>select * from table fore index (index) where  index = &quot;&quot;</p>
<p>为什么mysql 默认会选对主键得聚簇索引进行扫描？</p>
<h3 id="mysql物理存储">mysql物理存储</h3>
<h4 id="varchar这种变长字段在磁盘上到底是如何存储的">VARCHAR这种变长字段，在磁盘上到底是如何存储的</h4>
<p><strong>引⼊变长字段的长度列表，解决⼀⾏数据的读取问题：</strong></p>
<pre><code>将数据的长度转成16进制表示，放在数据存储的前面
</code></pre>
<p><strong>多个变长字段是如何存储的？</strong></p>
<pre><code>此时在磁盘中存储的，必须在他开头的变长字段长度列表中存储⼏个变长字段的长度，⼀定要注意⼀点，他这⾥是逆
序存储的！
</code></pre>
<p>0x05 null值列表 数据头 hello a a 0x02 null值列表 数据头 hi a a</p>
<p><strong>mysql的一行行数据紧凑存储有什么好处？</strong></p>
<pre><code>多行紧凑的原因有： 序列化反序列时的开销小；不易有内存碎片；定位数据时比较快速
</code></pre>
<h4 id="一行数据中的多个null字段值在磁盘上怎么存储">一行数据中的多个NULL字段值在磁盘上怎么存储</h4>
<p><strong>为什么一行数据的NULL值不能直接存储？</strong></p>
<pre><code>肯定不是按照字符串的方式存储，会浪费空间。
</code></pre>
<p><strong>NULL值是以二进制Bit来存储的？</strong></p>
<pre><code>bit值是1 说明是NULL，如果是0 说明不是NULL
</code></pre>
<p><strong>磁盘上的⼀⾏数据到底如何读取出来的？</strong></p>
<pre><code>我们结合上⾯的磁盘上的数据存储格式来思考⼀下，⼀⾏数据到底是如何读取出来的呢？
再看上⾯的磁盘数据存储格式：
0x09 0x04 00000101 头信息 column1=value1 column2=value2 ... columnN=valueN

⾸先他必然要把变长字段长度列表和NULL值列表读取出来，通过综合分析⼀下，就知道有⼏个变长字段，哪⼏个变长
字段是NULL，因为NULL值列表⾥谁是NULL谁不是NULL都⼀清⼆楚。
此时就可以从变长字段长度列表中解析出来不为NULL的变长字段的值长度，然后也知道哪⼏个字段是NULL的，此时
根据这些信息，就可以从实际的列值存储区域⾥，把你每个字段的值读取出来了。
如果是变长字段的值，就按照他的值长度来读取，如果是NULL，就知道他是个NULL，没有值存储，如果是定长字
段，就按照定长长度来读取，这样就可以完美的把你⼀⾏数据的值都读取出来了！
</code></pre>
<h4 id="磁盘文件中40个bit位的数据头以及真实数据是如何存储的">磁盘文件中40个bit位的数据头以及真实数据是如何存储的</h4>
<pre><code>每一行数据在磁盘上存储的时候，每一行数据都会有变长字段长度列表，逆序存放这行数据里的变长字段的长度，  
然后会有NULL值列表，对于允许NULL值得字段都会有一个bit位标识那个字段是否为NULL，也是逆序排序得
</code></pre>
<p>每一行数据存储得时候，还得有一个bit位得数据头，这个数据头是用来描述这行数据的。</p>
<pre><code>第一位bit和第二位bit都是预留位，是没有任何含义的。
接下来的bit位是delete_mask：他标识这行数据是否被删除了
下一个bit位是min_rec_mask：在B+树里每一层的非页字节点里最小值都有这个标记
接下来是4个bit位是n_owned：记录了一个记录数
接下来13个bit位是heap_no，他代表是当前这行数据在数据堆里的位置
然后是3个bit位record_type：也就是这行数据的类型
   0:代表普通类型
   1：代表是B+树非叶子节点
   2：代表是最小值的数据 
   3：代表最大值的数据
最后16位bit是next_record:这个是他下一条数据的指针
</code></pre>
<h4 id="一行数据实际在磁盘上的存储">一行数据实际在磁盘上的存储</h4>
<p><img src="image/mysqlcipancunchu.png" alt="img.png" loading="lazy">变长字段列表 NULL值列表 数据头 真实数据</p>
<p>在实际存储一行数据的时候，会在他真实数据部分，添加一些隐藏字段</p>
<pre><code>DB_ROW_ID 字段：这是一个行的唯一标识，是数据库内部的一个标识，不是你的主键ID字段，入股我们没有指定主键和
unique key 唯一索引的时候，他的内部就会自动加一个DB_ROW_ID

DB_TRX_ID字段，这个跟事务相关，他是说这是哪个事务更新的数据，这是事务ID。

DB_ROLL_PTR，这是回滚指针，用来进行事务回滚的
</code></pre>
<h4 id="行溢出">行溢出</h4>
<p>行溢出：就是一行的数据存储太多的内容，一个数据页都放不下，此时只能溢出这个数据页，把数据溢出存放到其他数据页里去，那些数据页就叫做溢出页。</p>
<h4 id="表空间">表空间</h4>
<pre><code>表空间：我们平时创建的那些表，其实就是都有一个表空间的概念，在磁盘上对会对应'表明.ibd'，这样的一个磁盘数据文件。’

‘一个表空间磁盘文件里，其实会有很多很多的数据页，为了便于管理，表空间又引入了**数据区（extent）**
一个数据区对应64个连续的数据页，每个数据页的大小是16kb，所以一个数据区就是1mb，然后256个数据区被划分为一组。

当我们需要执行CRUD操作的时候，说白，就是从磁盘上表空间的数据文件里，去加载一些数据页出来到buffer pool的缓存页里区使用
</code></pre>
<figure data-type="image" tabindex="18"><img src="image/shujuqu.png" alt="img.png" loading="lazy"></figure>
<h3 id="生产实践">生产实践</h3>
<h4 id="真实生产环境下的数据库机器配置如何规划">真实生产环境下的数据库机器配置如何规划</h4>
<p>普通应用的机器选择？</p>
<pre><code>就经验而言，普通的系统 4核8G ，每秒抗几百的请求没问题，
数据库通常是在8核16G以上正常的是16核32G
</code></pre>
<p>高并发场景数据库应该选择什么样的机器？</p>
<pre><code>磁盘，io，网络压力会比较大，最好采用ssd固态硬盘
</code></pre>
<h4 id="互联网公司的生产环境数据库是如何进行性能测试的">互联网公司的生产环境数据库是如何进行性能测试的？</h4>
<p>请求测试指标：QPS、TPS</p>
<pre><code>QPS：Query Per Second，每秒可以处理多少个请求，也就是说这个数据库每秒可以处理多少个sql

TPS：Transaction Per Second 。其实就是每秒可处理的事物
</code></pre>
<p>IO相关压测性指标</p>
<pre><code>IOPS：这个是机器随机IO并发处理能力
这个指标很关键，你在内存中更新的脏数据，最后都会由后台IO在不确定时间，刷回到磁盘里去。这个是随机IO的过程，
如果说IOPS指标太低了，那么会导致脏数据刷回磁盘的效率不高。


吞吐量：这个指机器的磁盘存储每秒可以读写多少个字节的数据
这个指标也很关键，因为大家通过学习都知道，我们在平时执行各种sql的时候，提交事物的时候，其实都会有大量会写redo log日志之类的，这些日志都会直接写磁盘

latency：这个指标说的往磁盘里写入一条数据的延迟。
这个指标同样很重要，因为我们执行sql语句和提交事物的时候，都需要顺序写redo log 次哦盘文件，所以此时
你写一条日志到磁盘文件里去，到底延迟是1ms还是100us，这就是对你的数据库sql语句执行性能是有影响的
</code></pre>
<p>其它指标</p>
<pre><code>CPU负载：PU负载是⼀个很重要的性能指标，因为假设你数据库压测到了每秒处理3000请求了，可能其他的性能指标
都还正常，但是此时CPU负载特别⾼，那么也说明你的数据库不能继续往下压测更⾼的QPS了，否则CPU是吃不消的。

网络负载：这个主要是要看看你的机器带宽情况下，在压测到⼀定的QPS和TPS的时候，每秒钟机器的⽹卡会输⼊多少
MB数据，会输出多少MB数据，因为有可能你的⽹络带宽最多每秒传输100MB的数据，那么可能你的QPS到1000的时候，⽹
卡就打满了，已经每秒传输100MB的数据了，此时即使其他指标都还算正常，但是你也不能继续压测下去了

内存负载：：这个就是看看在压测到⼀定情况下的时候，你的机器内存耗费了多少，如果说机器内存耗费过⾼了，说明也
不能继续压测下去了
</code></pre>
<h4 id="如何对生产环境中的数据库进行360度无死角压测httpsapppukyptrl1086pcxiaoe-techcomdetaili_5e383c5357307_mjhluwmb1fromp_5e0c2a35dbbc9_mndgdybatype6">如何对生产环境中的数据库进行360度无死角压测？（https://apppukyptrl1086.pc.xiaoe-tech.com/detail/i_5e383c5357307_MjhluwMb/1?from=p_5e0c2a35dbbc9_MNDGDYba&amp;type=6）</h4>
<p>在linux 安装sysbench</p>
<pre><code>curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bash
sudo yum -y install sysbench
sysbench --version
如果上⾯可以看到sysbench的版本号，就说明安装成功了

QPS：Query Per Second，每秒可以处理多少个请求，也就是说这个数据库每秒可以处理多少个sql

TPS：Transaction Per Second 。其实就是每秒可处理的事物
</code></pre>
<p>如何为生产环境中的数据库部署监控系统</p>
<pre><code>Prometheus：其实就是一个监控数据采集和存储系统，它可以利用采用缉拿空数据采集组件从你指定的Mysql数据库中采集他需要的监控数据
然后他自己由一个时序数据库，他会把采集道德监控数据放到自己的时序数据库中，本质就是存储在磁盘文件里。

Grafana：就是一个可视化的监控数据展示系统，他可以Prometheus采集到的大量mysql监控数据展示成各种精美报告，可以让我们直接看到mysql的监控情况。
</code></pre>
<h4 id="如何通过多个buffer-pool来优化数据库的并发性能">如何通过多个Buffer Pool来优化数据库的并发性能</h4>
<p>多线程并发访问一个Buffer Pool的时候必然会加锁，然后很多线程可能要串行着排队，一个个的依次执行操作。</p>
<p>一般来说，Mysql默认的规则是，如果你给Buffer Pool分配的内存大小小于1GB，那么最多就会给你一个Buffer Pool</p>
<p>但是如果你的机器内存就很大，那么此时你是可以同时设置多个Buffer Pool</p>
<pre><code>  innodb_buffer_pool_size = 8589934592
  innodb_buffer_pool_instance = 4
</code></pre>
<p>我们给Buffer Pool 设置了8GB的总内存，然后设置了4个Buffer Pool，也就是说每个Buffer Pool的大小是2GB</p>
<p>所以在生产实践中设置多个Buffer Pool 来优化高并发访问的性能，是mysql一个很重要的优化技巧。</p>
<h4 id="如何通过chunk来支持数据库运行期间的buffer-pool动态调整">如何通过chunk来支持数据库运行期间的Buffer Pool动态调整</h4>
<p>实际上Buffer Pool是由很多个chuck组成的，他的大小是innodb_buffer_pool_chunk_size 来控制的默认值是128M</p>
<pre><code>所以实际上我们可以做一个假设，比如现在我们给Buffer Pool 设置一个总大小是8GB，然后4个Buffer Pool ，那么每个Buffer Pool 就是2GB
 此时每个Buffer Pool 是由一系列的128M chuck组成的，也就是说每个Buffer Pool 会有16个chuck，然后每个Buffer Pool里的每个chuck里就是一系列
数据描述和缓存页，每个Buffer Pool里的多个chuck共享一套 free flush lru 链表 
</code></pre>
<h4 id="在生产环境中如何基于机器配置来合理设置buffer-pool">在生产环境中，如何基于机器配置来合理设置Buffer Pool</h4>
<p>Buffer Pool 的大小一般设置为机器大小的50-60%</p>
<p>确定了Buffer pool 的总大小之后，就得考虑设置多少个buffer pool以及chuck</p>
<p>一般来说： buffer pool总大小 = （chuck大小 * buffer pool数量）的倍数</p>
<h4 id="linux操作系统的存储系统软件层原理剖析以及io调度优化原理">Linux操作系统的存储系统软件层原理剖析以及IO调度优化原理</h4>
<p>Linux的存储系统分为VFS层、⽂件系统层、Page Cache缓存层、通⽤Block层、IO调度层、Block设备驱动 层、Block设备层，</p>
<figure data-type="image" tabindex="19"><img src="image/linuxcunchujiegou.png" alt="img.png" loading="lazy"></figure>
<p>当mysql发起随机读写或者一次顺序写redo log日志文件的顺序读写的时候，实际上会把磁盘IO请求交给linux操作系统的VFS层</p>
<p>VFS层：根据你是对哪个目录中的文件执执行磁盘IO操作，把IO请求交给具体的文件系统。</p>
<pre><code>举个例⼦，在linux中，有的⽬录⽐如/xx1/xx2⾥的⽂件其实是由NFS⽂件系统管理的，有的⽬录⽐如/xx3/xx4⾥的⽂件
其实是由Ext3⽂件系统管理的，那么这个时候VFS层需要根据你是对哪个⽬录下的⽂件发起的读写IO请求，把请求转
交给对应的⽂件系统，如下图所⽰

接着⽂件系统会先在Page Cache这个基于内存的缓存⾥找你要的数据在不在⾥⾯，如果有就基于内存缓存来执⾏读
写，如果没有就继续往下⼀层⾛，此时这个请求会交给通⽤Block层，在这⼀层会把你对⽂件的IO请求转换为Block IO
请求，如下图所⽰

接着IO请求转换为Block IO请求之后，会把这个Block IO请求交给IO调度层，在这⼀层⾥默认是⽤CFQ公平调度算法的
也就是说，可能假设此时你数据库发起了多个SQL语句同时在执⾏IO操作。
有⼀个SQL语句可能⾮常简单，⽐如update xxx set xx1=xx2 where id=1，他其实可能就只要更新磁盘上的⼀个block
⾥的数据就可以了
但是有的SQL语句，⽐如说select * from xx where xx1 like &quot;%xx%&quot;可能需要IO读取磁盘上的⼤量数据。
那么此时如果基于公平调度算法，就会导致他先执⾏第⼆个SQL语句的读取⼤量数据的IO操作，耗时很久，然后第⼀
个仅仅更新少量数据的SQL语句的IO操作，就⼀直在等待他，得不到执⾏的机会。
所以在这⾥，其实⼀般建议MySQL的⽣产环境，需要调整为deadline IO调度算法，他的核⼼思想就是，任何⼀个IO操
作都不能⼀直不停的等待，在指定时间范围内，都必须让他去执⾏。
所以基于deadline算法，上⾯第⼀个SQL语句的更新少量数据的IO操作可能在等待⼀会⼉之后，就会得到执⾏的机会，
这也是⼀个⽣产环境的IO调度优化经验。
我们看下图，此时IO请求被转交给了IO调度层


最后IO完成调度之后，就会决定哪个IO请求先执⾏，哪个IO请求后执⾏，此时可以执⾏的IO请求就会交给Block设备驱
动层，然后最后经过驱动把IO请求发送给真正的存储硬件，也就是Block设备层，如下图所⽰。

然后硬件设备完成了IO读写操作之后，要不然是写，要不然是读，最后就把响应经过上⾯的层级反向依次返回，最终
MySQL可以得到本次IO读写操作的结果
</code></pre>
<figure data-type="image" tabindex="20"><img src="image/RAIDchongfangdian.png" alt="img.png" loading="lazy"></figure>
<h4 id="数据库服务器使用的raid存储架构初步介绍">数据库服务器使用的RAID存储架构初步介绍</h4>
<pre><code>所以MySQL数据库软件都是安装在一台linux服务器上的，然后启动MySQL的进程，就是启动了一个MySQL数据库

MySQL运行过程中，他需要使用CPU、内存、磁盘和网卡这些硬件，但是不能直接使用，都是通过调用操作系统提供的接口，依托于操作系统来使用和运行的，然后linux操作系统负责操作底层的硬件。
</code></pre>
<figure data-type="image" tabindex="21"><img src="image/mysql_1.png" alt="img.png" loading="lazy"></figure>
<pre><code>数据库部署在机器上的时候，存储都是搭建的RAID存储架构

RAID就是一个磁盘冗余阵列

RAID这个技术，大致理解为用来管理机器里的多块磁盘的一种磁盘阵列技术！

有了他以后，你在往磁盘里读写数据的时候，他会告诉你应该在哪块磁盘上读写数据，
</code></pre>
<figure data-type="image" tabindex="22"><img src="image/img_1.png" alt="img_1.png" loading="lazy"></figure>
<pre><code>有了RAID这种多磁盘阵列技术之后，我们是不是就可以在一台服务器里加多块磁盘，扩大我们的磁盘存储空间了？

当我们往磁盘里写数据的时候，通过RAID技术可以帮助我们选择一块磁盘写入，在读取数据的时候，我们也知道从哪块磁盘去读取。
</code></pre>
<p>除此之外，RAID技术很重要的一个作用，就是他还可以<strong>实现数据冗余机制</strong></p>
<pre><code>所谓的数据冗余机制，就是如果你现在写入了一批数据在RAID中的一块磁盘上，然后这块磁盘现在坏了，无法读取了，那么岂不是你就丢失了一波数据？如下图所示

![img.png](image/mysql_3.png)
</code></pre>
<p>所以其实有的RAID磁盘冗余阵列技术里，是可以把你写入的同样一份数据，在两块磁盘上都写入的.<br>
这样可以让两块磁盘上的数据一样，作为冗余备份，然后当你一块磁盘坏掉的时候，可以从另外一块磁盘读取冗余数据出来，这一切都是RAID技术自动帮你管理的，不需要你操心， 如下图。</p>
<figure data-type="image" tabindex="23"><img src="image/mysql_4.png" alt="img.png" loading="lazy"></figure>
<p>所以RAID技术实际上就是管理多块磁盘的一种磁盘阵列技术，他有软件层面的东西，也有硬件层买的东西，比如有RAID卡这种硬件设备。</p>
<p>具体来说，RAID还可以分成不同的技术方案，比如RAID 0、RAID 1、RAID 0+1、RAID2，等等，一直到RAID 10，很多种不同的多磁盘管理技术方案</p>
<h4 id="数据库服务器上的raid存储架构的电池充放电原理">数据库服务器上的RAID存储架构的电池充放电原理</h4>
<p>RAID緩存模式設置為write back，意思是先寫緩存再寫磁盤整列<br>
<img src="image/RAIDchongfangdian.png" alt="img.png" loading="lazy"></p>
<p>鋰電池會性能减弱，所以需要对锂电池的配置订定时充放电。</p>
<p>充电的过程中 RAID缓存级别会从write back 变成write through，这个时候IO直接些磁盘。性能会下降，导致数据库抖动出现性能抖动。</p>
<h4 id="raid锂电池充放电导致的mysql数据库性能抖动的优化">RAID锂电池充放电导致的MySQL数据库性能抖动的优化</h4>
<p>RAID 0：同时些很多快磁盘，读写并发能力强，容易丢失数据 RAID 1：两块磁盘为镜像关系，所写的数据在另一块磁盘上都有，形成数据冗余，防止数据丢失，分摊读写的压力。</p>
<p>RAID 10 = RAID 0 + RAID 1：写的时候使用RAID 0 的思路，备份使用RAID 1 的思路。</p>
<p>对于RAID 锂电池充放电问题导致的存储性能抖动，一般有三种解决方案：</p>
<pre><code>1.给RAID卡把锂电池换成电容，电容是不用频繁充放电的，不会导致充放电的性能抖动，还有就是电容可以支持透明充放电，就是自动检查电量，自动进行充电，不会说在充放电的时候让写IO直接走磁盘，但是更换电容很麻烦，而且电容比较容易老化，这个其实一般不常用

2.手动充放电，这个比较常用，包括一些大家知道的顶尖互联网大厂的数据库服务器的RAID就是用了这个方案避免性能抖动，就是关闭RAID自动充放电，然后写一个脚本，脚本每隔一段时间自动在晚上凌晨的业务低峰时期，脚本手动触发充放电，这样可以避免业务高峰期的时候RAID自动充放电引起性能抖动

3.充放电的时候不要关闭write back，就是设置一下，锂电池充放电的时候不要把缓存级别从write back修改为write through，这个也是可以做到的，可以和第二个策略配合起来使用
</code></pre>
<h4 id="数据库无法连接故障的定位toomanyconnections">数据库无法连接故障的定位TooManyConnections</h4>
<p>TooManyConnections 说明数据库连接池已经满了，你的业务系统不能与他建立更多的连接了。</p>
<p>检查mysql的配置文件 my.conf，里面有个关键的参数max_connections就是mysql建立的最大连接数。</p>
<p>查看mysql实际最大连接数</p>
<pre><code>show variables like 'max_connections'
</code></pre>
<p>mysql无法设置max_connections期望值，只能强行限制为214？为什么？</p>
<pre><code>    简单来说，就是因为底层linux操作系统把进程可以打开的文件句柄数限制为1024了导致mysql最大连接数时214
    为什么linux的最大文件句柄限制为1024的时候，MySQL的最大连接数是214呢？ 
    原因其实是mysql内部源码写死的，它在源码中就是有一个公式，算下来如此罢了
</code></pre>
<p>如何解决经典的Too Many Connections故障，背后的原理是什么？</p>
<pre><code>    ulimit -HSn 65535
    然后就可以用如下命令检查最大文件句柄数是否被修改了
    cat /etc/security/limits.conf
    cat /etc/rc.local
    如果都修改好了之后，可以在mysql的my.cnf里确保max_connections参数也调整好了，然后就可以重启服务器，重启mysql，
    这样的话，linux的最大文件句柄就会生效了，mysql最大连接数也会生效了
</code></pre>
<h4 id="线上数据库不确定性的性能抖动优化">线上数据库不确定性的性能抖动优化</h4>
<p>sql语句性能会出现不正常的莫名其妙的抖动，平时可能即使毫秒，现在居然要几秒钟，根本原因有两种：</p>
<p>1.第一个可能buffer pool缓存页都满了，此时你的sql查询了很多数据，一下把很多缓存页flush到磁盘上去，刷磁盘太慢了，就会导致你的查询语句执行的很慢。</p>
<p>2.第二种可能是你执行更新语句的时候，redo log在磁盘上的所有文件都写满了，此时需要回到第一个redo log文件覆盖写，覆盖写可能就涉及到第一个redo log文件里有很多<br>
redo log日志对应的更新操作改动了缓存页，那写缓存页还没有flush到磁盘，此时就必须把哪些缓存页的flush到磁盘，才能执行后续的更新语句，那么这一等待<br>
必然会导致更新执行很慢</p>
<p>如何尽可能的优化Mysql的一些参数，减少这种缓存页flush到磁盘带来的性能抖动的问题？</p>
<pre><code>1.对于不是数据库的机器一定要采用ssd的磁盘，
2.innodb_io_capacity 这个参数是告诉数据库采用多大的io速率把缓存页flush到磁盘里去的
    可以使用fio工具测试磁盘最大随机io速率之后，就知道他每秒可以执行多少次随机io
3.innodb_flush_neighbors,意思是在flush缓存页到磁盘德时候，可能会把缓存页临近的其他缓存页也刷到磁盘
但是这样有时候会导致flush缓存也太多了，实际上如果你使用的是SSD固态硬盘，并没必要让他同时刷进临近的缓存页，
可以把innodb_flush_neighbors设置为0，禁止刷进缓存页
</code></pre>
<h2 id="mysql主从架构">mysql主从架构</h2>
<h3 id="主从架构的原理">主从架构的原理</h3>
<p>大致来说：就是主库接受增删改的操作，把增删改操作binlog写入本地文件，然后从库发送请求来拉取binlog,接着从库上重复执行一遍binlog的操作，就可以还原出一样的数据。</p>
<h3 id="主从复制架构的搭建最基础架构">主从复制架构的搭建(最基础架构)</h3>
<p>事前准备：</p>
<pre><code>1.首先确保主库和从库的server_id是不同的，这个是必然的
2.主库必须打开binlog功能，你必须打开binlog功能，主库才会写binlog到本地磁盘，接着按照如下步骤
</code></pre>
<p>1.在主库上创建一个主从复制的账号</p>
<pre><code>create user 'backup_user'@'192.168.31.%' identified by 'backup_123';
grant replication slave on *.* to 'backup_user'@'192.168.31.%';
flush privileges;
</code></pre>
<p>2.如果主库跑了一段时间，现在要挂一个从库，应该在凌晨时，对主库和从库做一个数据备份和导入</p>
<pre><code>可以使用mysqldump工具把主库在这个时刻的数据全量备份，但是此时一定是不允许操作主库的，主库的数据时不能有变动的
/usr/local/mysql/bin/mysqldump --single-transaction -uroot -proot --master-data=2 -A &gt; backup.sql

注意，mysqldump工具就是在你的Mysql安装目录的bin目录下，然后用上述命令对你的主库所有的数据都做一个备份，
备份会以sql语句的方式进入指定的backup.sql 文件，只要执行backup.sql 就可以恢复出来跟主库一样的数据了

--master-data=2，是说备份的sql文件里，要记录一下此时主库的binlog文件和position号这是为了主从复制做准备的

接着从库执行下面的命令取执行主库进行复制

CHANGE MASTER TO MASTER_HOST='192.168.31.229',
MASTER_USER='backup_user',MASTER_PASSWORD='backup_123',MASTER_LOG_FILE='mysqlbin.000015',MASTER_LOG_POS=1689;

可能有人会疑惑，上面的master机器的ip地址我们是知道的，master上用于执行复制的用户名和密码是我们自己创建
的，也没问题，但是master的binlog文件和position是怎么知道的？这不就是之前我们mysqldump导出的
backup.sql里就有，大家在执行上述命令前，打开那个backup.sql就可以看到如下内容：
MASTER_LOG_FILE='mysql-bin.000015',MASTER_LOG_POS=1689
然后你就把上述内容写入到主从复制的命令里去了。
接着执行一个开始进行主从复制的命令：start slave，再用show slave status查看一下主从复制的状态，主要看到
Slave_IO_Running和Slave_SQL_Running都是Yes就说明一切正常了，主从开始复制了。
接着就可以在主库插入一条数据，然后在从库查询这条数据，只要能够在从库查到这条数据，就说明主从复制已经成
功了。
这仅仅是最简单的一种主从复制，就是异步复制，就是之前讲过的那种原理，从库是异步拉取binlog来同步的，所以
肯定会出现短暂的主从不一致的问题的，比如你在主库刚插入数据，结果在从库立马查询，可能是查不到的。
</code></pre>
<p>只要你搭建出来主从复制架构，就可以实现读写分离了<br>
可以用mycat 或者sharding-sphere之类的中间件，就可以实现你的系统写入主库，从库去读取</p>
<p>主从架构默认是异步的复制方式，意思是说主库把日志写入到binlog文件，接着自己提交事务返回了，他不管从库是否受到日志</p>
<p>如果主库宕机，从库切换为主库，可能发生数据丢失。</p>
<h5 id="主从半复制生产实践">主从半复制（生产实践）</h5>
<p>因此一般来说，搭建主从复制都是采用半同步的方式复制的，这个半同步的意思是，你主库写入数据，日志进入binlog之后，起码确保binlog<br>
日志复制到从库，才提交事务。</p>
<p>这个半同步复制的方式有两种</p>
<pre><code>    1.第一种叫做AFTER_COMMIT方式，他不是默认的，他的意思是说，主库写入日志到binlog，等待binlog日志复制到从库
    主库就提交自己的本地事务，接着就等待从库返回给自己一个成功的响应，然后主库就返回提交事务成功的响应给客户端。
</code></pre>
<p>（传统的搭建方式）<br>
搭建半复制也很简单，在搭建好异步复制的基础上，安装好版复制的插件就可以了，先在主库上安装半复制插件同时还得开启半复制功能</p>
<pre><code>install plugin rpl_semi_sync_master soname 'semisync_master.so';
set global rpl_semi_sync_master_enabled=on;
show plugins;
</code></pre>
<p>可以看到你安装了这个插件那就ok了</p>
<p>接着从库上页安装这个插件以及开启半复制功能：</p>
<pre><code>install plugin rpl_semi_sync_master soname 'semisync_master.so';
set global rpl_semi_sync_master_enabled=on;
show plugins;
</code></pre>
<p>接着要重启从库的IO线程：stop slave io_thread; start slave io_thread;</p>
<p>然后在主库上检查一下半同步复制是否正常运行：show global status like '%semi%';，如果看到了<br>
Rpl_semi_sync_master_status的状态是ON，那么就可以了。</p>
<p>到此半同步复制就开启成功了，其实一般来说主从复制都建议做成半同步复制，因为这样配合高可用切换机制，就可以保证数<br>
据库有一个在线的从库热备份主库的数据了，而且主要主库宕机，从库立马切换为主库，数据不丢失，数据库还高可用。</p>
<p>（GTID）搭建方式</p>
<p>首先在主库进行配置;</p>
<pre><code> gtid_mode=on
 enforce_gtid_consistency=on
 log_bin=on
 server_id=单独设置一个
 binlog_format=row
 接着在从库进行配置：
 gtid_mode=on
 enforce_gtid_consistency=on
 log_slave_updates=1
 server_id=单独设置一个

 接着按照之前讲解的步骤在主库创建好用于复制的账号之后，就可以跟之前一样进行操作了，比如在主库dump出来一
 份数据，在从库里导入这份数据，利用mysqldump备份工具做的导出，备份文件里会有SET
 @@GLOBAL.GTID_PURGED=***一类的字样，可以照着执行一下就可以了。
 接着其余步骤都是跟之前类似的，最后执行一下show master status，可以看到executed_gtid_set，里面记录的是执行
 过的GTID，接着执行一下SQL：select * from gtid_executed，可以查询到，对比一下，就会发现对应上了。
 那么此时就说明开始GTID复制了

其实大家会发现无论是GTID复制，还是传统复制，都不难，很简单，往往这就是比较典型的MySQL主从复制的搭建方
式了，然后大家可以自行搜索一下MyCat中间件或者是Sharding-Sphere的官方文档，其实也都不难，大家照着文档
做，整合到Java代码里去，就可以做出来基于主从复制的读写分离的效果了。
那些中间件都是支持读写分离模式的，可以仅仅往主库去写，从从库去读，这都没问题的。
如果落地到项目里，那么就完成了一个主从架构以及读写分离的架构了，此时按照我们之前所说的，如果说你的数据
库之前对一个库的读写请求每秒总共是2000，此时读写分离后，也许就对主库每秒写TPS才几百，从库的读QPS是
1000多。
那么万一你要是从库的读QPS越来越大，达到了每秒几千，此时你是不是会压力很大？没关系，这个时候你可以给主
库做更多的从库，搭建从库，给他挂到主库上去，每次都在凌晨搞，先让系统停机，对外不使用，数据不更新。
接着对主库做个dump，导出数据，到从库导入数据，做一堆配置，然后让从库开始接着某个时间点开始继续从主库复
制就可以了，一旦搭建完毕，就等于给主库挂了一个新的从库上去，此时继续放开系统的对外限制，继续使用就可以
了，整个过程基本在1小时以内。
如果在凌晨比如2点停机1小时，基本对业务是没有影响的。
</code></pre>
<h4 id="主从复制数据延迟问题">主从复制数据延迟问题</h4>
<p>为什么会产生主从延迟问题？</p>
<pre><code>其实很简单，其实你主库是多线程写入的，速度很快，从库是单个线程缓慢拉去数据，所以才会导致从库的复制数据的速度是比较慢的
</code></pre>
<p>主从复制的延迟实践监控</p>
<pre><code>这个可以用一个工具进行监控，比较推荐的是percona-toolkit工具集里的pt-hearbeat工具，他会在主库创建一个hearbeat表，
然后会有一个线程定时更新这个表里的时间戳字段，从库上就会有一个monitor线程会负责检查从库同步过来的hearbeat表里的时间戳。
把时间戳和当前时间对比一下就知道
</code></pre>
<p>如何缩小主从同步的延迟时间？</p>
<pre><code>其实就是让从库也用多线程并行复制数据就可以了，这样从库复制的速度很快，延迟就会很低了。
mysql5.7就已经支持并行复制了，可以在从库里设置slave_parallel_workers &gt;0,
然后把slave_parallel_type设为LOGICAL——CLOCK。
</code></pre>
<h4 id="高可用架构">高可用架构</h4>
<p>主从复制说白了就是允许主库把数据复制到从库上，然后允许我们的系统往主库里写数据，从库里读数据，实现一个读写分离的模式。</p>
<p>那么读写分离的模式确定了，接着就可以考虑一下数据库的高可用架构了，所谓的高可用架就是说数据库突然宕机，比如说主库或者从库宕机了，那么数据库还能正常使用吗？</p>
<p>如果主库真的宕机了，那就真的麻烦了，因为主库一旦宕机，你就没法写入数据，从库毕竟是不允许写入数据的，只允许读取。</p>
<p>所以数据库的高可用架构，可以实现主库宕机的同时，把从库切换为主库，然后所有的请求都基于现在的这台服务器去经行去读和写入。，</p>
<p>一般生产环境用于数据库高可用架构额管理工具MHA，是日本人写的，用peer脚本写一个工具，这个工具就是战门用于监控主库的状态，如果</p>
<p>感觉不对，就可以把从库切换为从库。</p>
<p>这个MHA也是需要单独部署的，分为两种节点，一个是Manager节点，一个是Node节点，Manager节点一般都是单独部署一台机器的<br>
Node节点一般都是部署在每天Mysql机器上的，因为Node节点通过通过解析各个Mysql的日志来进行一些操作</p>
<p>Manager节点会通过探测集群里的Node节点去判断各个Node所在机器上的MySQL运行是否正常，如果发现某个Master故障，就直接把Slave提升为Master，然后让<br>
其他Slave都挂到新的Master上去，完全透明。</p>
<h4 id="分库分表">分库分表</h4>
<p>要实现分库分表需要数据库中间件支持的，业内常用的一般有Sharding-Sphere以及Mycat两种，都是国内开源的。</p>
<p>一般建议Mysql单表数据量不超过1000万，最好在500万以内，如果内控制在100万以内，那是最佳的选择了，单表控制在100万以内，性能上不会有太大的问题，<br>
前提是你要建立好所有就行，其实保证Mysql高性能通常没有什么高深的技巧，就是控制数据量不要太大，另外只要保证你的查询用上了索引，所以一般就不会有问题。</p>
<p>一般分库分表时往往要考虑三个维度：<br>
1.一个是必然要按照主键id为粒度去分库分表，也就是把主键id进行hash后，对表数量进行取模，然后把数据均匀的分布到这些表中，<br>
再把这些表分散到多台数据库里。<br>
2.另外两个维度是用户端和运营端<br>
用户端：用户可能要查询自己的订单<br>
运营端：公司可能要查询所有的订单<br>
如何解决，针对用户端，你就需要按照(userids，主键id)这个表结构去做一个索引映射表，</p>
<p>userid和主键id的一一对应映射关系要放到这个表里，然后针对userid为粒度取进行分表分库</p>
<p>也就是对userid进行hash后取模，然后把数据均匀分散到很多索引映射表，再把表放到很多数据库里。</p>
<p>然后每次用户端拿出app查询自己的订单，直接根据userid取hash后取模路由到一个索引映射表，找到用户的userid</p>
<p>这里当然可以做一个分页了，先拿到所有的主键id，再根据主键id取对应的数据库里，去分库分表的表里提取完整数据。</p>
<p>至于运营端，一般都是根据N多个条件对数据经行搜索，此时跟上次将的一样，可以把数据的搜索条件放到es里面</p>
<p>然后用es来进行复杂搜索，找出一波主键id，再根据主键id去分库分表里找到完整数据。</p>
<p>分库分表的玩法基本都是这套思路，按业务id分库分表，建立索引映射表的同时进行分库分表，数据同步到es做复杂搜索，</p>
<p>基本这套玩法就可以保证你的分库分表的场景下，各种业务都可以执行</p>
]]></content>
    </entry>
</feed>